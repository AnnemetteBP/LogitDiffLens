{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6294ae",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Notebook for Logit Lens and Hidden Acts ======\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, DownloadMode\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from old_lens.mi_utils.quant_configs.bnb_configs import load_bnb_in_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f24fe2",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Models & Dataset =============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340878a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\LogitLensData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:1000]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")\n",
    "\n",
    "nq_queries = nq_dataset['train']['query']\n",
    "nq_answers = nq_dataset['train']['answer']\n",
    "\n",
    "nq_queries_200 = nq_queries[:200]\n",
    "\"\"\"nq_queries_400 = nq_queries[200:400]\n",
    "nq_queries_600 = nq_queries[400:600]\n",
    "nq_queries_800 = nq_queries[600:800]\n",
    "nq_queries_1000 = nq_queries[800:1000]\"\"\"\n",
    "nq_queries_200_1 = nq_queries[:1]\n",
    "nq_1000 = nq_queries[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6d258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Models(Enum):\n",
    "    LAIN8B = \"Models/LLaMA3Instruct\"\n",
    "    HF100B = \"Models/HF1BitLLM100Btokens\"\n",
    "\n",
    "\n",
    "class Names(Enum):\n",
    "    LAIN8B = \"Meta-Llama-3-8B-Instruct-fp\"\n",
    "    HF100B = \"Llama3-8B-1.58-100B-tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f135f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tok(\n",
    "    model_name: str,\n",
    "    low_cpu_mem_usage: bool = True,\n",
    "    local_files_only: bool = True,\n",
    "    device_map: str = \"cpu\",\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    load_in_8bit: bool = False\n",
    "):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        return_dict=True,\n",
    "        output_attentions=True,\n",
    "        low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "        local_files_only=local_files_only,\n",
    "        device_map=device_map,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        torch_dtype=dtype,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    return model, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig, orig_tokenizer = load_model_and_tok(Models.LAIN8B.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50589c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66e51ee67e545bbacf4e3fda4981214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_8bit, orig_tokenizer = load_model_and_tok(Models.LAIN8B.value, dtype=torch.float16, load_in_8bit=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit, orig_tokenizer = load_bnb_in_4bit(Models.LAIN8B.value, double_quant=False, dtype=torch.float16, device_map=\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant, quant_tokenizer = load_model_and_tok(Models.HF100B.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdeb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for q in nq_1000:\n",
    "    ids = orig_tokenizer.encode(q, add_special_tokens=True)\n",
    "    lengths.append(len(ids))\n",
    "\n",
    "print(\"=== Token Length Statistics (LLaMA-3-8B-Instruct tokenizer) ===\")\n",
    "print(f\"Samples analyzed: {len(lengths)}\")\n",
    "print(f\"Mean length:       {np.mean(lengths):.2f}\")\n",
    "print(f\"Median length:     {np.median(lengths):.0f}\")\n",
    "print(f\"90th percentile:   {np.percentile(lengths, 90):.0f}\")\n",
    "print(f\"95th percentile:   {np.percentile(lengths, 95):.0f}\")\n",
    "print(f\"Max observed len:  {np.max(lengths)}\")\n",
    "print(f\"Min observed len:  {np.min(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "sig = inspect.signature(model_orig.model.layers[0].forward)\n",
    "print(sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61d15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_npz_as_metrics(npz_path, norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\")):\n",
    "    \"\"\"\n",
    "    Load a .npz file saved from the logit lens analysis\n",
    "    and reconstruct the 'metrics' list expected by preprocess_metrics().\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    metrics = []\n",
    "\n",
    "    num_rows = len(data[\"prompt_id\"])\n",
    "    for i in range(num_rows):\n",
    "        row = {\n",
    "            \"prompt_id\": int(data[\"prompt_id\"][i]),\n",
    "            \"layer_index\": int(data[\"layer_index\"][i]),\n",
    "            \"layer_name\": str(data[\"layer_name\"][i]),\n",
    "            \"batch_index\": int(data[\"batch_index\"][i]),\n",
    "            \"vocab_size\": int(data[\"vocab_size\"][i]),\n",
    "        }\n",
    "\n",
    "        # Optional: if you saved prompt_texts, etc.\n",
    "        if \"prompt_text\" in data:\n",
    "            row[\"prompt_text\"] = str(data[\"prompt_text\"][i])\n",
    "\n",
    "        # target_ids & attention_mask are stored per row\n",
    "        if \"target_ids\" in data:\n",
    "            row[\"target_ids\"] = torch.from_numpy(data[\"target_ids\"][i])\n",
    "        if \"attention_mask\" in data:\n",
    "            row[\"attention_mask\"] = torch.from_numpy(data[\"attention_mask\"][i])\n",
    "\n",
    "        # Attach all normalization modes\n",
    "        for mode in norm_modes:\n",
    "            if f\"logits_{mode}\" in data:\n",
    "                row[f\"logits_{mode}\"] = torch.from_numpy(data[f\"logits_{mode}\"][i])\n",
    "            if f\"hidden_{mode}\" in data:\n",
    "                row[f\"hidden_{mode}\"] = torch.from_numpy(data[f\"hidden_{mode}\"][i])\n",
    "\n",
    "        metrics.append(row)\n",
    "\n",
    "    print(f\"[loaded] {num_rows} records from {npz_path}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0977e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "data_path = \"saved_data/lens_data/m_8bit/m_8bit_modes_batch000\"\n",
    "if not data_path.endswith(\".npz\"):\n",
    "    data_path += \".npz\"\n",
    "data = np.load(data_path, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d91d27de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'saved_data/lens_data/m_8bit/m_8bit_modes_batch000.npz' with keys: hidden_raw, logits_raw, hidden_unit_rms, logits_unit_rms, hidden_norm_rms..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b729d26",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Logit Lens with Normaliztion =================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37206e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"MatMul8bitLt: inputs will be cast\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Normalization modes\n",
    "# ============================================================\n",
    "def apply_normalization(x, model, normalize_mode=\"raw\", block=None, layer_index=None):\n",
    "    x = x.to(torch.float32) \n",
    "    \n",
    "    if normalize_mode == \"raw\":\n",
    "        return x\n",
    "\n",
    "    elif normalize_mode == \"unit_rms\":\n",
    "\n",
    "        eps = 1e-5\n",
    "        return x / (x.pow(2).mean(dim=-1, keepdim=True).add(eps).sqrt())\n",
    "\n",
    "    elif normalize_mode == \"rms_layer\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            return block.post_attention_layernorm(x)\n",
    "        else:\n",
    "            #return model.model.norm(x)\n",
    "            return model.model.norm(x).to(torch.float32)\n",
    "\n",
    "    elif normalize_mode == \"norm_rms\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        else:\n",
    "            #return model.model.norm(x)\n",
    "            return model.model.norm(x).to(torch.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization_mode: {normalize_mode}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper: causal + padding mask for LLaMA blocks\n",
    "# ============================================================\n",
    "def build_full_attention_mask(input_ids, attention_mask, device, model=None):\n",
    "    bsz, seq_len = input_ids.shape\n",
    "\n",
    "    causal = torch.triu(torch.ones((seq_len, seq_len), device=device, dtype=torch.bool), diagonal=1)\n",
    "    causal = causal.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        padding_mask = torch.zeros((bsz, 1, 1, seq_len), device=device, dtype=torch.bool)\n",
    "    else:\n",
    "        padding_mask = (attention_mask[:, None, None, :] == 0)\n",
    "\n",
    "    full = causal | padding_mask\n",
    "\n",
    "    attn_impl = getattr(model.config, \"attn_implementation\", \"eager\") if model else \"eager\"\n",
    "\n",
    "    if attn_impl in [\"flash_attention_2\", \"sdpa\"]:\n",
    "        return full\n",
    "    else:\n",
    "        return full.to(torch.float32) * -1e9\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Collector with multiple normalization variants + attention weights (optional storage)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def collect_logit_lens_full(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    batch_index=0,\n",
    "    max_len=17,\n",
    "    device=None,\n",
    "    clamp_logits=False,         \n",
    "    clamp_value=100.0,         \n",
    "    save_path=None,\n",
    "    collect_attn=True,\n",
    "    save_attn=True,\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "):\n",
    "\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    bnb_layer_types = (\"Linear4bit\", \"Linear8bitLt\")\n",
    "    is_quantized = any(any(name in type(m).__name__ for name in bnb_layer_types)\n",
    "                       for m in model.modules())\n",
    "\n",
    "    if is_quantized:\n",
    "        try:\n",
    "            first_param_device = next(model.parameters()).device\n",
    "        except StopIteration:\n",
    "            first_param_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[info] Detected quantized model → device {first_param_device}\")\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = model.to(device).eval()\n",
    "\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # Tokenization\n",
    "    # ============================================================\n",
    "    encoded = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    for p in prompts:\n",
    "        ids = tokenizer.encode(p, add_special_tokens=False)\n",
    "        content = ids[: max_len - 2]\n",
    "        ids = torch.tensor([bos_id] + content + [eos_id], dtype=torch.long)\n",
    "        if len(ids) < max_len:\n",
    "            ids = F.pad(ids, (0, max_len - len(ids)), value=pad_id)\n",
    "        encoded.append(ids)\n",
    "\n",
    "    input_ids = torch.stack(encoded, dim=0).to(device)\n",
    "\n",
    "    # ============================================================\n",
    "    # Build attention mask (stop after first EOS)\n",
    "    # ============================================================\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        eos_positions = (ids == eos_id).nonzero(as_tuple=True)[0]\n",
    "        if len(eos_positions) > 0:\n",
    "            eos_pos = eos_positions[0].item()\n",
    "            attention_mask[i, eos_pos + 1:] = 0\n",
    "\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    full_mask = build_full_attention_mask(input_ids, attention_mask, device)\n",
    "    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "    vocab_size = model.lm_head.out_features\n",
    "\n",
    "    print(f\"[info] Tokenized {batch_size} prompts | seq_len={seq_len}\")\n",
    "    print(f\"[info] Collecting from {len(model.model.layers)} layers | quantized={is_quantized}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Projection helper (with consistent upcasting and sanitization)\n",
    "    # ============================================================\n",
    "    def project(x):\n",
    "        x_fp32 = x.to(torch.float32)\n",
    "\n",
    "        head_dtype = next(model.lm_head.parameters()).dtype\n",
    "        x_cast = x_fp32.to(head_dtype)\n",
    "\n",
    "        logits = model.lm_head(x_cast)\n",
    "\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if clamp_logits and is_quantized:\n",
    "            logits = logits.clamp(-clamp_value, clamp_value)\n",
    "\n",
    "        return logits.to(torch.float32)\n",
    "\n",
    "    # ============================================================\n",
    "    # Collection containers\n",
    "    # ============================================================\n",
    "    rows = []\n",
    "    all_hidden, all_logits, all_attn = {}, {}, {}\n",
    "\n",
    "    # ============================================================\n",
    "    # Embedding layer\n",
    "    # ============================================================\n",
    "    x = model.model.embed_tokens(input_ids).to(torch.float32)\n",
    "    hidden_variants = {mode: apply_normalization(x.clone(), model, mode, layer_index=-1)\n",
    "                       for mode in norm_modes}\n",
    "    logits_variants = {mode: project(hidden_variants[mode]) for mode in norm_modes}\n",
    "\n",
    "    for mode in norm_modes:\n",
    "        all_hidden[f\"embed_tokens_{mode}\"] = hidden_variants[mode].cpu()\n",
    "        all_logits[f\"embed_tokens_{mode}\"] = logits_variants[mode].cpu()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rows.append({\n",
    "            \"prompt_id\": i,\n",
    "            \"prompt_text\": prompts[i],\n",
    "            \"batch_index\": batch_index,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_index\": -1,\n",
    "            \"layer_name\": \"embed_tokens\",\n",
    "            \"input_ids\": input_ids[i].cpu(),\n",
    "            \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "            \"attention_mask\": attention_mask[i].cpu(),\n",
    "            **{f\"hidden_{m}\": hidden_variants[m][i, :-1].cpu() for m in norm_modes},\n",
    "            **{f\"logits_{m}\": logits_variants[m][i, :-1].cpu() for m in norm_modes},\n",
    "        })\n",
    "\n",
    "    # ============================================================\n",
    "    # Transformer layers\n",
    "    # ============================================================\n",
    "    for li, block in enumerate(model.model.layers):\n",
    "        out = block(\n",
    "            x, position_ids=position_ids,\n",
    "            attention_mask=full_mask,\n",
    "            output_attentions=collect_attn,\n",
    "        )\n",
    "        x = out[0]\n",
    "        attn = out[1] if collect_attn else None\n",
    "\n",
    "        layer_output = x.detach().clone().to(torch.float32)\n",
    "        hidden_variants = {\n",
    "            mode: apply_normalization(layer_output.clone(), model, mode, block=block, layer_index=li)\n",
    "            for mode in norm_modes\n",
    "        }\n",
    "        logits_variants = {mode: project(hidden_variants[mode]) for mode in norm_modes}\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            hidden_variants[mode] = hidden_variants[mode][:, :-1, :]\n",
    "            logits_variants[mode] = logits_variants[mode][:, :-1, :]\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            all_hidden[f\"layer.{li}_{mode}\"] = hidden_variants[mode].cpu()\n",
    "            all_logits[f\"layer.{li}_{mode}\"] = logits_variants[mode].cpu()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            record = {\n",
    "                \"prompt_id\": i,\n",
    "                \"prompt_text\": prompts[i],\n",
    "                \"batch_index\": batch_index,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"layer_index\": li,\n",
    "                \"layer_name\": f\"layer.{li}\",\n",
    "                \"input_ids\": input_ids[i].cpu(),\n",
    "                \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "                \"attention_mask\": attention_mask[i].cpu(),\n",
    "                **{f\"hidden_{m}\": hidden_variants[m][i].cpu() for m in norm_modes},\n",
    "                **{f\"logits_{m}\": logits_variants[m][i].cpu() for m in norm_modes},\n",
    "            }\n",
    "            if save_attn and attn is not None:\n",
    "                record[\"attn\"] = attn[i].cpu()\n",
    "            rows.append(record)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # ============================================================\n",
    "    # Final RMSNorm output\n",
    "    # ============================================================\n",
    "    x = model.model.norm(x.to(torch.float32))\n",
    "    h = x\n",
    "    l = project(h)\n",
    "    h, l = h[:, :-1, :], l[:, :-1, :]\n",
    "\n",
    "    all_hidden[\"output_true\"] = h.cpu()\n",
    "    all_logits[\"output_true\"] = l.cpu()\n",
    "\n",
    "    for mode in norm_modes:\n",
    "        all_hidden[f\"output_{mode}\"] = h.cpu()\n",
    "        all_logits[f\"output_{mode}\"] = l.cpu()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rows.append({\n",
    "            \"prompt_id\": i,\n",
    "            \"prompt_text\": prompts[i],\n",
    "            \"batch_index\": batch_index,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_index\": len(model.model.layers),\n",
    "            \"layer_name\": \"output\",\n",
    "            \"input_ids\": input_ids[i].cpu(),\n",
    "            \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "            \"attention_mask\": attention_mask[i].cpu(),\n",
    "            **{f\"hidden_{m}\": h[i].cpu() for m in norm_modes},\n",
    "            **{f\"logits_{m}\": l[i].cpu() for m in norm_modes},\n",
    "        })\n",
    "\n",
    "    # ============================================================\n",
    "    # Save and finish\n",
    "    # ============================================================\n",
    "    if save_path:\n",
    "        base = os.path.splitext(save_path)[0]\n",
    "        np_data = {}\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            np_data[f\"hidden_{mode}\"] = torch.stack([r[f\"hidden_{mode}\"] for r in rows]).cpu().numpy()\n",
    "            np_data[f\"logits_{mode}\"] = torch.stack([r[f\"logits_{mode}\"] for r in rows]).cpu().numpy()\n",
    "\n",
    "        np_data[\"prompt_id\"]   = np.array([r[\"prompt_id\"] for r in rows])\n",
    "        np_data[\"batch_index\"] = np.array([r[\"batch_index\"] for r in rows])\n",
    "        np_data[\"layer_index\"] = np.array([r[\"layer_index\"] for r in rows])\n",
    "        np_data[\"layer_name\"]  = np.array([r[\"layer_name\"] for r in rows], dtype=object)\n",
    "        np_data[\"vocab_size\"]  = np.array([r[\"vocab_size\"] for r in rows])\n",
    "\n",
    "        out_path = f\"{base}.npz\"\n",
    "        np.savez_compressed(out_path, **np_data)\n",
    "        print(f\"[saved] {out_path} ({os.path.getsize(out_path)/(1024*1024):.2f} MB)\")\n",
    "\n",
    "        try:\n",
    "            test = np.load(out_path, allow_pickle=True)\n",
    "            print(f\"[check] loaded OK: {list(test.keys())[:5]} ...\")\n",
    "            test.close()\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Load check failed: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"[ok] Collected {len(rows)} entries from {len(model.model.layers)} layers.\")\n",
    "    return rows, all_hidden, all_logits, all_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8653a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_logit_lens_in_batches(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    all_prompts,\n",
    "    batch_size=20,\n",
    "    save_prefix=\"logitlens_batch\",\n",
    "    max_len=17,\n",
    "    normalize_mode=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    device=None,\n",
    "    clamp_logits=False,\n",
    "    collect_attn=False,\n",
    "    save_attn=False \n",
    "):\n",
    "\n",
    "    num_batches = (len(all_prompts) + batch_size - 1) // batch_size\n",
    "    print(f\"[run] Processing {len(all_prompts)} prompts in {num_batches} batches of {batch_size}\")\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Running logit lens batches\"):\n",
    "        start = batch_idx * batch_size\n",
    "        end = min((batch_idx + 1) * batch_size, len(all_prompts))\n",
    "        batch_prompts = all_prompts[start:end]\n",
    "\n",
    "        save_path = f\"{save_prefix}_batch{batch_idx:03d}.pt\"\n",
    "\n",
    "        print(f\"\\n[batch {batch_idx+1}/{num_batches}] {len(batch_prompts)} prompts → {save_path}\")\n",
    "\n",
    "        try:\n",
    "            rows, hidden_dict, logits_dict, all_attn = collect_logit_lens_full(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                prompts=batch_prompts,\n",
    "                max_len=max_len,\n",
    "                device=device,\n",
    "                norm_modes=normalize_mode,\n",
    "                save_path=save_path,\n",
    "                clamp_logits=clamp_logits,\n",
    "                collect_attn=collect_attn,\n",
    "                save_attn=save_attn,\n",
    "            )\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[error] Batch {batch_idx} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        del rows, hidden_dict, logits_dict, all_attn, batch_prompts\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\n[done] All batches processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c78c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] Processing 1 prompts in 1 batches of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running logit lens batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch 1/1] 1 prompts → saved_data/lens_data/m_8bit/m_8bit_modes_batch000.pt\n",
      "[info] Detected quantized model → device cpu\n",
      "[info] Tokenized 1 prompts | seq_len=17\n",
      "[info] Collecting from 32 layers | quantized=True\n",
      "[saved] saved_data/lens_data/m_8bit/m_8bit_modes_batch000.npz (481.88 MB)\n",
      "[check] loaded OK: ['hidden_raw', 'logits_raw', 'hidden_unit_rms', 'logits_unit_rms', 'hidden_norm_rms'] ...\n",
      "[ok] Collected 34 entries from 32 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running logit lens batches: 100%|██████████| 1/1 [02:06<00:00, 126.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[done] All batches processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_logit_lens_in_batches(\n",
    "    model=model_8bit,\n",
    "    tokenizer=orig_tokenizer,\n",
    "    all_prompts=nq_queries_200_1,\n",
    "    batch_size=20,\n",
    "    max_len=17,\n",
    "    normalize_mode=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    save_prefix=\"saved_data/lens_data/m_8bit/m_8bit_modes\",\n",
    "    device=\"cpu\",\n",
    "    clamp_logits=False,\n",
    "    collect_attn=False,\n",
    "    save_attn=False  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hidden_raw', 'logits_raw', 'hidden_unit_rms', 'logits_unit_rms', 'hidden_norm_rms', 'logits_norm_rms', 'prompt_id', 'batch_index', 'layer_index', 'layer_name', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"saved_data/lens_data/m_8bit/m_8bit_modes_batch000.npz\", allow_pickle=True)\n",
    "\n",
    "print(list(data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f926122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_raw           shape=(34, 16, 4096), dtype=float32\n",
      "logits_raw           shape=(34, 16, 128256), dtype=float32\n",
      "hidden_unit_rms      shape=(34, 16, 4096), dtype=float32\n",
      "logits_unit_rms      shape=(34, 16, 128256), dtype=float32\n",
      "hidden_norm_rms      shape=(34, 16, 4096), dtype=float32\n",
      "logits_norm_rms      shape=(34, 16, 128256), dtype=float32\n",
      "prompt_id            shape=(34,), dtype=int64\n",
      "batch_index          shape=(34,), dtype=int64\n",
      "layer_index          shape=(34,), dtype=int64\n",
      "layer_name           shape=(34,), dtype=object\n",
      "vocab_size           shape=(34,), dtype=int64\n"
     ]
    }
   ],
   "source": [
    "for k in data.keys():\n",
    "    arr = data[k]\n",
    "    print(f\"{k:20s} shape={arr.shape}, dtype={arr.dtype}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f3fd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = dict(data)\n",
    "data.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2c345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_raw          : NaNs=0, Infs=0\n",
      "logits_raw          : NaNs=0, Infs=0\n",
      "hidden_unit_rms     : NaNs=0, Infs=0\n",
      "logits_unit_rms     : NaNs=0, Infs=0\n",
      "hidden_norm_rms     : NaNs=0, Infs=0\n",
      "logits_norm_rms     : NaNs=0, Infs=0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "path = \"saved_data/lens_data/m_8bit/m_8bit_modes_batch000.npz\"\n",
    "data = np.load(path, allow_pickle=True)\n",
    "\n",
    "for k in data.keys():\n",
    "    arr = data[k]\n",
    "    if not np.issubdtype(arr.dtype, np.floating):\n",
    "        continue \n",
    "    n_nan = np.isnan(arr).sum()\n",
    "    n_inf = np.isinf(arr).sum()\n",
    "    print(f\"{k:20s}: NaNs={n_nan:,}, Infs={n_inf:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db6d619f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logits_raw</td>\n",
       "      <td>-0.068328</td>\n",
       "      <td>1.736763</td>\n",
       "      <td>37.6875</td>\n",
       "      <td>-66.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logits_unit_rms</td>\n",
       "      <td>-0.564260</td>\n",
       "      <td>1.427832</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>-15.554688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logits_norm_rms</td>\n",
       "      <td>-0.349259</td>\n",
       "      <td>2.006538</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>-13.992188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name      mean       std      max        min\n",
       "0       logits_raw -0.068328  1.736763  37.6875 -66.750000\n",
       "1  logits_unit_rms -0.564260  1.427832  22.0000 -15.554688\n",
       "2  logits_norm_rms -0.349259  2.006538  26.0000 -13.992188"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = []\n",
    "for k in data.keys():\n",
    "    if \"logits\" in k:\n",
    "        arr = data[k]\n",
    "        summary.append({\n",
    "            \"name\": k,\n",
    "            \"mean\": float(np.mean(arr)),\n",
    "            \"std\": float(np.std(arr)),\n",
    "            \"max\": float(np.max(arr)),\n",
    "            \"min\": float(np.min(arr))\n",
    "        })\n",
    "pd.DataFrame(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fe977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(orig_tokenizer.decode([128000]))\n",
    "print(orig_tokenizer.decode([128009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bee5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hello world<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(orig_tokenizer.decode([128000, 9906, 1917, 128009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37103286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 128009\n",
      "128000 128009\n"
     ]
    }
   ],
   "source": [
    "print(orig_tokenizer.bos_token_id, orig_tokenizer.eos_token_id)\n",
    "print(model_8bit.config.bos_token_id, model_8bit.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b55c33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 9906, 1917]\n"
     ]
    }
   ],
   "source": [
    "ids = orig_tokenizer.encode(\"Hello world\", add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logit_lens_in_batches(\n",
    "    model=model_quant,\n",
    "    tokenizer=quant_tokenizer,\n",
    "    all_prompts=nq_queries_200,\n",
    "    batch_size=20,\n",
    "    max_len=18,\n",
    "    normalize_mode=\"raw\",\n",
    "    save_prefix=\"logs/new_model_lens/raw/m_quant_raw\",\n",
    "    device=\"cpu\",\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f33a9",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# TopK Comparison ==============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def safe_mean(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if torch.is_tensor(x):\n",
    "        if x.numel() == 0:\n",
    "            return 0.0\n",
    "        x = torch.nan_to_num(x.detach().float(), nan=0.0, posinf=0.0, neginf=0.0).flatten()\n",
    "        if x.numel() == 0:\n",
    "            return 0.0\n",
    "        return float(x.mean().item())\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        arr = np.asarray(x, dtype=np.float64).ravel()\n",
    "        if arr.size == 0:\n",
    "            return 0.0\n",
    "        arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return float(np.mean(arr))\n",
    "    try:\n",
    "        val = float(x)\n",
    "        if not np.isfinite(val):\n",
    "            return 0.0\n",
    "        return val\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def preprocess_metrics(metrics, lens_type=\"raw\"):\n",
    "    processed = []\n",
    "    for row in metrics:\n",
    "        logits = row.get(f\"logits_{lens_type}\", row.get(\"logits\"))\n",
    "        attn_mask = row.get(\"attention_mask\")\n",
    "        targets = row.get(\"target_ids\")\n",
    "\n",
    "        if logits is None or targets is None:\n",
    "            continue\n",
    "\n",
    "        # Convert to tensors\n",
    "        logits = logits.unsqueeze(0) if logits.ndim == 2 else logits\n",
    "        targets = targets.unsqueeze(0) if targets.ndim == 1 else targets\n",
    "\n",
    "        # --- Build eval mask (ignore BOS, keep up to first EOS)\n",
    "        if attn_mask is not None:\n",
    "            eval_mask = attn_mask.clone()\n",
    "            eval_mask[:, 0] = 0  # ignore BOS\n",
    "            eval_mask = eval_mask.bool()\n",
    "        else:\n",
    "            seq_len = logits.size(1)\n",
    "            eval_mask = torch.ones((1, seq_len), dtype=torch.bool)\n",
    "            eval_mask[:, 0] = 0\n",
    "\n",
    "        # Apply mask to logits + targets\n",
    "        logits = logits[:, eval_mask[0], :]\n",
    "        targets = targets[:, eval_mask[0]]\n",
    "\n",
    "        # Store\n",
    "        row_proc = dict(row)\n",
    "        row_proc[\"logits\"] = logits\n",
    "        row_proc[\"target_ids\"] = targets\n",
    "        row_proc[\"eval_mask\"] = eval_mask\n",
    "        processed.append(row_proc)\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "def summarize(results, mode=\"raw\"):\n",
    "    df = pd.DataFrame(results[mode])\n",
    "    summary = df.groupby(\"layer_index\").agg({\n",
    "        \"kl_ab\": \"mean\",\n",
    "        \"jsd\": \"mean\",\n",
    "        \"cosine_sim\": \"mean\",\n",
    "        \"l2_dist\": \"mean\",\n",
    "        \"ppl_A\": \"mean\",\n",
    "        \"ppl_B\": \"mean\"\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "\n",
    "def combine_results(output_dir=\"logs/new_summary\", run_name=\"run\"):\n",
    "    import glob, pandas as pd\n",
    "    files = glob.glob(f\"{output_dir}/{run_name}_*.parquet\")\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        mode = os.path.basename(f).replace(f\"{run_name}_\", \"\").replace(\".parquet\", \"\")\n",
    "        df = pd.read_parquet(f)\n",
    "        df[\"mode\"] = mode\n",
    "        dfs.append(df)\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merged] {len(combined)} total rows from {len(files)} files.\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_all_metrics_multi(\n",
    "    metrics_A,\n",
    "    metrics_B,\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk=(1, 5, 10, 20),\n",
    "    device=None,\n",
    "    eps=1e-9,\n",
    "    output_dir=\"logs/new_summary\",\n",
    "    run_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute divergence, similarity, and behavioral alignment metrics\n",
    "    across multiple normalization modes — in *wide-format*\n",
    "    (columns per normalization mode), including all top-k accuracy\n",
    "    and agreement/disagreement measures.\n",
    "    \"\"\"\n",
    "    import os, pandas as pd\n",
    "\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    topk = sorted(set([1] + list(topk)))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Helpers\n",
    "    # ------------------------------------------------------------\n",
    "    def safe(x):\n",
    "        return torch.nan_to_num(x, nan=0.0, posinf=30.0, neginf=-30.0)\n",
    "\n",
    "    def pad_vocab_dim(logits, target_vocab):\n",
    "        pad = target_vocab - logits.size(-1)\n",
    "        if pad > 0:\n",
    "            logits = F.pad(logits, (0, pad))\n",
    "        return logits\n",
    "\n",
    "    preprocess = lambda m, t: preprocess_metrics(m, lens_type=t)\n",
    "    metrics_proc_A = {mode: preprocess(metrics_A, mode) for mode in norm_modes}\n",
    "    metrics_proc_B = {mode: preprocess(metrics_B, mode) for mode in norm_modes}\n",
    "\n",
    "    base_rows = metrics_proc_A[norm_modes[0]]\n",
    "\n",
    "    # Base metadata template\n",
    "    meta_rows = [\n",
    "        {\n",
    "            \"prompt_id\": rA.get(\"prompt_id\"),\n",
    "            \"prompt_text\": rA.get(\"prompt_text\"),\n",
    "            \"batch_index\": rA.get(\"batch_index\"),\n",
    "            \"layer_index\": rA.get(\"layer_index\"),\n",
    "            \"layer_name\": rA.get(\"layer_name\", f\"layer.{rA.get('layer_index', -1)}\"),\n",
    "            \"vocab_size\": rA.get(\"vocab_size\"),\n",
    "        }\n",
    "        for rA in base_rows\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ============================================================\n",
    "    # MAIN LOOP — per (prompt × layer)\n",
    "    # ============================================================\n",
    "    for meta in meta_rows:\n",
    "        pid, lid = meta[\"prompt_id\"], meta[\"layer_index\"]\n",
    "        merged = meta.copy()\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            row_A = [r for r in metrics_proc_A[mode] if r[\"prompt_id\"] == pid and r[\"layer_index\"] == lid][0]\n",
    "            row_B = [r for r in metrics_proc_B[mode] if r[\"prompt_id\"] == pid and r[\"layer_index\"] == lid][0]\n",
    "\n",
    "            logits_A = safe(row_A[\"logits\"].to(device).float())\n",
    "            logits_B = safe(row_B[\"logits\"].to(device).float())\n",
    "            targets = row_A[\"target_ids\"].to(device)\n",
    "\n",
    "            vocab_target = max(logits_A.size(-1), logits_B.size(-1))\n",
    "            logits_A, logits_B = pad_vocab_dim(logits_A, vocab_target), pad_vocab_dim(logits_B, vocab_target)\n",
    "\n",
    "            # --- Probabilities ---\n",
    "            logp_A = F.log_softmax(logits_A, dim=-1)\n",
    "            logp_B = F.log_softmax(logits_B, dim=-1)\n",
    "            probs_A, probs_B = logp_A.exp(), logp_B.exp()\n",
    "\n",
    "            # ========================================================\n",
    "            # === Divergence and Similarity Metrics\n",
    "            # ========================================================\n",
    "            kl_ab = torch.sum(probs_A * (logp_A - logp_B), dim=-1).mean().item()\n",
    "            kl_ba = torch.sum(probs_B * (logp_B - logp_A), dim=-1).mean().item()\n",
    "            jsd = 0.5 * (kl_ab + kl_ba)\n",
    "            tvd = 0.5 * torch.sum(torch.abs(probs_A - probs_B), dim=-1).mean().item()\n",
    "\n",
    "            entropy_A = (-torch.sum(probs_A * logp_A, dim=-1)).mean().item()\n",
    "            entropy_B = (-torch.sum(probs_B * logp_B, dim=-1)).mean().item()\n",
    "\n",
    "            cosine_sim = F.cosine_similarity(logits_A, logits_B, dim=-1).mean().item()\n",
    "            l2_dist = torch.norm(logits_A - logits_B, dim=-1).mean().item()\n",
    "\n",
    "            # ========================================================\n",
    "            # === Ground-truth log-probabilities\n",
    "            # ========================================================\n",
    "            gather_idx = targets.unsqueeze(-1).clamp(max=logp_A.size(-1) - 1)\n",
    "            logp_gt_A = logp_A.gather(-1, gather_idx).squeeze(-1)\n",
    "            logp_gt_B = logp_B.gather(-1, gather_idx).squeeze(-1)\n",
    "            logp_diff = (logp_gt_A - logp_gt_B).mean().item()\n",
    "\n",
    "            # ========================================================\n",
    "            # === Dispersion + Perplexity\n",
    "            # ========================================================\n",
    "            sorted_A, _ = torch.sort(probs_A, dim=-1)\n",
    "            sorted_B, _ = torch.sort(probs_B, dim=-1)\n",
    "            iqr_A = (torch.quantile(sorted_A, 0.75, dim=-1) - torch.quantile(sorted_A, 0.25, dim=-1)).mean().item()\n",
    "            iqr_B = (torch.quantile(sorted_B, 0.75, dim=-1) - torch.quantile(sorted_B, 0.25, dim=-1)).mean().item()\n",
    "\n",
    "            ce_A = F.cross_entropy(logits_A.view(-1, vocab_target), targets.view(-1), reduction=\"mean\")\n",
    "            ce_B = F.cross_entropy(logits_B.view(-1, vocab_target), targets.view(-1), reduction=\"mean\")\n",
    "            ppl_A, ppl_B = torch.exp(ce_A).item(), torch.exp(ce_B).item()\n",
    "\n",
    "            # ========================================================\n",
    "            # === Top-k Accuracy + Agreement/Disagreement\n",
    "            # ========================================================\n",
    "            max_k = max(topk)\n",
    "            top_vals_A, top_idx_A = torch.topk(probs_A, max_k, dim=-1)\n",
    "            top_vals_B, top_idx_B = torch.topk(probs_B, max_k, dim=-1)\n",
    "\n",
    "            for k in topk:\n",
    "                tkA, tkB = top_idx_A[:, :, :k], top_idx_B[:, :, :k]\n",
    "                tvA, tvB = top_vals_A[:, :, :k], top_vals_B[:, :, :k]\n",
    "\n",
    "                # --- Basic overlaps ---\n",
    "                inter_mask = (tkA.unsqueeze(-1) == tkB.unsqueeze(-2))\n",
    "                inter = inter_mask.any(dim=-1).sum(dim=2).float()\n",
    "                jaccard = (inter / (2 * k - inter + eps)).mean().item()\n",
    "                disagree = (1 - jaccard)\n",
    "\n",
    "                # --- Accuracy ---\n",
    "                tgt_expand = targets.unsqueeze(0) if targets.ndim == 1 else targets\n",
    "                acc_A = (tkA == tgt_expand.unsqueeze(-1)).any(dim=-1).float().mean().item()\n",
    "                acc_B = (tkB == tgt_expand.unsqueeze(-1)).any(dim=-1).float().mean().item()\n",
    "\n",
    "                # --- Correctness-based ---\n",
    "                acc_A_mat = (tkA == tgt_expand.unsqueeze(-1)).any(dim=-1).float()\n",
    "                acc_B_mat = (tkB == tgt_expand.unsqueeze(-1)).any(dim=-1).float()\n",
    "                agree_correct = (acc_A_mat * acc_B_mat).mean().item()\n",
    "                disagree_correct = ((acc_A_mat + acc_B_mat == 1).float()).mean().item()\n",
    "                agree_wrong = (((1 - acc_A_mat) * (1 - acc_B_mat)).float()).mean().item()\n",
    "\n",
    "                # --- Shared probability mass ---\n",
    "                prob_mass_A, prob_mass_B = tvA.sum(dim=-1), tvB.sum(dim=-1)\n",
    "                shared_mass = torch.zeros_like(prob_mass_A)\n",
    "                for i in range(tkA.size(0)):\n",
    "                    shared_tokens = set(tkA[i].reshape(-1).tolist()) & set(tkB[i].reshape(-1).tolist())\n",
    "                    if shared_tokens:\n",
    "                        st = torch.tensor(list(shared_tokens), device=device)\n",
    "                        shared_mass[i] = 0.5 * (probs_A[i, :, st].sum() + probs_B[i, :, st].sum())\n",
    "                prob_overlap = (shared_mass / (0.5 * (prob_mass_A + prob_mass_B) + eps)).mean().item()\n",
    "\n",
    "                # --- Add as flattened columns ---\n",
    "                merged.update({\n",
    "                    f\"acc_A_top{k}_{mode}\": acc_A,\n",
    "                    f\"acc_B_top{k}_{mode}\": acc_B,\n",
    "                    f\"jaccard_top{k}_{mode}\": jaccard,\n",
    "                    f\"disagree_top{k}_{mode}\": disagree,\n",
    "                    f\"agree_correct_top{k}_{mode}\": agree_correct,\n",
    "                    f\"disagree_correct_top{k}_{mode}\": disagree_correct,\n",
    "                    f\"agree_wrong_top{k}_{mode}\": agree_wrong,\n",
    "                    f\"prob_overlap_top{k}_{mode}\": prob_overlap,\n",
    "                })\n",
    "\n",
    "            # ========================================================\n",
    "            # === Add scalar metrics for this mode\n",
    "            # ========================================================\n",
    "            merged.update({\n",
    "                f\"kl_ab_{mode}\": kl_ab,\n",
    "                f\"kl_ba_{mode}\": kl_ba,\n",
    "                f\"jsd_{mode}\": jsd,\n",
    "                f\"tvd_{mode}\": tvd,\n",
    "                f\"entropy_A_{mode}\": entropy_A,\n",
    "                f\"entropy_B_{mode}\": entropy_B,\n",
    "                f\"cosine_sim_{mode}\": cosine_sim,\n",
    "                f\"l2_dist_{mode}\": l2_dist,\n",
    "                f\"logp_diff_{mode}\": logp_diff,\n",
    "                f\"ppl_A_{mode}\": ppl_A,\n",
    "                f\"ppl_B_{mode}\": ppl_B,\n",
    "                f\"ppl_diff_{mode}\": ppl_A - ppl_B,\n",
    "                f\"iqr_A_{mode}\": iqr_A,\n",
    "                f\"iqr_B_{mode}\": iqr_B,\n",
    "            })\n",
    "\n",
    "        results.append(merged)\n",
    "\n",
    "    # ============================================================\n",
    "    # SAVE WIDE-FORMAT RESULTS\n",
    "    # ============================================================\n",
    "    df = pd.DataFrame(results)\n",
    "    out_path = os.path.join(output_dir, f\"{run_name or 'run'}_metrics_wide.parquet\")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(f\"[saved] {len(df)} rows with full multi-mode metrics → {out_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043609b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_npz_as_metrics(npz_path, norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\")):\n",
    "    \"\"\"\n",
    "    Load a .npz file saved from the logit lens analysis\n",
    "    and reconstruct the 'metrics' list expected by preprocess_metrics().\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    metrics = []\n",
    "\n",
    "    num_rows = len(data[\"prompt_id\"])\n",
    "    for i in range(num_rows):\n",
    "        row = {\n",
    "            \"prompt_id\": int(data[\"prompt_id\"][i]),\n",
    "            \"layer_index\": int(data[\"layer_index\"][i]),\n",
    "            \"layer_name\": str(data[\"layer_name\"][i]),\n",
    "            \"batch_index\": int(data[\"batch_index\"][i]),\n",
    "            \"vocab_size\": int(data[\"vocab_size\"][i]),\n",
    "        }\n",
    "\n",
    "        # Optional: if you saved prompt_texts, etc.\n",
    "        if \"prompt_text\" in data:\n",
    "            row[\"prompt_text\"] = str(data[\"prompt_text\"][i])\n",
    "\n",
    "        # target_ids & attention_mask are stored per row\n",
    "        if \"target_ids\" in data:\n",
    "            row[\"target_ids\"] = torch.from_numpy(data[\"target_ids\"][i])\n",
    "        if \"attention_mask\" in data:\n",
    "            row[\"attention_mask\"] = torch.from_numpy(data[\"attention_mask\"][i])\n",
    "\n",
    "        # Attach all normalization modes\n",
    "        for mode in norm_modes:\n",
    "            if f\"logits_{mode}\" in data:\n",
    "                row[f\"logits_{mode}\"] = torch.from_numpy(data[f\"logits_{mode}\"][i])\n",
    "            if f\"hidden_{mode}\" in data:\n",
    "                row[f\"hidden_{mode}\"] = torch.from_numpy(data[f\"hidden_{mode}\"][i])\n",
    "\n",
    "        metrics.append(row)\n",
    "\n",
    "    print(f\"[loaded] {num_rows} records from {npz_path}\")\n",
    "    return metrics\n",
    "\n",
    "metrics_A = load_npz_as_metrics(\"model_A_batch_03.npz\")\n",
    "metrics_B = load_npz_as_metrics(\"model_B_batch_03.npz\")\n",
    "\n",
    "results = compute_all_metrics_multi(metrics_A, metrics_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_raw = pd.read_parquet(\"logs/new_summary/run_raw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "valid_positions = record[\"attention_mask\"][:-1].bool()\n",
    "logits = record[\"logits\"][valid_positions]\n",
    "targets = record[\"target_ids\"][valid_positions]\n",
    "\n",
    "bos_id, eos_id = tokenizer.bos_token_id, tokenizer.eos_token_id\n",
    "mask = (record[\"attention_mask\"][:-1].bool()) '\\'\n",
    "        & (record[\"input_ids\"][:-1] != bos_id) '\\'\n",
    "        & (record[\"input_ids\"][:-1] != eos_id)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b50b1",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Hidden Acts Similarity & PPL =================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c298e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_hidden_batches(folder, pattern=\"*.pt\"):\n",
    "    \"\"\"Load all saved hidden-state batches collected with collect_hidden_states_full().\"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    all_rows = []\n",
    "    for f in files:\n",
    "        print(f\"[load] {f}\")\n",
    "        rows = torch.load(f, map_location=\"cpu\")\n",
    "        all_rows.extend(rows)\n",
    "    print(f\"[ok] Loaded {len(all_rows)} rows from {len(files)} files.\")\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "def analyze_hidden_similarity_batched(rows_A, rows_B, compute_perplexity=True):\n",
    "    \"\"\"\n",
    "    Compare hidden activations from two models (A vs B).\n",
    "    Computes:\n",
    "      - L2 distance (positional, sequence, layer)\n",
    "      - Cosine similarity (positional, sequence, layer)\n",
    "      - Optional: perplexity per model if logits available\n",
    "\n",
    "    Returns:\n",
    "        df_detailed : detailed per-prompt/layer/position metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def safe(x, default=None, eps=1e-9):\n",
    "        \"\"\"Clamp and sanitize tensor values.\"\"\"\n",
    "        if x is None:\n",
    "            return torch.tensor(default if default is not None else 0.0)\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=30.0, neginf=-30.0)\n",
    "        if torch.is_floating_point(x):\n",
    "            mean_val = x.mean().item() if x.numel() > 0 else 0.0\n",
    "            if mean_val < 1.0:  # looks like log-probs\n",
    "                x = torch.clamp(x, min=-30.0, max=30.0)\n",
    "            else:\n",
    "                x = torch.clamp(x, min=0.0, max=1e6)\n",
    "        return x\n",
    "\n",
    "    detailed = []\n",
    "    index_B = {(r[\"prompt_id\"], r[\"layer_name\"]): r for r in rows_B if \"hidden\" in r}\n",
    "\n",
    "    for rA in tqdm(rows_A, desc=\"Comparing hidden states\"):\n",
    "        key = (rA[\"prompt_id\"], rA[\"layer_name\"])\n",
    "        rB = index_B.get(key)\n",
    "        if not rB or \"hidden\" not in rA or \"hidden\" not in rB:\n",
    "            continue\n",
    "\n",
    "        hA, hB = rA[\"hidden\"], rB[\"hidden\"]\n",
    "        if not isinstance(hA, torch.Tensor) or not isinstance(hB, torch.Tensor):\n",
    "            continue\n",
    "        if hA.shape != hB.shape:\n",
    "            continue\n",
    "\n",
    "        # === Hidden-state metrics ===\n",
    "        diff = hA - hB\n",
    "        l2_pos = torch.norm(diff, dim=-1)\n",
    "        cos_pos = F.cosine_similarity(hA, hB, dim=-1)\n",
    "        l2_seq = l2_pos.mean().item()\n",
    "        cos_seq = cos_pos.mean().item()\n",
    "\n",
    "        # === Optional perplexity ===\n",
    "        pplA = pplB = ppl_diff_signed = None\n",
    "        if compute_perplexity and \"logits\" in rA and \"logits\" in rB:\n",
    "            for label, logits in zip((\"A\", \"B\"), (rA[\"logits\"], rB[\"logits\"])):\n",
    "                if logits is not None and \"target_ids\" in rA:\n",
    "                    log_probs = safe(F.log_softmax(logits, dim=-1))\n",
    "                    target = rA[\"target_ids\"]\n",
    "                    nll = -log_probs.gather(-1, target.unsqueeze(-1)).squeeze(-1)\n",
    "                    mask = (target != 0).float()\n",
    "                    mean_nll = (nll * mask).sum() / mask.sum()\n",
    "                    ppl = math.exp(mean_nll.item())\n",
    "                    if label == \"A\":\n",
    "                        pplA = ppl\n",
    "                    else:\n",
    "                        pplB = ppl\n",
    "\n",
    "            # signed PPL difference (log-scale for stability)\n",
    "            if pplA is not None and pplB is not None:\n",
    "                ppl_diff_signed = math.log(pplB + 1e-9) - math.log(pplA + 1e-9)\n",
    "\n",
    "        detailed.append({\n",
    "            \"prompt_id\": rA[\"prompt_id\"],\n",
    "            \"batch_index\": rA.get(\"batch_index\"),\n",
    "            \"layer_index\": rA[\"layer_index\"],\n",
    "            \"layer_name\": rA[\"layer_name\"],\n",
    "            \"mean_l2_seq\": l2_seq,\n",
    "            \"mean_cos_seq\": cos_seq,\n",
    "            \"l2_pos\": l2_pos.cpu(),\n",
    "            \"cos_pos\": cos_pos.cpu(),\n",
    "            \"ppl_A\": pplA,\n",
    "            \"ppl_B\": pplB,\n",
    "            \"ppl_diff_signed\": ppl_diff_signed,\n",
    "        })\n",
    "\n",
    "    if not detailed:\n",
    "        raise ValueError(\"No valid pairs found. Check layer alignment and hidden data.\")\n",
    "\n",
    "    df_detailed = pd.DataFrame(detailed)\n",
    "    print(f\"[ok] Computed similarity metrics for {df_detailed['layer_name'].nunique()} layers.\")\n",
    "    return df_detailed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dc426",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Head Semantics ===============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# orig→quant: orig_quant\n",
    "# quant→orig quant_orig\n",
    "@torch.no_grad()\n",
    "def project_hidden_through_other_head(\n",
    "    projector_model,\n",
    "    hidden_file,\n",
    "    batch_num=0,\n",
    "    save_prefix=\"saved_data/projections\",\n",
    "    direction_tag=\"orig_quant\",\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Projects pre-saved hidden activations (from collect_hidden_states_full)\n",
    "    through ANOTHER model's LM head (cross-model projection).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Ensure model + device setup ---\n",
    "    projector_model = projector_model.to(device).eval()\n",
    "    Path(save_prefix).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Handle batch number formatting ---\n",
    "    if isinstance(batch_num, str) and not batch_num.isdigit():\n",
    "        batch_str = batch_num\n",
    "    else:\n",
    "        try:\n",
    "            batch_str = f\"{int(batch_num):03d}\"\n",
    "        except Exception:\n",
    "            batch_str = str(batch_num)\n",
    "\n",
    "    # --- Define save path ---\n",
    "    save_path = f\"{save_prefix}/proj_{direction_tag}_batch{batch_str}.pt\"\n",
    "    print(f\"[run] Projecting hidden activations {direction_tag} → {save_path}\")\n",
    "\n",
    "    # --- Load pre-saved activations ---\n",
    "    rows = torch.load(hidden_file, map_location=\"cpu\", weights_only=False)\n",
    "    results = []\n",
    "\n",
    "    # --- Main projection loop ---\n",
    "    for r in tqdm(rows, desc=f\"[batch {batch_str}] {direction_tag} projection\"):\n",
    "        if \"hidden\" not in r:\n",
    "            continue\n",
    "\n",
    "        h = r[\"hidden\"].to(device)\n",
    "        logits_cross = projector_model.lm_head(h).cpu()\n",
    "\n",
    "        # Preserve all meta info\n",
    "        results.append({\n",
    "            \"prompt_id\": r.get(\"prompt_id\"),\n",
    "            \"prompt_text\": r.get(\"prompt_text\"),\n",
    "            \"batch_index\": r.get(\"batch_index\", batch_num),\n",
    "            \"layer_index\": r.get(\"layer_index\"),\n",
    "            \"layer_name\": r.get(\"layer_name\"),\n",
    "            \"input_ids\": r.get(\"input_ids\"),\n",
    "            \"target_ids\": r.get(\"target_ids\"),\n",
    "            \"logits_crossproj\": logits_cross,\n",
    "        })\n",
    "\n",
    "        # Free memory\n",
    "        del h, logits_cross\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # --- Save the result ---\n",
    "    torch.save(results, save_path)\n",
    "    print(f\"[saved] → {save_path} ({len(results)} entries, batch_index={batch_num})\")\n",
    "    return save_path\n",
    "\n",
    "batch_num = \"009\"\n",
    "\n",
    "# (1) Project original model’s hidden states through quantized model’s head (quant→orig)\n",
    "project_hidden_through_other_head(\n",
    "    projector_model=model_quant,\n",
    "    hidden_file=f\"saved_data/hidden_acts/m_orig_batch{batch_num}.pt\",\n",
    "    batch_num=batch_num,\n",
    "    save_prefix=\"saved_data/projections\",\n",
    "    direction_tag=\"orig_quant\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "batch_num = \"009\"\n",
    "\n",
    "# (2) Project quantized model’s hidden states through original model’s head\n",
    "project_hidden_through_other_head(\n",
    "    projector_model=model_orig,\n",
    "    hidden_file=f\"saved_data/hidden_acts/m_quant_batch{batch_num}.pt\",\n",
    "    batch_num=batch_num,\n",
    "    save_prefix=\"saved_data/projections\",\n",
    "    direction_tag=\"quant_orig\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_projection_comparison_single_batch(\n",
    "    file_A,\n",
    "    file_B,\n",
    "    tokenizer,\n",
    "    batch_num=0,\n",
    "    save_prefix=\"saved_data/head_semantics/semantics_\",\n",
    "    topk_levels=(1, 5, 10, 20),\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare *already projected* logits from two models (e.g., orig→quant vs quant→orig).\n",
    "    Computes:\n",
    "      • cosine & L2 distances between logits (sequence-level + per-position)\n",
    "      • Gini coefficients (sequence-level + per-position)\n",
    "      • top-k Jaccard overlaps (per-position + sequence-level)\n",
    "      • decoded top-k tokens for interpretability\n",
    "    \"\"\"\n",
    "\n",
    "    Path(save_prefix).parent.mkdir(parents=True, exist_ok=True)\n",
    "    save_path = f\"{save_prefix}batch{int(batch_num):03d}.pt\"\n",
    "\n",
    "    rows_A = torch.load(file_A, map_location=device, weights_only=False)\n",
    "    rows_B = torch.load(file_B, map_location=device, weights_only=False)\n",
    "\n",
    "    # --- Helper for Gini coefficient ---\n",
    "    def gini_coefficient(x):\n",
    "        \"\"\"Compute Gini per position (for logits → probs).\"\"\"\n",
    "        probs = torch.softmax(x, dim=-1)\n",
    "        n = probs.shape[-1]\n",
    "        sorted_p, _ = torch.sort(probs, dim=-1)\n",
    "        cum_p = torch.cumsum(sorted_p, dim=-1)\n",
    "        gini = 1 - 2 * cum_p.sum(dim=-1) / (n * cum_p[..., -1]) + 1 / n\n",
    "        return gini\n",
    "\n",
    "    index_B = {(r[\"prompt_id\"], r[\"layer_name\"]): r for r in rows_B}\n",
    "    results = []\n",
    "\n",
    "    for rA in tqdm(rows_A, desc=f\"[batch {batch_num}] Comparing projected logits\"):\n",
    "        key = (rA[\"prompt_id\"], rA[\"layer_name\"])\n",
    "        rB = index_B.get(key)\n",
    "        if not rB or \"logits_crossproj\" not in rA or \"logits_crossproj\" not in rB:\n",
    "            continue\n",
    "\n",
    "        logits_A = rA[\"logits_crossproj\"].to(device)   # [T, V]\n",
    "        logits_B = rB[\"logits_crossproj\"].to(device)   # [T, V]\n",
    "        vocab_dim = logits_A.shape[-1]\n",
    "        seq_len = logits_A.shape[0]\n",
    "\n",
    "        # --- Per-position metrics ---\n",
    "        cos_pos = F.cosine_similarity(logits_A, logits_B, dim=-1)           # [T]\n",
    "        l2_pos = torch.norm(logits_A - logits_B, dim=-1)                    # [T]\n",
    "        gini_A_pos = gini_coefficient(logits_A)                             # [T]\n",
    "        gini_B_pos = gini_coefficient(logits_B)                             # [T]\n",
    "        delta_gini_pos = gini_B_pos - gini_A_pos\n",
    "\n",
    "        # --- Sequence-level summaries ---\n",
    "        cos_seq = cos_pos.mean().item()\n",
    "        l2_seq = l2_pos.mean().item()\n",
    "        gini_A_seq = gini_A_pos.mean().item()\n",
    "        gini_B_seq = gini_B_pos.mean().item()\n",
    "        delta_gini_seq = gini_B_seq - gini_A_seq\n",
    "\n",
    "        # --- Top-k Jaccard overlaps ---\n",
    "        jaccard_pos_dict = {}\n",
    "        jaccard_seq_dict = {}\n",
    "        decoded_shift = {}\n",
    "\n",
    "        for k in topk_levels:\n",
    "            k_safe = min(k, vocab_dim)\n",
    "            jaccard_pos = []\n",
    "            for t in range(seq_len):\n",
    "                topA_t = torch.topk(logits_A[t], k_safe).indices.tolist()\n",
    "                topB_t = torch.topk(logits_B[t], k_safe).indices.tolist()\n",
    "                inter = len(set(topA_t) & set(topB_t))\n",
    "                union = len(set(topA_t) | set(topB_t))\n",
    "                jaccard_pos.append(inter / union if union > 0 else 0.0)\n",
    "            jaccard_pos_tensor = torch.tensor(jaccard_pos)\n",
    "            jaccard_pos_dict[f\"jaccard_top{k}_pos\"] = jaccard_pos_tensor.cpu()\n",
    "            jaccard_seq_dict[f\"jaccard_top{k}_seq\"] = jaccard_pos_tensor.mean().item()\n",
    "\n",
    "            # Decode top-k tokens for mean logits (for readability)\n",
    "            mean_A = logits_A.mean(dim=0)\n",
    "            mean_B = logits_B.mean(dim=0)\n",
    "            topA_k = torch.topk(mean_A, k_safe).indices.tolist()\n",
    "            topB_k = torch.topk(mean_B, k_safe).indices.tolist()\n",
    "            decoded_shift[f\"top{k}_A\"] = [tokenizer.decode([i]) for i in topA_k]\n",
    "            decoded_shift[f\"top{k}_B\"] = [tokenizer.decode([i]) for i in topB_k]\n",
    "\n",
    "        # --- Aggregate everything ---\n",
    "        result_entry = {\n",
    "            \"prompt_id\": rA[\"prompt_id\"],\n",
    "            \"batch_index\": rA.get(\"batch_index\", batch_num),\n",
    "            \"layer_index\": rA[\"layer_index\"],\n",
    "            \"layer_name\": rA[\"layer_name\"],\n",
    "            \"input_ids\": rA.get(\"input_ids\"),\n",
    "            \"target_ids\": rA.get(\"target_ids\"),\n",
    "\n",
    "            # --- sequence-level summary ---\n",
    "            \"cosine_seq\": cos_seq,\n",
    "            \"l2_seq\": l2_seq,\n",
    "            \"gini_A_seq\": gini_A_seq,\n",
    "            \"gini_B_seq\": gini_B_seq,\n",
    "            \"delta_gini_seq\": delta_gini_seq,\n",
    "            **jaccard_seq_dict,\n",
    "\n",
    "            # --- detailed per-position tensors ---\n",
    "            \"cosine_pos\": cos_pos.cpu(),\n",
    "            \"l2_pos\": l2_pos.cpu(),\n",
    "            \"gini_A_pos\": gini_A_pos.cpu(),\n",
    "            \"gini_B_pos\": gini_B_pos.cpu(),\n",
    "            \"delta_gini_pos\": delta_gini_pos.cpu(),\n",
    "            **jaccard_pos_dict,\n",
    "\n",
    "            \"decoded_shift\": decoded_shift,\n",
    "        }\n",
    "\n",
    "        results.append(result_entry)\n",
    "        del logits_A, logits_B\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    torch.save(results, save_path)\n",
    "    print(f\"[saved] → {save_path} ({len(results)} entries)\")\n",
    "    return results\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "orig_tokenizer = AutoTokenizer.from_pretrained(\"Models/LLaMA3Instruct\")\n",
    "\n",
    "batch_num = \"002\"\n",
    "\n",
    "run_projection_comparison_single_batch(\n",
    "    file_A=f\"saved_data/projections/proj_orig_quant_batch{batch_num}.pt\",\n",
    "    file_B=f\"saved_data/projections/proj_quant_orig_batch{batch_num}.pt\",\n",
    "    tokenizer=orig_tokenizer,\n",
    "    batch_num=batch_num,\n",
    "    save_prefix=\"saved_data/projection_comparisons/semantics_\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit-lens-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
