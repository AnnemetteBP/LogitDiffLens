{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6294ae",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Notebook for Logit Lens and Hidden Acts ======\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, DownloadMode\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from old_lens.mi_utils.quant_configs.bnb_configs import load_bnb_in_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f24fe2",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Models & Dataset =============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340878a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\LogitLensData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:1000]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")\n",
    "\n",
    "nq_queries = nq_dataset['train']['query']\n",
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_1000 = nq_queries[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_500 = nq_queries[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Models(Enum):\n",
    "    LAIN8B = \"Models/LLaMA3Instruct\"\n",
    "    HF100B = \"Models/HF1BitLLM100Btokens\"\n",
    "\n",
    "\n",
    "class Names(Enum):\n",
    "    LAIN8B = \"Meta-Llama-3-8B-Instruct-fp\"\n",
    "    HF100B = \"Llama3-8B-1.58-100B-tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tok(\n",
    "    model_name: str,\n",
    "    low_cpu_mem_usage: bool = True,\n",
    "    local_files_only: bool = True,\n",
    "    device_map: str = \"cpu\",\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    load_in_8bit: bool = False\n",
    "):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        return_dict=True,\n",
    "        output_attentions=True,\n",
    "        low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "        local_files_only=local_files_only,\n",
    "        device_map=device_map,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        torch_dtype=dtype,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    return model, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig, orig_tokenizer = load_model_and_tok(Models.LAIN8B.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = orig_tokenizer.eos_token_id\n",
    "bos_token_id = orig_tokenizer.bos_token_id\n",
    "print(f\"eos: {eos_token_id}\\nbos: {bos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50589c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8bit, orig_tokenizer = load_model_and_tok(Models.LAIN8B.value, dtype=torch.float16, load_in_8bit=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit, orig_tokenizer = load_bnb_in_4bit(Models.LAIN8B.value, double_quant=False, dtype=torch.float16, device_map=\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant, quant_tokenizer = load_model_and_tok(Models.HF100B.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdeb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for q in nq_500:\n",
    "    ids = orig_tokenizer.encode(q, add_special_tokens=True)\n",
    "    lengths.append(len(ids))\n",
    "\n",
    "print(\"=== Token Length Statistics (LLaMA-3-8B-Instruct tokenizer) ===\")\n",
    "print(f\"Samples analyzed: {len(lengths)}\")\n",
    "print(f\"Mean length:       {np.mean(lengths):.2f}\")\n",
    "print(f\"Median length:     {np.median(lengths):.0f}\")\n",
    "print(f\"90th percentile:   {np.percentile(lengths, 90):.0f}\")\n",
    "print(f\"95th percentile:   {np.percentile(lengths, 95):.0f}\")\n",
    "print(f\"Max observed len:  {np.max(lengths)}\")\n",
    "print(f\"Min observed len:  {np.min(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "sig = inspect.signature(model_orig.model.layers[0].forward)\n",
    "print(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_weights(model, model_name, save_dir=\"saved_data/weights\", save_parquet=True):\n",
    "    print(f\"\\nðŸ”¹ Extracting weights from {model_name} ...\")\n",
    "\n",
    "    out_dir = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    records = []  \n",
    "\n",
    "    for name, param in tqdm(model.named_parameters(), desc=f\"{model_name} weights\"):\n",
    "        if not param.requires_grad:\n",
    "            continue  \n",
    "\n",
    "        w = param.detach().to(torch.float32).cpu()\n",
    "        fname = name.replace(\".\", \"_\") + \".pt\"\n",
    "        fpath = os.path.join(out_dir, fname)\n",
    "        torch.save(w, fpath)\n",
    "\n",
    "        layer_idx = -1\n",
    "        if \"layers.\" in name:\n",
    "            try:\n",
    "                layer_idx = int(name.split(\"layers.\")[1].split(\".\")[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        records.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"param_name\": name,\n",
    "            \"layer_index\": layer_idx,\n",
    "            \"shape\": tuple(w.shape),\n",
    "            \"numel\": w.numel(),\n",
    "            \"path\": fpath\n",
    "        })\n",
    "\n",
    "    print(f\"Saved all weights for {model_name} to {out_dir}\")\n",
    "    print(f\"Total params: {sum(r['numel'] for r in records):,}\")\n",
    "    print(f\"Example tensor: {records[0]['param_name']}  shape={records[0]['shape']}\")\n",
    "\n",
    "    if save_parquet:\n",
    "        df_meta = pd.DataFrame(records)\n",
    "        pq_path = os.path.join(save_dir, f\"{model_name}_weights_meta.parquet\")\n",
    "        df_meta.to_parquet(pq_path, index=False)\n",
    "        print(f\"Saved weight metadata summary â†’ {pq_path}\")\n",
    "\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_and_save_weights(model_orig, \"m_orig\")\n",
    "#extract_and_save_weights(model_quant, \"m_quant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b729d26",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Logit Lens with Normaliztion =================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37206e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"MatMul8bitLt: inputs will be cast\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Normalization modes\n",
    "# ============================================================\n",
    "def apply_normalization(x, model, normalize_mode=\"raw\", block=None, layer_index=None):\n",
    "    x = x.to(torch.float32) \n",
    "    \n",
    "    if normalize_mode == \"raw\":\n",
    "        return x\n",
    "\n",
    "    elif normalize_mode == \"unit_rms\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            eps = 1e-5\n",
    "            return x / (x.pow(2).mean(dim=-1, keepdim=True).add(eps).sqrt())\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    elif normalize_mode == \"rms_layer\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            return block.post_attention_layernorm(x.to(torch.float32))\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    elif normalize_mode == \"norm_rms\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            return model.model.norm(x).to(torch.float32)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization_mode: {normalize_mode}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper: causal + padding mask for LLaMA blocks\n",
    "# ============================================================\n",
    "def build_full_attention_mask(input_ids, attention_mask, device, model=None):\n",
    "    bsz, seq_len = input_ids.shape\n",
    "\n",
    "    causal = torch.triu(\n",
    "        torch.ones((seq_len, seq_len), device=device, dtype=torch.bool), diagonal=1\n",
    "    ).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        padding_mask = torch.zeros((bsz, 1, 1, seq_len), device=device, dtype=torch.bool)\n",
    "    else:\n",
    "        padding_mask = (attention_mask[:, None, None, :] == 0)\n",
    "\n",
    "    full = causal | padding_mask\n",
    "\n",
    "    attn_impl = getattr(getattr(model, \"config\", None), \"attn_implementation\", None)\n",
    "    attn_impl = attn_impl or \"eager\" \n",
    "\n",
    "    if attn_impl in [\"flash_attention_2\", \"sdpa\"]:\n",
    "        return full.to(torch.bool)\n",
    "    else:\n",
    "        return full.to(torch.float32) * -1e9\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Collector with multiple normalization variants + attention weights (optional storage)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def collect_logit_lens_full(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    batch_index=0,\n",
    "    max_len=17,\n",
    "    device=None,\n",
    "    clamp_logits=False,         \n",
    "    clamp_value=100.0,         \n",
    "    save_path=None,\n",
    "    collect_attn=True,\n",
    "    save_attn=True,\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "):\n",
    "\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    bnb_layer_types = (\"Linear4bit\", \"Linear8bitLt\")\n",
    "    is_quantized = any(any(name in type(m).__name__ for name in bnb_layer_types)\n",
    "                       for m in model.modules())\n",
    "\n",
    "    if is_quantized:\n",
    "        try:\n",
    "            first_param_device = next(model.parameters()).device\n",
    "        except StopIteration:\n",
    "            first_param_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[info] Detected quantized model â†’ device {first_param_device}\")\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = model.to(device).eval()\n",
    "\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # Tokenization\n",
    "    # ============================================================\n",
    "    encoded = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    for p in prompts:\n",
    "        ids = tokenizer.encode(p, add_special_tokens=False)\n",
    "        content = ids[: max_len - 2]\n",
    "        ids = torch.tensor([bos_id] + content + [eos_id], dtype=torch.long)\n",
    "        if len(ids) < max_len:\n",
    "            ids = F.pad(ids, (0, max_len - len(ids)), value=pad_id)\n",
    "        encoded.append(ids)\n",
    "\n",
    "    input_ids = torch.stack(encoded, dim=0).to(device)\n",
    "\n",
    "    # ============================================================\n",
    "    # Build attention mask (stop after first EOS)\n",
    "    # ============================================================\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        eos_positions = (ids == eos_id).nonzero(as_tuple=True)[0]\n",
    "        if len(eos_positions) > 0:\n",
    "            eos_pos = eos_positions[0].item()\n",
    "            attention_mask[i, eos_pos + 1:] = 0\n",
    "\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    full_mask = build_full_attention_mask(input_ids, attention_mask, device)\n",
    "    #print(f\"[mask] dtype={full_mask.dtype}, shape={tuple(full_mask.shape)}, example={full_mask.flatten()[0].item()}\")\n",
    "    print(\n",
    "        f\"[mask] dtype={full_mask.dtype}, shape={tuple(full_mask.shape)}, \"\n",
    "        f\"min={full_mask.min().item()}, max={full_mask.max().item()}, \"\n",
    "        f\"unique={torch.unique(full_mask)}\"\n",
    "    )\n",
    "    assert (full_mask == 0).sum() < full_mask.numel(), \"Mask seems to contain only zeros â€” check logic!\"\n",
    "\n",
    "    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "    vocab_size = model.lm_head.out_features\n",
    "\n",
    "    print(f\"[info] Tokenized {batch_size} prompts | seq_len={seq_len}\")\n",
    "    print(f\"[info] Collecting from {len(model.model.layers)} layers | quantized={is_quantized}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Projection helper (with consistent upcasting and sanitization)\n",
    "    # ============================================================\n",
    "    def project(x):\n",
    "        x_fp32 = x.to(torch.float32)\n",
    "\n",
    "        head_dtype = next(model.lm_head.parameters()).dtype\n",
    "        x_cast = x_fp32.to(head_dtype)\n",
    "\n",
    "        logits = model.lm_head(x_cast)\n",
    "\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if clamp_logits and is_quantized:\n",
    "            logits = logits.clamp(-clamp_value, clamp_value)\n",
    "\n",
    "        return logits.to(torch.float32)\n",
    "\n",
    "    # ============================================================\n",
    "    # Collection containers\n",
    "    # ============================================================\n",
    "    rows = []\n",
    "    all_hidden, all_logits, all_attn = {}, {}, {}\n",
    "\n",
    "    # ============================================================\n",
    "    # Embedding layer\n",
    "    # ============================================================\n",
    "    x = model.model.embed_tokens(input_ids).to(torch.float32)\n",
    "    hidden_variants = {mode: apply_normalization(x.clone(), model, mode, layer_index=-1)\n",
    "                       for mode in norm_modes}\n",
    "    logits_variants = {mode: project(hidden_variants[mode]) for mode in norm_modes}\n",
    "\n",
    "    for mode in norm_modes:\n",
    "        all_hidden[f\"embed_tokens_{mode}\"] = hidden_variants[mode].cpu()\n",
    "        all_logits[f\"embed_tokens_{mode}\"] = logits_variants[mode].cpu()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rows.append({\n",
    "            \"prompt_id\": i,\n",
    "            \"prompt_text\": prompts[i],\n",
    "            \"batch_index\": batch_index,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_index\": -1,\n",
    "            \"layer_name\": \"embed_tokens\",\n",
    "            \"input_ids\": input_ids[i].cpu(),\n",
    "            \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "            \"attention_mask\": attention_mask[i].cpu(),\n",
    "            **{f\"hidden_{m}\": hidden_variants[m][i, :-1].cpu() for m in norm_modes},\n",
    "            **{f\"logits_{m}\": logits_variants[m][i, :-1].cpu() for m in norm_modes},\n",
    "        })\n",
    "\n",
    "    # ============================================================\n",
    "    # Transformer layers\n",
    "    # ============================================================\n",
    "    for li, block in enumerate(model.model.layers):\n",
    "        out = block(\n",
    "            x, position_ids=position_ids,\n",
    "            attention_mask=full_mask,\n",
    "            output_attentions=collect_attn,\n",
    "        )\n",
    "        x = out[0]\n",
    "        attn = out[1] if collect_attn else None\n",
    "\n",
    "        layer_output = x.detach().clone().to(torch.float32)\n",
    "        hidden_variants = {\n",
    "            mode: apply_normalization(layer_output.clone(), model, mode, block=block, layer_index=li)\n",
    "            for mode in norm_modes\n",
    "        }\n",
    "        logits_variants = {mode: project(hidden_variants[mode]) for mode in norm_modes}\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            hidden_variants[mode] = hidden_variants[mode][:, :-1, :]\n",
    "            logits_variants[mode] = logits_variants[mode][:, :-1, :]\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            all_hidden[f\"layer.{li}_{mode}\"] = hidden_variants[mode].cpu()\n",
    "            all_logits[f\"layer.{li}_{mode}\"] = logits_variants[mode].cpu()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            record = {\n",
    "                \"prompt_id\": i,\n",
    "                \"prompt_text\": prompts[i],\n",
    "                \"batch_index\": batch_index,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"layer_index\": li,\n",
    "                \"layer_name\": f\"layer.{li}\",\n",
    "                \"input_ids\": input_ids[i].cpu(),\n",
    "                \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "                \"attention_mask\": attention_mask[i].cpu(),\n",
    "                **{f\"hidden_{m}\": hidden_variants[m][i].cpu() for m in norm_modes},\n",
    "                **{f\"logits_{m}\": logits_variants[m][i].cpu() for m in norm_modes},\n",
    "            }\n",
    "            if save_attn and attn is not None:\n",
    "                record[\"attn\"] = attn[i].cpu()\n",
    "            rows.append(record)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # ============================================================\n",
    "    # Final RMSNorm output\n",
    "    # ============================================================\n",
    "    x = model.model.norm(x.to(torch.float32))\n",
    "    h = x\n",
    "    l = project(h)\n",
    "    h, l = h[:, :-1, :], l[:, :-1, :]\n",
    "\n",
    "    all_hidden[\"output_true\"] = h.cpu()\n",
    "    all_logits[\"output_true\"] = l.cpu()\n",
    "\n",
    "    for mode in norm_modes:\n",
    "        all_hidden[f\"output_{mode}\"] = h.cpu()\n",
    "        all_logits[f\"output_{mode}\"] = l.cpu()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rows.append({\n",
    "            \"prompt_id\": i,\n",
    "            \"prompt_text\": prompts[i],\n",
    "            \"batch_index\": batch_index,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_index\": len(model.model.layers),\n",
    "            \"layer_name\": \"output\",\n",
    "            \"input_ids\": input_ids[i].cpu(),\n",
    "            \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "            \"attention_mask\": attention_mask[i].cpu(),\n",
    "            **{f\"hidden_{m}\": h[i].cpu() for m in norm_modes},\n",
    "            **{f\"logits_{m}\": l[i].cpu() for m in norm_modes},\n",
    "        })\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Save and finish \n",
    "    # ============================================================\n",
    "    if save_path:\n",
    "        torch.save(rows, save_path)\n",
    "        print(f\"[saved] Logit-lens data â†’ {save_path}\")\n",
    "\n",
    "\n",
    "    print(f\"[info] Model has {len(model.model.layers)} transformer blocks (plus embedding + output).\")\n",
    "\n",
    "    return rows, all_hidden, all_logits, all_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8653a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_logit_lens_in_batches(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    all_prompts,\n",
    "    batch_size=20,\n",
    "    save_prefix=\"logitlens_batch\",\n",
    "    max_len=17,\n",
    "    normalize_mode=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    device=None,\n",
    "    clamp_logits=False,\n",
    "    collect_attn=False,\n",
    "    save_attn=False \n",
    "):\n",
    "\n",
    "    num_batches = (len(all_prompts) + batch_size - 1) // batch_size\n",
    "    print(f\"[run] Processing {len(all_prompts)} prompts in {num_batches} batches of {batch_size}\")\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Running logit lens batches\"):\n",
    "        start = batch_idx * batch_size\n",
    "        end = min((batch_idx + 1) * batch_size, len(all_prompts))\n",
    "        batch_prompts = all_prompts[start:end]\n",
    "\n",
    "        save_path = f\"{save_prefix}_batch{batch_idx:03d}.pt\"\n",
    "\n",
    "        print(f\"\\n[batch {batch_idx+1}/{num_batches}] {len(batch_prompts)} prompts â†’ {save_path}\")\n",
    "\n",
    "        try:\n",
    "            rows, hidden_dict, logits_dict, all_attn = collect_logit_lens_full(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                prompts=batch_prompts,\n",
    "                max_len=max_len,\n",
    "                device=device,\n",
    "                norm_modes=normalize_mode,\n",
    "                save_path=save_path,\n",
    "                clamp_logits=clamp_logits,\n",
    "                collect_attn=collect_attn,\n",
    "                save_attn=save_attn,\n",
    "            )\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[error] Batch {batch_idx} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        del rows, hidden_dict, logits_dict, all_attn, batch_prompts\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\n[done] All batches processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logit_lens_in_batches(\n",
    "    model=model_8bit,\n",
    "    tokenizer=orig_tokenizer,\n",
    "    all_prompts=nq_500,\n",
    "    batch_size=10,\n",
    "    max_len=17,\n",
    "    normalize_mode=(\"raw\", \"unit_rms\", \"norm_rms\"), \n",
    "    save_prefix=\"saved_data/lens_data/m_8bit/m_8bit_modes\",\n",
    "    device=\"cpu\",\n",
    "    clamp_logits=False,\n",
    "    collect_attn=False,\n",
    "    save_attn=False  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data = torch.load(\"saved_data/lens_data/m_orig/m_orig_modes_batch000.pt\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39183c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df = pd.DataFrame(ll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591013aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df[\"layer_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ef638",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_tokenizer.decode([128000]))\n",
    "print(orig_tokenizer.decode([128009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bee5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_tokenizer.decode([128000, 9906, 1917, 128009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37103286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_tokenizer.bos_token_id, orig_tokenizer.eos_token_id)\n",
    "print(model_8bit.config.bos_token_id, model_8bit.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = orig_tokenizer.encode(\"Hello world\", add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f33a9",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# TopK Comparison ==============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Cache BOSâ€“EOS valid indices per prompt\n",
    "# ============================================================\n",
    "_mask_cache = {}\n",
    "\n",
    "\n",
    "def preprocess_metrics(metrics, lens_type=\"raw\"):\n",
    "    \"\"\"Trim logits/hidden/targets to BOSâ€“EOS span, cached per prompt_id.\"\"\"\n",
    "    processed = []\n",
    "    for row in metrics:\n",
    "        pid = row.get(\"prompt_id\")\n",
    "        logits = row.get(f\"logits_{lens_type}\")\n",
    "        hidden = row.get(f\"hidden_{lens_type}\")\n",
    "        attn_mask = row.get(\"attention_mask\")\n",
    "        targets = row.get(\"target_ids\")\n",
    "\n",
    "        if logits is None or targets is None or attn_mask is None:\n",
    "            continue\n",
    "\n",
    "        # reuse cached BOSâ€“EOS mask\n",
    "        if pid in _mask_cache:\n",
    "            valid_pos = _mask_cache[pid]\n",
    "        else:\n",
    "            if not isinstance(attn_mask, torch.Tensor):\n",
    "                attn_mask = torch.tensor(attn_mask)\n",
    "            if attn_mask.ndim == 4:\n",
    "                attn_mask = attn_mask[:, 0, 0, :]\n",
    "            elif attn_mask.ndim == 1:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "            mask_1d = attn_mask[0]\n",
    "            true_pos = mask_1d.nonzero(as_tuple=True)[0]\n",
    "            if true_pos.numel() < 2:\n",
    "                continue\n",
    "\n",
    "            bos_idx, eos_idx = int(true_pos[0]), int(true_pos[-1])\n",
    "            eval_mask = torch.zeros_like(mask_1d, dtype=torch.bool)\n",
    "            if eos_idx > bos_idx + 1:\n",
    "                eval_mask[bos_idx + 1:eos_idx] = True\n",
    "            valid_pos = eval_mask.nonzero(as_tuple=True)[0]\n",
    "            if valid_pos.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            _mask_cache[pid] = valid_pos\n",
    "\n",
    "        if logits.ndim == 2:\n",
    "            logits = logits.unsqueeze(0)\n",
    "        if targets.ndim == 1:\n",
    "            targets = targets.unsqueeze(0)\n",
    "        if hidden is not None and hidden.ndim == 2:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "\n",
    "        logits_trim = logits[:, valid_pos, :].contiguous()\n",
    "        targets_trim = targets[:, valid_pos].contiguous()\n",
    "        hidden_trim = hidden[:, valid_pos, :].contiguous() if hidden is not None else None\n",
    "\n",
    "        row_out = dict(row)\n",
    "        row_out[f\"logits_{lens_type}\"] = logits_trim\n",
    "        row_out[f\"hidden_{lens_type}\"] = hidden_trim\n",
    "        row_out[\"target_ids\"] = targets_trim\n",
    "        processed.append(row_out)\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute metrics and top-k similarities for A vs B\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def compute_topk(\n",
    "    metrics_A,\n",
    "    metrics_B,\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk=(1, 5, 10, 20),\n",
    "    device=\"cpu\",\n",
    "    eps = 1e-12,\n",
    "    output_dir=\"logs/new_summary\",\n",
    "    run_name=None,\n",
    "    batch_idx=None,\n",
    "    debug=False,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- safe clean helper ---\n",
    "    \"\"\"def clean(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().to(\"cpu\", copy=False).float()\n",
    "            x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return x.tolist()\n",
    "        return x\"\"\"\n",
    "    def clean(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().to(\"cpu\", copy=False).float()\n",
    "            x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return x.view(-1).tolist()\n",
    "        elif isinstance(x, (list, np.ndarray)):\n",
    "            return [float(v) if np.isfinite(v) else 0.0 for v in x]\n",
    "        else:\n",
    "            return float(x) if np.isfinite(x) else 0.0\n",
    "\n",
    "    # --- preprocess all normalization modes ---\n",
    "    proc_modes = {\n",
    "        m: (preprocess_metrics(metrics_A, m), preprocess_metrics(metrics_B, m))\n",
    "        for m in norm_modes\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[mask cache] built for {len(_mask_cache)} prompt_ids\")\n",
    "        for pid, mask in list(_mask_cache.items())[:10]: \n",
    "            print(f\"  prompt_id={pid:<5} â†’ len={len(mask)}  positions={mask.tolist()}\")\n",
    "        lengths = [len(v) for v in _mask_cache.values()]\n",
    "        print(f\"  unique mask lengths: {sorted(set(lengths))}\")\n",
    "\n",
    "    rec_map = {}\n",
    "\n",
    "    # --- main computation ---\n",
    "    for mode in norm_modes:\n",
    "        A_trim, B_trim = proc_modes[mode]\n",
    "        if not A_trim or not B_trim:\n",
    "            if debug:\n",
    "                print(f\"[skip] no valid rows for mode={mode}\")\n",
    "            continue\n",
    "\n",
    "        for rA in A_trim:\n",
    "            pid, lid = rA[\"prompt_id\"], rA[\"layer_index\"]\n",
    "            rB = next((r for r in B_trim if r.get(\"prompt_id\") == pid and r.get(\"layer_index\") == lid), None)\n",
    "            if rB is None:\n",
    "                continue\n",
    "\n",
    "            key = (pid, lid)\n",
    "            record = rec_map.setdefault(\n",
    "                key,\n",
    "                dict(\n",
    "                    prompt_id=pid,\n",
    "                    batch_index=rA.get(\"batch_index\", batch_idx),\n",
    "                    layer_index=lid,\n",
    "                    layer_name=rA.get(\"layer_name\"),\n",
    "                    prompt_text=rA.get(\"prompt_text\"),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # --- tensor setup ---\n",
    "            logits_A, logits_B = rA[f\"logits_{mode}\"], rB[f\"logits_{mode}\"]\n",
    "            targets = rA[\"target_ids\"]\n",
    "            hidden_A, hidden_B = rA.get(f\"hidden_{mode}\"), rB.get(f\"hidden_{mode}\")\n",
    "\n",
    "            if logits_A.ndim == 2: logits_A = logits_A.unsqueeze(0)\n",
    "            if logits_B.ndim == 2: logits_B = logits_B.unsqueeze(0)\n",
    "            if targets.ndim == 1: targets = targets.unsqueeze(0)\n",
    "\n",
    "            logits_A, logits_B = logits_A.to(device).float(), logits_B.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            if hidden_A is not None: hidden_A = hidden_A.to(device).float()\n",
    "            if hidden_B is not None: hidden_B = hidden_B.to(device).float()\n",
    "\n",
    "            L = targets.size(1)\n",
    "            logits_A, logits_B = logits_A[:, :L, :], logits_B[:, :L, :]\n",
    "            if hidden_A is not None: hidden_A = hidden_A[:, :L, :]\n",
    "            if hidden_B is not None: hidden_B = hidden_B[:, :L, :]\n",
    "\n",
    "            vocab = max(logits_A.size(-1), logits_B.size(-1))\n",
    "            if logits_A.size(-1) < vocab:\n",
    "                logits_A = F.pad(logits_A, (0, vocab - logits_A.size(-1)))\n",
    "            if logits_B.size(-1) < vocab:\n",
    "                logits_B = F.pad(logits_B, (0, vocab - logits_B.size(-1)))\n",
    "\n",
    "            # --- probability space ---\n",
    "            logp_A = F.log_softmax(logits_A, dim=-1)\n",
    "            logp_B = F.log_softmax(logits_B, dim=-1)\n",
    "\n",
    "            # Compute probabilities cleanly and normalize to avoid drift\n",
    "            pA = torch.exp(logp_A)\n",
    "            pB = torch.exp(logp_B)\n",
    "\n",
    "            #pA = pA / (pA.sum(-1, keepdim=True) + eps)\n",
    "            #pB = pB / (pB.sum(-1, keepdim=True) + eps)\n",
    "\n",
    "            # unpack first dimension (batch=1)\n",
    "            logp_A, logp_B = logp_A[0], logp_B[0]\n",
    "            pA, pB, tgt = pA[0], pB[0], targets[0]\n",
    "\n",
    "            # --- basic metrics ---\n",
    "            kl_ab = torch.sum(pA * (logp_A - logp_B), dim=-1).clamp_min(0.0)\n",
    "            kl_ba = torch.sum(pB * (logp_B - logp_A), dim=-1).clamp_min(0.0)\n",
    "            js_div = 0.5 * (kl_ab + kl_ba)\n",
    "            js_dist = torch.sqrt(torch.clamp(js_div, min=0.0) + eps)\n",
    "\n",
    "            # move to cpu and clean\n",
    "            kl_ab = clean(kl_ab)\n",
    "            kl_ba = clean(kl_ba)\n",
    "            js_div = clean(js_div)\n",
    "            js_dist = clean(js_dist)\n",
    "\n",
    "            # TVD and entropy\n",
    "            tvd = clean(0.5 * torch.sum(torch.abs(pA - pB), dim=-1))\n",
    "            entropy_A = clean(-torch.sum(pA * logp_A, dim=-1))\n",
    "            entropy_B = clean(-torch.sum(pB * logp_B, dim=-1))\n",
    "\n",
    "\n",
    "            # === per-position L2 ===\n",
    "            if hidden_A is not None and hidden_B is not None:\n",
    "                cosine = clean(F.cosine_similarity(hidden_A[0], hidden_B[0], dim=-1))\n",
    "                l2_tensor = torch.sqrt(torch.sum((hidden_A[0] - hidden_B[0]) ** 2, dim=-1))\n",
    "                l2 = clean(l2_tensor)\n",
    "                #if debug:\n",
    "                    #print(f\"[debug] pid={pid} lid={lid} L2 shape={l2_tensor.shape} mean={l2_tensor.mean().item():.3g}\")\n",
    "            else:\n",
    "                cosine, l2 = [0.0] * L, [0.0] * L\n",
    "\n",
    "            # log-likelihood difference for ground-truth tokens\n",
    "            logp_A_gt = torch.gather(logp_A, -1, tgt.unsqueeze(-1)).squeeze(-1)\n",
    "            logp_B_gt = torch.gather(logp_B, -1, tgt.unsqueeze(-1)).squeeze(-1)\n",
    "            logp_diff = logp_A_gt - logp_B_gt\n",
    "\n",
    "            # probability assignments for the same ground-truth tokens\n",
    "            p_A_gt = torch.exp(logp_A_gt)\n",
    "            p_B_gt = torch.exp(logp_B_gt)\n",
    "            p_diff = p_A_gt - p_B_gt\n",
    "\n",
    "            # clean everything for storage\n",
    "            logp_A_gt = clean(logp_A_gt)\n",
    "            logp_B_gt = clean(logp_B_gt)\n",
    "            logp_diff = clean(logp_diff)\n",
    "            p_A_gt = clean(p_A_gt)\n",
    "            p_B_gt = clean(p_B_gt)\n",
    "            p_diff = clean(p_diff)\n",
    "\n",
    "            # === per-position cross-entropy and perplexity ===\n",
    "            ce_A_pos = F.cross_entropy(\n",
    "                logits_A.view(-1, vocab), targets.view(-1), reduction=\"none\"\n",
    "            ).view(targets.shape)\n",
    "            ce_B_pos = F.cross_entropy(\n",
    "                logits_B.view(-1, vocab), targets.view(-1), reduction=\"none\"\n",
    "            ).view(targets.shape)\n",
    "\n",
    "            # Compute per-position perplexity\n",
    "            ppl_A_pos = torch.exp(ce_A_pos)\n",
    "            ppl_B_pos = torch.exp(ce_B_pos)\n",
    "\n",
    "            # Convert everything safely to Python lists\n",
    "            ppl_A_pos = clean(ppl_A_pos)\n",
    "            ppl_B_pos = clean(ppl_B_pos)\n",
    "\n",
    "            # Compute per-position difference as list comprehension to avoid tensor ops\n",
    "            ppl_diff = [a - b for a, b in zip(ppl_A_pos, ppl_B_pos)]\n",
    "\n",
    "            record.update(\n",
    "                {\n",
    "                    f\"kl_ab_{mode}\": kl_ab,\n",
    "                    f\"kl_ba_{mode}\": kl_ba,\n",
    "                    f\"js_div_{mode}\": js_div,\n",
    "                    f\"js_dist_{mode}\": js_dist,\n",
    "                    f\"tvd_{mode}\": tvd,\n",
    "                    f\"entropy_A_{mode}\": entropy_A,\n",
    "                    f\"entropy_B_{mode}\": entropy_B,\n",
    "                    f\"cosine_sim_{mode}\": cosine,\n",
    "                    f\"l2_dist_{mode}\": l2,   \n",
    "                    f\"logp_diff_{mode}\": logp_diff,\n",
    "                    f\"logp_A_gt_{mode}\": logp_A_gt,\n",
    "                    f\"logp_B_gt_{mode}\": logp_B_gt,\n",
    "                    f\"logp_diff_{mode}\": logp_diff,\n",
    "                    f\"p_A_gt_{mode}\": p_A_gt,\n",
    "                    f\"p_B_gt_{mode}\": p_B_gt,\n",
    "                    f\"p_diff_{mode}\": p_diff,\n",
    "                    f\"ppl_A_{mode}\": ppl_A_pos,\n",
    "                    f\"ppl_B_{mode}\": ppl_B_pos,\n",
    "                    f\"ppl_diff_{mode}\": ppl_diff,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # === top-k metrics ===\n",
    "            fams = {key: {} for key in [\n",
    "                f\"acc_A_{mode}\", f\"acc_B_{mode}\",\n",
    "                f\"jaccard_{mode}\", f\"disagree_set_{mode}\", f\"agree_set_{mode}\",\n",
    "                f\"agree_correct_{mode}\", f\"disagree_correct_{mode}\",\n",
    "                f\"agree_wrong_{mode}\", f\"prob_overlap_{mode}\",\n",
    "                f\"top_pred_ids_A_{mode}\", f\"top_pred_vals_A_{mode}\",\n",
    "                f\"top_pred_ids_B_{mode}\", f\"top_pred_vals_B_{mode}\"\n",
    "            ]}\n",
    "\n",
    "            max_k = max(topk)\n",
    "            top_vals_A, top_idx_A = torch.topk(pA, max_k, -1)\n",
    "            top_vals_B, top_idx_B = torch.topk(pB, max_k, -1)\n",
    "\n",
    "            top_vals_A, top_vals_B = top_vals_A.cpu(), top_vals_B.cpu()\n",
    "            top_idx_A, top_idx_B = top_idx_A.cpu(), top_idx_B.cpu()\n",
    "            tgt_cpu = tgt.cpu()\n",
    "\n",
    "            for k in topk:\n",
    "                tkA, tkB = top_idx_A[:, :k], top_idx_B[:, :k]\n",
    "                tvA, tvB = top_vals_A[:, :k], top_vals_B[:, :k]\n",
    "\n",
    "                acc_A = (tkA == tgt_cpu.unsqueeze(1)).any(1).float()\n",
    "                acc_B = (tkB == tgt_cpu.unsqueeze(1)).any(1).float()\n",
    "\n",
    "                # --- set overlap ---\n",
    "                inter = torch.tensor(\n",
    "                    [len(set(tkA[i].tolist()) & set(tkB[i].tolist())) for i in range(L)],\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "                jaccard = inter / (2 * k - inter + eps)\n",
    "                disagree_set = 1.0 - jaccard\n",
    "                agree_set = (inter > 0).float()\n",
    "\n",
    "                # --- correctness relations ---\n",
    "                agree_correct = acc_A * acc_B\n",
    "                disagree_correct = ((acc_A + acc_B).round() == 1).float()  # XOR\n",
    "                agree_wrong = ((1 - acc_A) * (1 - acc_B)).float()\n",
    "\n",
    "                # --- probability mass overlap ---\n",
    "                pmA, pmB = tvA.sum(1), tvB.sum(1)\n",
    "                shared_mass = torch.zeros_like(pmA)\n",
    "                for i in range(L):\n",
    "                    shared = set(tkA[i].tolist()) & set(tkB[i].tolist())\n",
    "                    if shared:\n",
    "                        shared_mass[i] = 0.5 * (\n",
    "                            pA[i, list(shared)].sum().cpu() + pB[i, list(shared)].sum().cpu()\n",
    "                        )\n",
    "                prob_overlap = shared_mass / (0.5 * (pmA + pmB) + eps)\n",
    "\n",
    "                # --- save results ---\n",
    "                fams[f\"acc_A_{mode}\"][f\"@{k}\"] = acc_A.tolist()\n",
    "                fams[f\"acc_B_{mode}\"][f\"@{k}\"] = acc_B.tolist()\n",
    "                fams[f\"jaccard_{mode}\"][f\"@{k}\"] = jaccard.tolist()\n",
    "                fams[f\"disagree_set_{mode}\"][f\"@{k}\"] = disagree_set.tolist()\n",
    "                fams[f\"agree_set_{mode}\"][f\"@{k}\"] = agree_set.tolist()\n",
    "                fams[f\"agree_correct_{mode}\"][f\"@{k}\"] = agree_correct.tolist()\n",
    "                fams[f\"disagree_correct_{mode}\"][f\"@{k}\"] = disagree_correct.tolist()\n",
    "                fams[f\"agree_wrong_{mode}\"][f\"@{k}\"] = agree_wrong.tolist()\n",
    "                fams[f\"prob_overlap_{mode}\"][f\"@{k}\"] = prob_overlap.tolist()\n",
    "\n",
    "                # top predictions\n",
    "                if k == 1:\n",
    "                    fams[f\"top_pred_ids_A_{mode}\"][f\"@{k}\"] = [int(x) for x in tkA[:, 0].tolist()]\n",
    "                    fams[f\"top_pred_vals_A_{mode}\"][f\"@{k}\"] = [float(x) for x in tvA[:, 0].tolist()]\n",
    "                    fams[f\"top_pred_ids_B_{mode}\"][f\"@{k}\"] = [int(x) for x in tkB[:, 0].tolist()]\n",
    "                    fams[f\"top_pred_vals_B_{mode}\"][f\"@{k}\"] = [float(x) for x in tvB[:, 0].tolist()]\n",
    "                else:\n",
    "                    fams[f\"top_pred_ids_A_{mode}\"][f\"@{k}\"] = [[int(x) for x in arr] for arr in tkA.tolist()]\n",
    "                    fams[f\"top_pred_vals_A_{mode}\"][f\"@{k}\"] = [[float(x) for x in arr] for arr in tvA.tolist()]\n",
    "                    fams[f\"top_pred_ids_B_{mode}\"][f\"@{k}\"] = [[int(x) for x in arr] for arr in tkB.tolist()]\n",
    "                    fams[f\"top_pred_vals_B_{mode}\"][f\"@{k}\"] = [[float(x) for x in arr] for arr in tvB.tolist()]\n",
    "\n",
    "\n",
    "            record.update(fams)\n",
    "\n",
    "            # --- cleanup per iteration ---\n",
    "            del logits_A, logits_B, logp_A, logp_B, pA, pB, hidden_A, hidden_B, targets\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # --- finalize and save ---\n",
    "    df = pd.DataFrame(list(rec_map.values()), dtype=object)\n",
    "    out_path = os.path.join(output_dir, f\"{run_name or 'run'}_batch{int(batch_idx or 0):03d}.parquet\")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[saved] {len(df)} rows â†’ {out_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f667ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, torch, psutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def mem_report(note=\"\"):\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"[mem] {note} used {mem.used/1e9:.1f} / {mem.total/1e9:.1f} GB\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_topk_streaming(\n",
    "    dir_A,\n",
    "    dir_B,\n",
    "    output_dir=\"saved_data/topk\",\n",
    "    norm_modes=(\"raw\",\"unit_rms\",\"norm_rms\"),\n",
    "    topk=(1,5,10,20),\n",
    "    device=None,\n",
    "    run_name=\"run\",\n",
    "    debug=True\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    files_A = sorted([f for f in os.listdir(dir_A) if f.endswith(\".pt\")])\n",
    "    files_B = sorted([f for f in os.listdir(dir_B) if f.endswith(\".pt\")])\n",
    "    assert len(files_A) == len(files_B), \"Mismatch in number of files\"\n",
    "\n",
    "    print(f\"[info] Found {len(files_A)} file pairs to process\")\n",
    "\n",
    "    for batch_idx, (fa, fb) in enumerate(tqdm(zip(files_A, files_B), total=len(files_A))):\n",
    "        path_A = os.path.join(dir_A, fa)\n",
    "        path_B = os.path.join(dir_B, fb)\n",
    "        print(f\"\\n[batch {batch_idx}] {fa} vs {fb}\")\n",
    "        mem_report(\"before loading\")\n",
    "\n",
    "        metrics_A = torch.load(path_A, map_location=\"cpu\")\n",
    "        metrics_B = torch.load(path_B, map_location=\"cpu\")\n",
    "\n",
    "        print(\"  [compute] running compute_topk ...\")\n",
    "        df = compute_topk(\n",
    "            metrics_A,\n",
    "            metrics_B,\n",
    "            norm_modes=norm_modes,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "            output_dir=output_dir,\n",
    "            run_name=run_name,\n",
    "            batch_idx=batch_idx,\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "        print(f\"  [saved] {run_name}_batch{batch_idx}.parquet\")\n",
    "\n",
    "        del df, metrics_A, metrics_B\n",
    "        _mask_cache.clear()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        mem_report(\"after cleanup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Found 50 file pairs to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch 0] m_orig_modes_batch000.pt vs m_4bit_modes_batch000.pt\n",
      "[mem] before loading used 7.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 15]\n",
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch000.parquet\n",
      "  [saved] m_orig_m_4bit_batch0.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1/50 [02:36<2:07:30, 156.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mem] after cleanup used 8.2 / 66.6 GB\n",
      "\n",
      "[batch 1] m_orig_modes_batch001.pt vs m_4bit_modes_batch001.pt\n",
      "[mem] before loading used 8.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 11, 15]\n",
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch001.parquet\n",
      "  [saved] m_orig_m_4bit_batch1.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 2/50 [05:14<2:05:56, 157.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mem] after cleanup used 19.2 / 66.6 GB\n",
      "\n",
      "[batch 2] m_orig_modes_batch002.pt vs m_4bit_modes_batch002.pt\n",
      "[mem] before loading used 19.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n",
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch002.parquet\n",
      "  [saved] m_orig_m_4bit_batch2.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3/50 [07:48<2:01:56, 155.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mem] after cleanup used 29.1 / 66.6 GB\n",
      "\n",
      "[batch 3] m_orig_modes_batch003.pt vs m_4bit_modes_batch003.pt\n",
      "[mem] before loading used 29.1 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4/50 [10:22<1:58:50, 155.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch003.parquet\n",
      "  [saved] m_orig_m_4bit_batch3.parquet\n",
      "[mem] after cleanup used 36.4 / 66.6 GB\n",
      "\n",
      "[batch 4] m_orig_modes_batch004.pt vs m_4bit_modes_batch004.pt\n",
      "[mem] before loading used 36.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5/50 [12:57<1:56:20, 155.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch004.parquet\n",
      "  [saved] m_orig_m_4bit_batch4.parquet\n",
      "[mem] after cleanup used 37.1 / 66.6 GB\n",
      "\n",
      "[batch 5] m_orig_modes_batch005.pt vs m_4bit_modes_batch005.pt\n",
      "[mem] before loading used 37.1 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6/50 [15:30<1:53:11, 154.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch005.parquet\n",
      "  [saved] m_orig_m_4bit_batch5.parquet\n",
      "[mem] after cleanup used 36.7 / 66.6 GB\n",
      "\n",
      "[batch 6] m_orig_modes_batch006.pt vs m_4bit_modes_batch006.pt\n",
      "[mem] before loading used 36.7 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  unique mask lengths: [8, 9, 10, 11, 13, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7/50 [18:06<1:51:07, 155.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch006.parquet\n",
      "  [saved] m_orig_m_4bit_batch6.parquet\n",
      "[mem] after cleanup used 37.1 / 66.6 GB\n",
      "\n",
      "[batch 7] m_orig_modes_batch007.pt vs m_4bit_modes_batch007.pt\n",
      "[mem] before loading used 37.1 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  unique mask lengths: [8, 9, 10, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8/50 [20:39<1:48:01, 154.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch007.parquet\n",
      "  [saved] m_orig_m_4bit_batch7.parquet\n",
      "[mem] after cleanup used 36.8 / 66.6 GB\n",
      "\n",
      "[batch 8] m_orig_modes_batch008.pt vs m_4bit_modes_batch008.pt\n",
      "[mem] before loading used 36.8 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  unique mask lengths: [8, 9, 11, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9/50 [23:16<1:46:00, 155.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch008.parquet\n",
      "  [saved] m_orig_m_4bit_batch8.parquet\n",
      "[mem] after cleanup used 36.9 / 66.6 GB\n",
      "\n",
      "[batch 9] m_orig_modes_batch009.pt vs m_4bit_modes_batch009.pt\n",
      "[mem] before loading used 36.9 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 10/50 [25:49<1:42:59, 154.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch009.parquet\n",
      "  [saved] m_orig_m_4bit_batch9.parquet\n",
      "[mem] after cleanup used 36.9 / 66.6 GB\n",
      "\n",
      "[batch 10] m_orig_modes_batch010.pt vs m_4bit_modes_batch010.pt\n",
      "[mem] before loading used 36.9 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11/50 [28:23<1:40:17, 154.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch010.parquet\n",
      "  [saved] m_orig_m_4bit_batch10.parquet\n",
      "[mem] after cleanup used 36.9 / 66.6 GB\n",
      "\n",
      "[batch 11] m_orig_modes_batch011.pt vs m_4bit_modes_batch011.pt\n",
      "[mem] before loading used 36.9 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=4     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  unique mask lengths: [8, 9, 10, 11, 13, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 12/50 [31:03<1:38:48, 156.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch011.parquet\n",
      "  [saved] m_orig_m_4bit_batch11.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 12] m_orig_modes_batch012.pt vs m_4bit_modes_batch012.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=1     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [33:39<1:36:21, 156.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch012.parquet\n",
      "  [saved] m_orig_m_4bit_batch12.parquet\n",
      "[mem] after cleanup used 37.2 / 66.6 GB\n",
      "\n",
      "[batch 13] m_orig_modes_batch013.pt vs m_4bit_modes_batch013.pt\n",
      "[mem] before loading used 37.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 14/50 [36:12<1:33:08, 155.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch013.parquet\n",
      "  [saved] m_orig_m_4bit_batch13.parquet\n",
      "[mem] after cleanup used 37.2 / 66.6 GB\n",
      "\n",
      "[batch 14] m_orig_modes_batch014.pt vs m_4bit_modes_batch014.pt\n",
      "[mem] before loading used 37.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [38:46<1:30:12, 154.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch014.parquet\n",
      "  [saved] m_orig_m_4bit_batch14.parquet\n",
      "[mem] after cleanup used 37.0 / 66.6 GB\n",
      "\n",
      "[batch 15] m_orig_modes_batch015.pt vs m_4bit_modes_batch015.pt\n",
      "[mem] before loading used 37.0 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=6     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=7     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [41:28<1:28:53, 156.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch015.parquet\n",
      "  [saved] m_orig_m_4bit_batch15.parquet\n",
      "[mem] after cleanup used 37.9 / 66.6 GB\n",
      "\n",
      "[batch 16] m_orig_modes_batch016.pt vs m_4bit_modes_batch016.pt\n",
      "[mem] before loading used 37.9 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=4     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=5     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=6     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=7     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  unique mask lengths: [9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [44:10<1:27:13, 158.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch016.parquet\n",
      "  [saved] m_orig_m_4bit_batch16.parquet\n",
      "[mem] after cleanup used 38.5 / 66.6 GB\n",
      "\n",
      "[batch 17] m_orig_modes_batch017.pt vs m_4bit_modes_batch017.pt\n",
      "[mem] before loading used 38.5 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [9, 10, 11, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [46:48<1:24:22, 158.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch017.parquet\n",
      "  [saved] m_orig_m_4bit_batch17.parquet\n",
      "[mem] after cleanup used 38.3 / 66.6 GB\n",
      "\n",
      "[batch 18] m_orig_modes_batch018.pt vs m_4bit_modes_batch018.pt\n",
      "[mem] before loading used 38.3 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [49:23<1:21:17, 157.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch018.parquet\n",
      "  [saved] m_orig_m_4bit_batch18.parquet\n",
      "[mem] after cleanup used 37.2 / 66.6 GB\n",
      "\n",
      "[batch 19] m_orig_modes_batch019.pt vs m_4bit_modes_batch019.pt\n",
      "[mem] before loading used 37.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [52:01<1:18:47, 157.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch019.parquet\n",
      "  [saved] m_orig_m_4bit_batch19.parquet\n",
      "[mem] after cleanup used 37.2 / 66.6 GB\n",
      "\n",
      "[batch 20] m_orig_modes_batch020.pt vs m_4bit_modes_batch020.pt\n",
      "[mem] before loading used 37.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [54:40<1:16:18, 157.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch020.parquet\n",
      "  [saved] m_orig_m_4bit_batch20.parquet\n",
      "[mem] after cleanup used 37.7 / 66.6 GB\n",
      "\n",
      "[batch 21] m_orig_modes_batch021.pt vs m_4bit_modes_batch021.pt\n",
      "[mem] before loading used 37.7 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [57:14<1:13:12, 156.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch021.parquet\n",
      "  [saved] m_orig_m_4bit_batch21.parquet\n",
      "[mem] after cleanup used 37.3 / 66.6 GB\n",
      "\n",
      "[batch 22] m_orig_modes_batch022.pt vs m_4bit_modes_batch022.pt\n",
      "[mem] before loading used 37.3 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 10, 11, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [59:50<1:10:26, 156.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch022.parquet\n",
      "  [saved] m_orig_m_4bit_batch22.parquet\n",
      "[mem] after cleanup used 37.3 / 66.6 GB\n",
      "\n",
      "[batch 23] m_orig_modes_batch023.pt vs m_4bit_modes_batch023.pt\n",
      "[mem] before loading used 37.3 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [1:02:30<1:08:16, 157.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch023.parquet\n",
      "  [saved] m_orig_m_4bit_batch23.parquet\n",
      "[mem] after cleanup used 37.3 / 66.6 GB\n",
      "\n",
      "[batch 24] m_orig_modes_batch024.pt vs m_4bit_modes_batch024.pt\n",
      "[mem] before loading used 37.3 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  unique mask lengths: [8, 9, 10, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [1:05:05<1:05:20, 156.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch024.parquet\n",
      "  [saved] m_orig_m_4bit_batch24.parquet\n",
      "[mem] after cleanup used 37.2 / 66.6 GB\n",
      "\n",
      "[batch 25] m_orig_modes_batch025.pt vs m_4bit_modes_batch025.pt\n",
      "[mem] before loading used 37.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=4     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=5     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [1:07:46<1:03:17, 158.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch025.parquet\n",
      "  [saved] m_orig_m_4bit_batch25.parquet\n",
      "[mem] after cleanup used 38.2 / 66.6 GB\n",
      "\n",
      "[batch 26] m_orig_modes_batch026.pt vs m_4bit_modes_batch026.pt\n",
      "[mem] before loading used 38.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [1:10:23<1:00:30, 157.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch026.parquet\n",
      "  [saved] m_orig_m_4bit_batch26.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 27] m_orig_modes_batch027.pt vs m_4bit_modes_batch027.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [1:12:58<57:29, 156.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch027.parquet\n",
      "  [saved] m_orig_m_4bit_batch27.parquet\n",
      "[mem] after cleanup used 37.5 / 66.6 GB\n",
      "\n",
      "[batch 28] m_orig_modes_batch028.pt vs m_4bit_modes_batch028.pt\n",
      "[mem] before loading used 37.5 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 11, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [1:15:35<54:54, 156.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch028.parquet\n",
      "  [saved] m_orig_m_4bit_batch28.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 29] m_orig_modes_batch029.pt vs m_4bit_modes_batch029.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [1:18:12<52:17, 156.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch029.parquet\n",
      "  [saved] m_orig_m_4bit_batch29.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 30] m_orig_modes_batch030.pt vs m_4bit_modes_batch030.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=1     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [9, 10, 11, 12, 13, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [1:20:49<49:44, 157.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch030.parquet\n",
      "  [saved] m_orig_m_4bit_batch30.parquet\n",
      "[mem] after cleanup used 37.9 / 66.6 GB\n",
      "\n",
      "[batch 31] m_orig_modes_batch031.pt vs m_4bit_modes_batch031.pt\n",
      "[mem] before loading used 37.9 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=5     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [1:23:26<47:04, 156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch031.parquet\n",
      "  [saved] m_orig_m_4bit_batch31.parquet\n",
      "[mem] after cleanup used 37.6 / 66.6 GB\n",
      "\n",
      "[batch 32] m_orig_modes_batch032.pt vs m_4bit_modes_batch032.pt\n",
      "[mem] before loading used 37.6 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=1     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  unique mask lengths: [9, 10, 11, 12, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [1:26:05<44:40, 157.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch032.parquet\n",
      "  [saved] m_orig_m_4bit_batch32.parquet\n",
      "[mem] after cleanup used 38.4 / 66.6 GB\n",
      "\n",
      "[batch 33] m_orig_modes_batch033.pt vs m_4bit_modes_batch033.pt\n",
      "[mem] before loading used 38.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [1:28:41<41:54, 157.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch033.parquet\n",
      "  [saved] m_orig_m_4bit_batch33.parquet\n",
      "[mem] after cleanup used 38.2 / 66.6 GB\n",
      "\n",
      "[batch 34] m_orig_modes_batch034.pt vs m_4bit_modes_batch034.pt\n",
      "[mem] before loading used 38.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [1:31:18<39:16, 157.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch034.parquet\n",
      "  [saved] m_orig_m_4bit_batch34.parquet\n",
      "[mem] after cleanup used 38.2 / 66.6 GB\n",
      "\n",
      "[batch 35] m_orig_modes_batch035.pt vs m_4bit_modes_batch035.pt\n",
      "[mem] before loading used 38.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [1:33:55<36:38, 157.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch035.parquet\n",
      "  [saved] m_orig_m_4bit_batch35.parquet\n",
      "[mem] after cleanup used 37.5 / 66.6 GB\n",
      "\n",
      "[batch 36] m_orig_modes_batch036.pt vs m_4bit_modes_batch036.pt\n",
      "[mem] before loading used 37.5 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=1     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=2     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [1:36:32<33:59, 156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch036.parquet\n",
      "  [saved] m_orig_m_4bit_batch36.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 37] m_orig_modes_batch037.pt vs m_4bit_modes_batch037.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [1:39:05<31:11, 155.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch037.parquet\n",
      "  [saved] m_orig_m_4bit_batch37.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 38] m_orig_modes_batch038.pt vs m_4bit_modes_batch038.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  unique mask lengths: [9, 10, 11, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [1:41:45<28:46, 156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch038.parquet\n",
      "  [saved] m_orig_m_4bit_batch38.parquet\n",
      "[mem] after cleanup used 37.7 / 66.6 GB\n",
      "\n",
      "[batch 39] m_orig_modes_batch039.pt vs m_4bit_modes_batch039.pt\n",
      "[mem] before loading used 37.7 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [1:44:20<26:03, 156.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch039.parquet\n",
      "  [saved] m_orig_m_4bit_batch39.parquet\n",
      "[mem] after cleanup used 37.6 / 66.6 GB\n",
      "\n",
      "[batch 40] m_orig_modes_batch040.pt vs m_4bit_modes_batch040.pt\n",
      "[mem] before loading used 37.6 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 11, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [1:46:54<23:22, 155.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch040.parquet\n",
      "  [saved] m_orig_m_4bit_batch40.parquet\n",
      "[mem] after cleanup used 37.5 / 66.6 GB\n",
      "\n",
      "[batch 41] m_orig_modes_batch041.pt vs m_4bit_modes_batch041.pt\n",
      "[mem] before loading used 37.5 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  unique mask lengths: [8, 9, 10, 12, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [1:49:32<20:50, 156.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch041.parquet\n",
      "  [saved] m_orig_m_4bit_batch41.parquet\n",
      "[mem] after cleanup used 37.4 / 66.6 GB\n",
      "\n",
      "[batch 42] m_orig_modes_batch042.pt vs m_4bit_modes_batch042.pt\n",
      "[mem] before loading used 37.4 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [1:52:02<18:01, 154.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch042.parquet\n",
      "  [saved] m_orig_m_4bit_batch42.parquet\n",
      "[mem] after cleanup used 37.6 / 66.6 GB\n",
      "\n",
      "[batch 43] m_orig_modes_batch043.pt vs m_4bit_modes_batch043.pt\n",
      "[mem] before loading used 37.6 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=9     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  unique mask lengths: [8, 9, 10, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [1:54:31<15:17, 152.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch043.parquet\n",
      "  [saved] m_orig_m_4bit_batch43.parquet\n",
      "[mem] after cleanup used 37.5 / 66.6 GB\n",
      "\n",
      "[batch 44] m_orig_modes_batch044.pt vs m_4bit_modes_batch044.pt\n",
      "[mem] before loading used 37.5 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=14  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  prompt_id=2     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=3     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=7     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [1:57:08<12:49, 153.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch044.parquet\n",
      "  [saved] m_orig_m_4bit_batch44.parquet\n",
      "[mem] after cleanup used 37.8 / 66.6 GB\n",
      "\n",
      "[batch 45] m_orig_modes_batch045.pt vs m_4bit_modes_batch045.pt\n",
      "[mem] before loading used 37.8 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [1:59:41<10:15, 153.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch045.parquet\n",
      "  [saved] m_orig_m_4bit_batch45.parquet\n",
      "[mem] after cleanup used 37.8 / 66.6 GB\n",
      "\n",
      "[batch 46] m_orig_modes_batch046.pt vs m_4bit_modes_batch046.pt\n",
      "[mem] before loading used 37.8 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=7     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  unique mask lengths: [8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [2:02:17<07:43, 154.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch046.parquet\n",
      "  [saved] m_orig_m_4bit_batch46.parquet\n",
      "[mem] after cleanup used 37.8 / 66.6 GB\n",
      "\n",
      "[batch 47] m_orig_modes_batch047.pt vs m_4bit_modes_batch047.pt\n",
      "[mem] before loading used 37.8 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=9     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  unique mask lengths: [8, 9, 10, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [2:04:52<05:09, 154.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch047.parquet\n",
      "  [saved] m_orig_m_4bit_batch47.parquet\n",
      "[mem] after cleanup used 37.8 / 66.6 GB\n",
      "\n",
      "[batch 48] m_orig_modes_batch048.pt vs m_4bit_modes_batch048.pt\n",
      "[mem] before loading used 37.8 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=1     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=2     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=3     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=7     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=9     â†’ len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  unique mask lengths: [8, 9, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [2:07:26<02:34, 154.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch048.parquet\n",
      "  [saved] m_orig_m_4bit_batch48.parquet\n",
      "[mem] after cleanup used 37.8 / 66.6 GB\n",
      "\n",
      "[batch 49] m_orig_modes_batch049.pt vs m_4bit_modes_batch049.pt\n",
      "[mem] before loading used 37.8 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     â†’ len=12  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  prompt_id=1     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=2     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=3     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=4     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=5     â†’ len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=6     â†’ len=13  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "  prompt_id=7     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=8     â†’ len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=9     â†’ len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  unique mask lengths: [8, 9, 11, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [2:10:03<00:00, 156.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 340 rows â†’ saved_data/topk/m_4bit/m_orig_m_4bit_batch049.parquet\n",
      "  [saved] m_orig_m_4bit_batch49.parquet\n",
      "[mem] after cleanup used 37.8 / 66.6 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"m_orig_m_quant\"\n",
    "\n",
    "run_topk_streaming(\n",
    "    dir_A=\"saved_data/lens_data/m_orig\",\n",
    "    dir_B=\"saved_data/lens_data/m_quant\",\n",
    "    output_dir=\"saved_data/topk/m_quant\",\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk=(1, 5, 10),\n",
    "    #eos_token_id=128009,\n",
    "    #bos_token_id=128000,\n",
    "    run_name=run_name,\n",
    "    device=\"cpu\",\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fdf070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 98)\n",
      "['prompt_id', 'batch_index', 'layer_index', 'layer_name', 'prompt_text', 'kl_ab_raw', 'kl_ba_raw', 'js_div_raw', 'js_dist_raw', 'tvd_raw', 'entropy_A_raw', 'entropy_B_raw', 'cosine_sim_raw', 'l2_dist_raw', 'logp_diff_raw', 'logp_A_gt_raw', 'logp_B_gt_raw', 'p_A_gt_raw', 'p_B_gt_raw', 'p_diff_raw', 'ppl_A_raw', 'ppl_B_raw', 'ppl_diff_raw', 'acc_A_raw', 'acc_B_raw', 'jaccard_raw', 'disagree_set_raw', 'agree_set_raw', 'agree_correct_raw', 'disagree_correct_raw', 'agree_wrong_raw', 'prob_overlap_raw', 'top_pred_ids_A_raw', 'top_pred_vals_A_raw', 'top_pred_ids_B_raw', 'top_pred_vals_B_raw', 'kl_ab_unit_rms', 'kl_ba_unit_rms', 'js_div_unit_rms', 'js_dist_unit_rms', 'tvd_unit_rms', 'entropy_A_unit_rms', 'entropy_B_unit_rms', 'cosine_sim_unit_rms', 'l2_dist_unit_rms', 'logp_diff_unit_rms', 'logp_A_gt_unit_rms', 'logp_B_gt_unit_rms', 'p_A_gt_unit_rms', 'p_B_gt_unit_rms', 'p_diff_unit_rms', 'ppl_A_unit_rms', 'ppl_B_unit_rms', 'ppl_diff_unit_rms', 'acc_A_unit_rms', 'acc_B_unit_rms', 'jaccard_unit_rms', 'disagree_set_unit_rms', 'agree_set_unit_rms', 'agree_correct_unit_rms', 'disagree_correct_unit_rms', 'agree_wrong_unit_rms', 'prob_overlap_unit_rms', 'top_pred_ids_A_unit_rms', 'top_pred_vals_A_unit_rms', 'top_pred_ids_B_unit_rms', 'top_pred_vals_B_unit_rms', 'kl_ab_norm_rms', 'kl_ba_norm_rms', 'js_div_norm_rms', 'js_dist_norm_rms', 'tvd_norm_rms', 'entropy_A_norm_rms', 'entropy_B_norm_rms', 'cosine_sim_norm_rms', 'l2_dist_norm_rms', 'logp_diff_norm_rms', 'logp_A_gt_norm_rms', 'logp_B_gt_norm_rms', 'p_A_gt_norm_rms', 'p_B_gt_norm_rms', 'p_diff_norm_rms', 'ppl_A_norm_rms', 'ppl_B_norm_rms', 'ppl_diff_norm_rms', 'acc_A_norm_rms', 'acc_B_norm_rms', 'jaccard_norm_rms', 'disagree_set_norm_rms', 'agree_set_norm_rms', 'agree_correct_norm_rms', 'disagree_correct_norm_rms', 'agree_wrong_norm_rms', 'prob_overlap_norm_rms', 'top_pred_ids_A_norm_rms', 'top_pred_vals_A_norm_rms', 'top_pred_ids_B_norm_rms', 'top_pred_vals_B_norm_rms']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"saved_data/topk/m_8bit/m_orig_m_8bit_batch000.parquet\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc96c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-887.90625  ,  585.3984375, -138.21875  , -244.84375  ,\n",
       "         91.25     , 2274.140625 ,  452.65625  ,  700.578125 ,\n",
       "       -129.6015625,  887.921875 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ppl_diff_raw\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f48216b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.52686605e-08,  3.06945367e-08,  9.48220986e-08, -3.63343133e-08,\n",
       "       -7.19282980e-08, -8.07731340e-08,  1.75841706e-08, -7.15890565e-08,\n",
       "        2.51075107e-08, -1.16006049e-08])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"p_diff_raw\"][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [\"raw\", \"unit_rms\", \"norm_rms\"]:\n",
    "    print(m, df.iloc[0][f\"acc_A_{m}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ca179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cosine_sim_norm_rms\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ec611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cosine_sim_raw\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"l2_dist_raw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = [\"kl_ab_raw\", \"js_div_raw\", \"disagree_correct_raw\"]\n",
    "def arr_len(x):\n",
    "    if isinstance(x, dict) and \"@1\" in x: \n",
    "        return len(x[\"@1\"])\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return len(x)\n",
    "    return np.nan\n",
    "\n",
    "for c in cols:\n",
    "    df[f\"len_{c}\"] = df[c].apply(arr_len)\n",
    "\n",
    "print(df[[\"prompt_id\",\"layer_index\",\"len_kl_ab_raw\",\"len_js_div_raw\",\"len_disagree_correct_raw\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d06d96",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Plot TopK Summaries ==========================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "BASE = Path(\"saved_data\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "OUT_DIR = BASE / \"figures_topk_modes_fixed_full\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPK_METRICS = [\"acc_A\", \"acc_B\", \"jaccard\", \"disagree_correct\"]\n",
    "CONT_METRICS = [\"cosine_sim\", \"l2_dist\", \"tvd\", \"ppl_diff\"]\n",
    "MODES = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "TOPK = [1, 5, 10]\n",
    "\n",
    "\n",
    "def merge_parquet_files(input_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(input_dir).glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {input_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        d = pd.read_parquet(f)\n",
    "        d[\"batch_index\"] = i\n",
    "        dfs.append(d)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merge] merged {len(files)} parquet files â†’ {len(df)} rows\")\n",
    "\n",
    "    n_batches = df[\"batch_index\"].nunique()\n",
    "    n_prompts = df[\"prompt_id\"].nunique()\n",
    "    n_pairs = df.groupby([\"prompt_id\", \"batch_index\"]).ngroups\n",
    "    print(f\"[diag] unique batch_index={n_batches} | unique prompt_id={n_prompts} | unique (prompt,batch)={n_pairs}\")\n",
    "\n",
    "    if n_pairs < len(files) * 10:\n",
    "        print(\"[warn] fewer promptâ€“batch pairs than expected â€” possible ID overlap?\")\n",
    "    else:\n",
    "        print(\"[ok] promptâ€“batch pairs look good\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_topk(df, metric, mode, k):\n",
    "    base = f\"{metric}_{mode}\"\n",
    "    if base not in df.columns:\n",
    "        return np.full(len(df), np.nan)\n",
    "    return df[base].apply(\n",
    "        lambda d: np.mean(d.get(f\"@{k}\", [])) if isinstance(d, dict) and f\"@{k}\" in d else np.nan\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_flat(df, metric, mode):\n",
    "    col = f\"{metric}_{mode}\"\n",
    "    if col not in df.columns:\n",
    "        return np.full(len(df), np.nan)\n",
    "    return df[col].apply(\n",
    "        lambda v: np.mean(v) if isinstance(v, (list, np.ndarray)) and len(v) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    topk_dir = BASE / \"topk\" / model\n",
    "    df = merge_parquet_files(topk_dir)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for metric in TOPK_METRICS:\n",
    "        for mode in MODES:\n",
    "            for k in TOPK:\n",
    "                vals = extract_topk(df, metric, mode, k)\n",
    "                dsub = pd.DataFrame({\n",
    "                    \"model\": model,\n",
    "                    \"metric\": metric,\n",
    "                    \"mode\": mode,\n",
    "                    \"topk\": k,\n",
    "                    \"layer_index\": df.get(\"layer_index\", pd.Series(np.zeros(len(df)))),\n",
    "                    \"value\": vals\n",
    "                })\n",
    "                all_rows.append(dsub)\n",
    "\n",
    "    for metric in CONT_METRICS:\n",
    "        for mode in MODES:\n",
    "            vals = extract_flat(df, metric, mode)\n",
    "            dsub = pd.DataFrame({\n",
    "                \"model\": model,\n",
    "                \"metric\": metric,\n",
    "                \"mode\": mode,\n",
    "                \"topk\": 1,  \n",
    "                \"layer_index\": df.get(\"layer_index\", pd.Series(np.zeros(len(df)))),\n",
    "                \"value\": vals\n",
    "            })\n",
    "            all_rows.append(dsub)\n",
    "\n",
    "    dfs.append(pd.concat(all_rows, ignore_index=True))\n",
    "\n",
    "df_long = pd.concat(dfs, ignore_index=True).dropna(subset=[\"value\"])\n",
    "print(f\"[ok] merged all models â†’ {len(df_long)} rows total\")\n",
    "print(df_long.groupby([\"metric\", \"mode\"])[\"value\"].count().unstack(fill_value=0))\n",
    "\n",
    "\n",
    "palette = dict(raw=\"#4C72B0\", unit_rms=\"#55A868\", norm_rms=\"#C44E52\")\n",
    "\n",
    "for model in MODELS:\n",
    "    dsub = df_long[df_long[\"model\"] == model]\n",
    "    if dsub.empty:\n",
    "        continue\n",
    "\n",
    "    metrics_unique = dsub[\"metric\"].unique().tolist()\n",
    "    nrows = int(np.ceil(len(metrics_unique) / 2))\n",
    "    fig, axes = plt.subplots(nrows, 2, figsize=(14, 4 * nrows))\n",
    "    fig.suptitle(f\"Top-K + Continuous Metrics â€” {model}\", fontsize=16, weight=\"bold\")\n",
    "\n",
    "    for ax, metric in zip(axes.flatten(), metrics_unique):\n",
    "        d = dsub[dsub[\"metric\"] == metric]\n",
    "        sns.lineplot(\n",
    "            data=d,\n",
    "            x=\"layer_name\", \n",
    "            y=\"value\",\n",
    "            hue=\"mode\",\n",
    "            style=\"topk\" if metric in TOPK_METRICS else None,\n",
    "            markers=True,\n",
    "            errorbar=None,\n",
    "            lw=2.0,\n",
    "            palette=palette,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(metric.upper())\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Mean value\")\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "        ax.set_ylim(0, 1 if metric in TOPK_METRICS or metric in [\"tvd\", \"cosine_sim\"] else None)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    out_path = OUT_DIR / f\"overview_{model}_clean.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[saved] {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b15b55",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# TopK Correlations ============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b62baa",
   "metadata": {},
   "source": [
    "### ==============================================\n",
    "### Correlation with Pooling =====================\n",
    "### =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, pointbiserialr, chi2_contingency\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Merge parquet files\n",
    "# ============================================================\n",
    "def merge_parquet_files(input_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(input_dir).glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {input_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        d = pd.read_parquet(f)\n",
    "        d[\"batch_index\"] = i\n",
    "        dfs.append(d)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merge] merged {len(files)} parquet files â†’ {len(df)} rows\")\n",
    "\n",
    "    n_batches = df[\"batch_index\"].nunique()\n",
    "    n_prompts = df[\"prompt_id\"].nunique()\n",
    "    n_pairs = df.groupby([\"prompt_id\", \"batch_index\"]).ngroups\n",
    "    print(f\"[diag] unique batch_index={n_batches} | unique prompt_id={n_prompts} | unique (prompt,batch)={n_pairs}\")\n",
    "\n",
    "    if n_pairs < len(files) * 10:\n",
    "        print(\"[warn] fewer promptâ€“batch pairs than expected â€” possible ID overlap?\")\n",
    "    else:\n",
    "        print(\"[ok] promptâ€“batch pairs look good\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Expand nested top-k metrics (safe names)\n",
    "# ============================================================\n",
    "def expand_topk_metrics(df, metrics, modes=(\"raw\",\"unit_rms\",\"norm_rms\"), topk_levels=(1,5,10)):\n",
    "    new_cols=[]\n",
    "    for metric in metrics:\n",
    "        for mode in modes:\n",
    "            base=f\"{metric}_{mode}\"\n",
    "            if base not in df.columns:\n",
    "                continue\n",
    "            for k in topk_levels:\n",
    "                new=f\"{base}_@{k}\"\n",
    "                df[new]=df[base].apply(\n",
    "                    lambda d: np.array(d.get(f\"@{k}\",[]),float)\n",
    "                    if isinstance(d,dict) and f\"@{k}\" in d else np.array([])\n",
    "                )\n",
    "                new_cols.append(new)\n",
    "    print(f\"[expand] added {len(new_cols)} flattened top-k columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper\n",
    "# ============================================================\n",
    "\"\"\"def safe_flatten(x):\n",
    "    if isinstance(x,(list,np.ndarray)):\n",
    "        return np.array(x,float).flatten()\n",
    "    return np.array([],float)\"\"\"\n",
    "def safe_flatten(v):\n",
    "    \"\"\"Ensure flattening of nested lists/arrays into 1D np.array.\"\"\"\n",
    "    if isinstance(v, (list, np.ndarray)):\n",
    "        return np.asarray(v, dtype=float).flatten()\n",
    "    try:\n",
    "        return np.array([float(v)], dtype=float)\n",
    "    except Exception:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# Extract anchors \n",
    "# ============================================================\n",
    "def preprocess_anchors(df, modes=(\"raw\", \"unit_rms\", \"norm_rms\")):\n",
    "    for mode in modes:\n",
    "        col = f\"disagree_correct_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        # find max sequence length (@1-level)\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda d: len(d.get(\"@1\", [])) if isinstance(d, dict) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(d):\n",
    "            if isinstance(d, dict) and \"@1\" in d:\n",
    "                arr = np.asarray(d[\"@1\"], dtype=float)\n",
    "            else:\n",
    "                arr = np.full(max_len, np.nan, dtype=float)\n",
    "            # pad to consistent length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[f\"{col}_@1\"] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col}_@1 â†’ len={max_len}\")\n",
    "\n",
    "    for mode in modes:\n",
    "        col = f\"p_diff__{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        # find max length across all prompts\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda v: len(v) if isinstance(v, (list, np.ndarray)) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(v):\n",
    "            if isinstance(v, (list, np.ndarray)):\n",
    "                arr = np.asarray(v, dtype=float)\n",
    "            else:\n",
    "                try:\n",
    "                    arr = np.array([float(v)], dtype=float)\n",
    "                except Exception:\n",
    "                    arr = np.array([np.nan], dtype=float)\n",
    "            # pad to uniform length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[col] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col} â†’ len={max_len}\")\n",
    "\n",
    "    print(\"[done] Anchors preprocessed (binary + continuous, unified padding)]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation \n",
    "# ============================================================\n",
    "def _is_binary(arr, tol=1e-6):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if len(arr) == 0:\n",
    "        return False\n",
    "    u = np.unique(np.round(arr, 6))\n",
    "    return np.all((np.abs(u - 0) < tol) | (np.abs(u - 1) < tol))\n",
    "\n",
    "\n",
    "def _phi_coefficient(x, y):\n",
    "    \"\"\"Phi coefficient for two binary arrays (0/1).\"\"\"\n",
    "    x, y = np.asarray(x).astype(int), np.asarray(y).astype(int)\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        table = pd.crosstab(x, y)\n",
    "        if table.shape != (2, 2):\n",
    "            return np.nan\n",
    "        chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
    "        sign = np.sign((table.loc[1,1]*table.loc[0,0]) - (table.loc[1,0]*table.loc[0,1]))\n",
    "        return float(sign * np.sqrt(chi2 / len(x)))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- metric-type mapping ---\n",
    "\"\"\"METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"binary\",\n",
    "    \"agree_correct\": \"binary\",\n",
    "    \"agree_wrong\": \"binary\",\n",
    "    \"agree_set\": \"binary\",\n",
    "    \"disagree_set\": \"binary\",\n",
    "\n",
    "    \"jaccard_@1\": \"binary\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\"\"\"\n",
    "METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"continuous\",\n",
    "    \"agree_correct\": \"continuous\",\n",
    "    \"agree_wrong\": \"continuous\",\n",
    "    \"agree_set\": \"continuous\",\n",
    "    \"disagree_set\": \"continuous\",\n",
    "\n",
    "    \"jaccard_@1\": \"continuous\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\n",
    "\n",
    "\"\"\"def _choose_corr_func_fixed(anchor, metric):\n",
    "    anchor_t = \"binary\" if \"disagree_correct\" in anchor else \"continuous\"\n",
    "    metric_t = METRIC_TYPES.get(metric, \"continuous\")\n",
    "\n",
    "    if anchor_t == \"binary\" and metric_t == \"binary\":\n",
    "        return \"phi\", _phi_coefficient\n",
    "    elif anchor_t == \"binary\" or metric_t == \"binary\":\n",
    "        return \"pointbiserial\", pointbiserialr\n",
    "    else:\n",
    "        return \"spearman\", spearmanr\"\"\"\n",
    "def _choose_corr_func_fixed(anchor, metric):\n",
    "    \"\"\"Deterministic correlation type based on known metric types.\"\"\"\n",
    "    anchor_t = METRIC_TYPES.get(anchor, \"continuous\")  \n",
    "    metric_t = METRIC_TYPES.get(metric, \"continuous\")\n",
    "\n",
    "    if anchor_t == \"binary\" and metric_t == \"binary\":\n",
    "        return \"phi\", _phi_coefficient\n",
    "    elif anchor_t == \"binary\" or metric_t == \"binary\":\n",
    "        return \"pointbiserial\", pointbiserialr\n",
    "    else:\n",
    "        return \"spearman\", spearmanr\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation (pooled version)\n",
    "# ============================================================\n",
    "def correlate_layers_by_anchor(\n",
    "    df,\n",
    "    anchor,\n",
    "    metrics,\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk_levels=(1, 5, 10),\n",
    "    n_boot=10,\n",
    "    n_perm=10,\n",
    "    seed=42,\n",
    "    output_dir=None,\n",
    "    model=\"m_8bit\",\n",
    "    min_valid=3,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    results = []\n",
    "    df_out = df[df[\"layer_name\"].str.lower() == \"output\"]\n",
    "\n",
    "    # --- Build anchor map ---\n",
    "    anchor_map = {}\n",
    "    for _, row in df_out.iterrows():\n",
    "        key = (int(row[\"prompt_id\"]), int(row.get(\"batch_index\", 0)))\n",
    "        anchor_vecs = {}\n",
    "        for mode in modes:\n",
    "            col1 = f\"{anchor}_{mode}_@1\"\n",
    "            col2 = f\"{anchor}_{mode}\"\n",
    "            col = col1 if col1 in df.columns else col2\n",
    "            val = row.get(col)\n",
    "            if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
    "                anchor_vecs[mode] = np.array(val, float)\n",
    "        if anchor_vecs:\n",
    "            anchor_map[key] = anchor_vecs\n",
    "    print(f\"[anchor_map] built {len(anchor_map)} anchors\")\n",
    "\n",
    "    # --- Force raw for these metrics ---\n",
    "    SHARED_METRICS = {\"cosine_sim\", \"l2_dist\"}\n",
    "\n",
    "    # ============================================================\n",
    "    # MAIN LOOP\n",
    "    # ============================================================\n",
    "    for mode in modes:\n",
    "        for (lname, lidx), layer_df in tqdm(df.groupby([\"layer_name\", \"layer_index\"]), desc=f\"{anchor}_{mode}\"):\n",
    "            for metric in metrics:\n",
    "                is_topk_only = metric in {\"acc_A\", \"acc_B\", \"jaccard\"}\n",
    "                suffixes = [f\"_@{k}\" for k in topk_levels] if is_topk_only else [\"\"] + [f\"_@{k}\" for k in topk_levels]\n",
    "\n",
    "                for suffix in suffixes:\n",
    "                    src_mode = \"raw\" if metric in SHARED_METRICS else mode\n",
    "                    mcol = f\"{metric}_{src_mode}{suffix}\"\n",
    "                    if mcol not in layer_df.columns:\n",
    "                        continue\n",
    "\n",
    "                    anchor_vals, metric_vals = [], []\n",
    "                    for (pid, bid), group in layer_df.groupby([\"prompt_id\", \"batch_index\"]):\n",
    "                        key = (int(pid), int(bid))\n",
    "                        if key not in anchor_map or mode not in anchor_map[key]:\n",
    "                            continue\n",
    "                        a = anchor_map[key][mode]\n",
    "                        m = np.asarray(group[mcol].iloc[0], dtype=float).flatten()\n",
    "                        n = min(len(a), len(m))\n",
    "                        if n < min_valid:\n",
    "                            continue\n",
    "                        a, m = a[:n], m[:n]\n",
    "                        mask = np.isfinite(a) & np.isfinite(m)\n",
    "                        if mask.sum() < min_valid:\n",
    "                            continue\n",
    "                        anchor_vals.append(a[mask])\n",
    "                        metric_vals.append(m[mask])\n",
    "\n",
    "                    # --- NaN / missing cases ---\n",
    "                    if not anchor_vals:\n",
    "                        results.append({\n",
    "                            \"mode\": mode,\n",
    "                            \"anchor\": anchor,\n",
    "                            \"metric\": f\"{metric}{suffix}\",\n",
    "                            \"corr_type\": \"undefined\",\n",
    "                            \"layer_name\": lname,\n",
    "                            \"layer_index\": lidx,\n",
    "                            \"rho\": np.nan,\n",
    "                            \"rho_boot_median\": np.nan,\n",
    "                            \"ci_low\": np.nan,\n",
    "                            \"ci_high\": np.nan,\n",
    "                            \"p_val\": np.nan,\n",
    "                            \"p_perm\": np.nan,\n",
    "                            \"n\": 0,\n",
    "                            \"pooling\": \"pooled\",\n",
    "                            \"reason\": \"no_data\"\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    A = np.concatenate(anchor_vals)\n",
    "                    M = np.concatenate(metric_vals)\n",
    "                    mask = np.isfinite(A) & np.isfinite(M)\n",
    "                    n_used = int(mask.sum())\n",
    "                    if n_used < min_valid:\n",
    "                        continue\n",
    "                    A, M = A[mask], M[mask]\n",
    "\n",
    "                    # --- No variance case ---\n",
    "                    if np.std(A) == 0 or np.std(M) == 0:\n",
    "                        results.append({\n",
    "                            \"mode\": mode,\n",
    "                            \"anchor\": anchor,\n",
    "                            \"metric\": f\"{metric}{suffix}\",\n",
    "                            \"corr_type\": \"spearman\",\n",
    "                            \"layer_name\": lname,\n",
    "                            \"layer_index\": lidx,\n",
    "                            \"rho\": np.nan,\n",
    "                            \"rho_boot_median\": np.nan,\n",
    "                            \"ci_low\": np.nan,\n",
    "                            \"ci_high\": np.nan,\n",
    "                            \"p_val\": np.nan,\n",
    "                            \"p_perm\": np.nan,\n",
    "                            \"n\": n_used,\n",
    "                            \"pooling\": \"pooled\",\n",
    "                            \"reason\": \"no_variance\"\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    # --- correlation computation ---\n",
    "                    cname, func = _choose_corr_func_fixed(anchor, f\"{metric}{suffix}\")\n",
    "                    try:\n",
    "                        rho, pval = func(A, M)\n",
    "                    except Exception:\n",
    "                        rho, pval = np.nan, np.nan\n",
    "\n",
    "                    if not np.isfinite(rho):\n",
    "                        continue\n",
    "\n",
    "                    # --- bootstrap ---\n",
    "                    boot_rhos = []\n",
    "                    for _ in range(n_boot):\n",
    "                        idx = np.random.choice(n_used, n_used, replace=True)\n",
    "                        try:\n",
    "                            rho_b, _ = func(A[idx], M[idx])\n",
    "                        except Exception:\n",
    "                            rho_b = np.nan\n",
    "                        if np.isfinite(rho_b):\n",
    "                            boot_rhos.append(rho_b)\n",
    "                    if len(boot_rhos) > 20:\n",
    "                        ci_low, ci_high = np.percentile(boot_rhos, [2.5, 97.5])\n",
    "                        rho_boot = np.median(boot_rhos)\n",
    "                    else:\n",
    "                        ci_low = ci_high = rho_boot = np.nan\n",
    "\n",
    "                    # --- permutation test ---\n",
    "                    perm_rhos = []\n",
    "                    for _ in range(n_perm):\n",
    "                        try:\n",
    "                            rho_p, _ = func(A, np.random.permutation(M))\n",
    "                        except Exception:\n",
    "                            rho_p = np.nan\n",
    "                        if np.isfinite(rho_p):\n",
    "                            perm_rhos.append(rho_p)\n",
    "                    if len(perm_rhos) > 20:\n",
    "                        perm_rhos = np.array(perm_rhos)\n",
    "                        p_perm = (np.sum(np.abs(perm_rhos) >= abs(rho)) + 1) / (len(perm_rhos) + 1)\n",
    "                    else:\n",
    "                        p_perm = np.nan\n",
    "\n",
    "                    results.append({\n",
    "                        \"mode\": mode,\n",
    "                        \"anchor\": anchor,\n",
    "                        \"metric\": f\"{metric}{suffix}\",\n",
    "                        \"corr_type\": cname,\n",
    "                        \"layer_name\": lname,\n",
    "                        \"layer_index\": lidx,\n",
    "                        \"rho\": rho,\n",
    "                        \"rho_boot_median\": rho_boot,\n",
    "                        \"ci_low\": ci_low,\n",
    "                        \"ci_high\": ci_high,\n",
    "                        \"p_val\": pval,\n",
    "                        \"p_perm\": p_perm,\n",
    "                        \"n\": n_used,\n",
    "                        \"pooling\": \"pooled\",\n",
    "                        \"reason\": \"ok\"\n",
    "                    })\n",
    "\n",
    "    # ============================================================\n",
    "    # Wrap up\n",
    "    # ============================================================\n",
    "    df_corr = pd.DataFrame(results)\n",
    "    print(f\"[ok] {anchor} â†’ {len(df_corr)} pooled correlations\")\n",
    "    print(f\"[nan check] {df_corr['rho'].isna().sum()} NaN correlations out of {len(df_corr)} total\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = Path(output_dir) / f\"lw_{model}_{anchor}_corr_pooled.csv\"\n",
    "        df_corr.to_csv(out_path, index=False)\n",
    "        print(f\"[saved correlations] {out_path}\")\n",
    "\n",
    "    return df_corr\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Summarize correlations + Global Spearman\n",
    "# ============================================================\n",
    "def summarize_correlations(df_corr, output_dir, model, ci_mode=\"both\"):\n",
    "    df = df_corr.copy()\n",
    "    if \"n\" not in df.columns:\n",
    "        df[\"n\"] = 1\n",
    "\n",
    "    df[\"z\"] = np.nan\n",
    "    mask_finite = np.isfinite(df[\"rho\"])\n",
    "    df.loc[mask_finite, \"z\"] = np.arctanh(np.clip(df.loc[mask_finite, \"rho\"], -0.999999, 0.999999))\n",
    "\n",
    "    drop_cols = [c for c in [\"prompt_id\", \"batch_index\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    group_cols = [\"mode\", \"anchor\", \"metric\", \"corr_type\", \"layer_name\", \"layer_index\"]\n",
    "\n",
    "    def _weighted_stats(g):\n",
    "        out = {}\n",
    "        if not np.any(np.isfinite(g[\"rho\"])) or g[\"n\"].sum() == 0:\n",
    "            for key in [\n",
    "                \"rho_mean\", \"rho_low\", \"rho_high\",\n",
    "                \"rho_boot_mean\", \"ci_low_emp\", \"ci_high_emp\", \"n_total\"\n",
    "            ]:\n",
    "                out[key] = np.nan\n",
    "            out[\"n_total\"] = 0\n",
    "        else:\n",
    "            z = g[\"z\"].dropna()\n",
    "            w = g.loc[z.index, \"n\"]\n",
    "            z_mean = np.average(z, weights=w)\n",
    "            z_std = np.sqrt(np.average((z - z_mean)**2, weights=w))\n",
    "            out[\"rho_mean\"] = np.tanh(z_mean)\n",
    "            out[\"rho_low\"] = np.tanh(z_mean - 1.96 * z_std)\n",
    "            out[\"rho_high\"] = np.tanh(z_mean + 1.96 * z_std)\n",
    "            out[\"n_total\"] = g[\"n\"].sum()\n",
    "            out[\"rho_boot_mean\"] = np.average(g[\"rho_boot_median\"].fillna(0), weights=w)\n",
    "            out[\"ci_low_emp\"] = np.average(g[\"ci_low\"].fillna(0), weights=w)\n",
    "            out[\"ci_high_emp\"] = np.average(g[\"ci_high\"].fillna(0), weights=w)\n",
    "\n",
    "        for k in group_cols:\n",
    "            out[k] = g[k].iloc[0]\n",
    "        return pd.DataFrame([out])\n",
    "\n",
    "    df_summary = pd.concat(\n",
    "        [_weighted_stats(g) for _, g in df.groupby(group_cols, group_keys=False)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # choose CI mode for plotting\n",
    "    if ci_mode == \"empirical\":\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"ci_low_emp\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"ci_high_emp\"]\n",
    "    else:\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"rho_low\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"rho_high\"]\n",
    "\n",
    "    # --- compute global Spearman (across all layers) ---\n",
    "    valid = df_summary[np.isfinite(df_summary[\"rho_mean\"])]\n",
    "    if not valid.empty:\n",
    "        z_vals = np.arctanh(valid[\"rho_mean\"].clip(-0.999999, 0.999999))\n",
    "        weights = valid[\"n_total\"].fillna(1)\n",
    "        z_mean = np.average(z_vals, weights=weights)\n",
    "        rho_global = np.tanh(z_mean)\n",
    "        n_valid_layers = len(valid)\n",
    "        print(f\"[global Spearman rho] {rho_global:.3f} (across {n_valid_layers} valid layers)\")\n",
    "    else:\n",
    "        rho_global = np.nan\n",
    "        n_valid_layers = 0\n",
    "        print(\"[global Spearman rho] not computable (no valid layers)\")\n",
    "\n",
    "    # --- report missing / no-variance layers ---\n",
    "    missing_info = df_corr[df_corr[\"reason\"].isin([\"no_variance\", \"no_data\"])]\n",
    "    if not missing_info.empty:\n",
    "        for metric in missing_info[\"metric\"].unique():\n",
    "            n_missing = len(missing_info[missing_info[\"metric\"] == metric])\n",
    "            print(f\"[info] {metric}: {n_missing} layers had no variance or missing data\")\n",
    "\n",
    "    # --- save summary ---\n",
    "    out_summary = Path(output_dir) / f\"lw_{model}_corr_pooled_summary_{ci_mode}.csv\"\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "    print(f\"[saved summary] {out_summary}\")\n",
    "    print(f\"[diag] rows={len(df_summary)} groups={df[group_cols].drop_duplicates().shape[0]}\")\n",
    "    print(f\"[NaN summary rows: {df_summary['rho_mean'].isna().sum()}]\")\n",
    "\n",
    "    return df_summary, rho_global, n_valid_layers\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run pooled correlation pipeline \n",
    "# ============================================================\n",
    "BASE = Path(\"saved_data\")\n",
    "model = \"m_quant\"\n",
    "output_dir = BASE / \"summary\" / model\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "topk_dir = BASE / \"topk\" / model\n",
    "df_topk = merge_parquet_files(topk_dir)\n",
    "df_topk = expand_topk_metrics(\n",
    "    df_topk,\n",
    "    metrics=[\"jaccard\", \"acc_A\", \"acc_B\", \"disagree_correct\"],\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\")\n",
    ")\n",
    "df_topk = preprocess_anchors(df_topk)\n",
    "\n",
    "for m in [\"cosine_sim\", \"l2_dist\"]:\n",
    "    if f\"{m}_raw\" in df_topk.columns:\n",
    "        for mode in [\"unit_rms\", \"norm_rms\"]:\n",
    "            col_src = f\"{m}_raw\"\n",
    "            col_dst = f\"{m}_{mode}\"\n",
    "            if col_dst not in df_topk.columns:\n",
    "                df_topk[col_dst] = df_topk[col_src]\n",
    "                print(f\"[copy] propagated {col_src} â†’ {col_dst}\")\n",
    "\n",
    "anchors = [\"disagree_correct\", \"p_diff\"]\n",
    "metrics = [\n",
    "    \"kl_ab\", \"kl_ba\", \"js_div\", \"js_dist\", \"tvd\",\n",
    "    \"entropy_A\", \"entropy_B\", \"cosine_sim\", \"l2_dist\",\n",
    "    \"ppl_diff\", \"jaccard\", \"acc_A\", \"acc_B\"\n",
    "]\n",
    "\n",
    "df_corr_all = []\n",
    "for a in anchors:\n",
    "    df_corr_all.append(correlate_layers_by_anchor(df_topk, a, metrics, model=model))\n",
    "df_corr_all = pd.concat(df_corr_all, ignore_index=True)\n",
    "\n",
    "pooled_corr_path = output_dir / f\"lw_{model}_corr_pooled.csv\"\n",
    "pooled_summary_path = output_dir / f\"lw_{model}_corr_pooled_summary.csv\"\n",
    "\n",
    "df_corr_all.to_csv(pooled_corr_path, index=False)\n",
    "print(f\"[saved pooled correlations] {pooled_corr_path}\")\n",
    "\n",
    "df_summary = summarize_correlations(df_corr_all, output_dir=output_dir, model=model)\n",
    "print(f\"[saved pooled summary] {pooled_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_all[df_corr_all[\"rho\"].isna()].groupby(\"layer_name\").size()\n",
    "\n",
    "df_corr_all.groupby(\"metric\")[\"rho\"].apply(lambda s: s.isna().mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_all.query(\"metric.str.contains('cosine_sim')\").groupby(\"mode\")[\"rho\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    df_corr_all, df_summary,\n",
    "    on=[\"mode\",\"anchor\",\"metric\",\"corr_type\",\"layer_name\",\"layer_index\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_corr\",\"_sum\")\n",
    ")\n",
    "print(merged[[\"rho\",\"rho_mean\"]].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ae653",
   "metadata": {},
   "source": [
    "### ==============================================\n",
    "### Correlation Per Prompt =======================\n",
    "### =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, pointbiserialr, chi2_contingency\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Merge parquet files\n",
    "# ============================================================\n",
    "def merge_parquet_files(input_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(input_dir).glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {input_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        d = pd.read_parquet(f)\n",
    "        d[\"batch_index\"] = i\n",
    "        dfs.append(d)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merge] merged {len(files)} parquet files â†’ {len(df)} rows\")\n",
    "\n",
    "    n_batches = df[\"batch_index\"].nunique()\n",
    "    n_prompts = df[\"prompt_id\"].nunique()\n",
    "    n_pairs = df.groupby([\"prompt_id\", \"batch_index\"]).ngroups\n",
    "    print(f\"[diag] unique batch_index={n_batches} | unique prompt_id={n_prompts} | unique (prompt,batch)={n_pairs}\")\n",
    "\n",
    "    if n_pairs < len(files) * 10:\n",
    "        print(\"[warn] fewer promptâ€“batch pairs than expected â€” possible ID overlap?\")\n",
    "    else:\n",
    "        print(\"[ok] promptâ€“batch pairs look good\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Expand nested top-k metrics \n",
    "# ============================================================\n",
    "def expand_topk_metrics(df, metrics, modes=(\"raw\",\"unit_rms\",\"norm_rms\"), topk_levels=(1,5,10)):\n",
    "    new_cols=[]\n",
    "    for metric in metrics:\n",
    "        for mode in modes:\n",
    "            base=f\"{metric}_{mode}\"\n",
    "            if base not in df.columns:\n",
    "                continue\n",
    "            for k in topk_levels:\n",
    "                new=f\"{base}_@{k}\"\n",
    "                df[new]=df[base].apply(\n",
    "                    lambda d: np.array(d.get(f\"@{k}\",[]),float)\n",
    "                    if isinstance(d,dict) and f\"@{k}\" in d else np.array([])\n",
    "                )\n",
    "                new_cols.append(new)\n",
    "    print(f\"[expand] added {len(new_cols)} flattened top-k columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper\n",
    "# ============================================================\n",
    "\"\"\"def safe_flatten(x):\n",
    "    if isinstance(x,(list,np.ndarray)):\n",
    "        return np.array(x,float).flatten()\n",
    "    return np.array([],float)\"\"\"\n",
    "def safe_flatten(v):\n",
    "    \"\"\"Ensure flattening of nested lists/arrays into 1D np.array.\"\"\"\n",
    "    if isinstance(v, (list, np.ndarray)):\n",
    "        return np.asarray(v, dtype=float).flatten()\n",
    "    try:\n",
    "        return np.array([float(v)], dtype=float)\n",
    "    except Exception:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# Extract anchors \n",
    "# ============================================================\n",
    "def extract_top1_disagreement_anchor(df, modes=(\"raw\",\"unit_rms\",\"norm_rms\")):\n",
    "    for mode in modes:\n",
    "        col=f\"disagree_correct_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "        max_len=int(df[col].apply(\n",
    "            lambda d: len(d.get(\"@1\",[])) if isinstance(d,dict) else 0\n",
    "        ).max() or 0)\n",
    "        def to_array(d):\n",
    "            if isinstance(d,dict) and \"@1\" in d:\n",
    "                arr=np.array(d[\"@1\"],float)\n",
    "            else:\n",
    "                arr=np.full(max_len,np.nan)\n",
    "            return arr\n",
    "        df[f\"{col}_@1\"]=df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col}_@1 â†’ len={max_len}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_anchors(df, modes=(\"raw\", \"unit_rms\", \"norm_rms\")):\n",
    "    for mode in modes:\n",
    "        col = f\"disagree_correct_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda d: len(d.get(\"@1\", [])) if isinstance(d, dict) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(d):\n",
    "            if isinstance(d, dict) and \"@1\" in d:\n",
    "                arr = np.asarray(d[\"@1\"], dtype=float)\n",
    "            else:\n",
    "                arr = np.full(max_len, np.nan, dtype=float)\n",
    "            # pad to consistent length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[f\"{col}_@1\"] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col}_@1 â†’ len={max_len}\")\n",
    "\n",
    "    for mode in modes:\n",
    "        col = f\"p_diff_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda v: len(v) if isinstance(v, (list, np.ndarray)) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(v):\n",
    "            if isinstance(v, (list, np.ndarray)):\n",
    "                arr = np.asarray(v, dtype=float)\n",
    "            else:\n",
    "                try:\n",
    "                    arr = np.array([float(v)], dtype=float)\n",
    "                except Exception:\n",
    "                    arr = np.array([np.nan], dtype=float)\n",
    "            # pad to uniform length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[col] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col} â†’ len={max_len}\")\n",
    "\n",
    "    print(\"[done] Anchors preprocessed (binary + continuous, unified padding)]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation \n",
    "# ============================================================\n",
    "def _is_binary(arr, tol=1e-6):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if len(arr) == 0:\n",
    "        return False\n",
    "    u = np.unique(np.round(arr, 6))\n",
    "    return np.all((np.abs(u - 0) < tol) | (np.abs(u - 1) < tol))\n",
    "\n",
    "\n",
    "def _phi_coefficient(x, y):\n",
    "    \"\"\"Phi coefficient for two binary arrays (0/1).\"\"\"\n",
    "    x, y = np.asarray(x).astype(int), np.asarray(y).astype(int)\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        table = pd.crosstab(x, y)\n",
    "        if table.shape != (2, 2):\n",
    "            return np.nan\n",
    "        chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
    "        sign = np.sign((table.loc[1, 1] * table.loc[0, 0]) - (table.loc[1, 0] * table.loc[0, 1]))\n",
    "        return float(sign * np.sqrt(chi2 / len(x)))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- metric-type mapping ---\n",
    "\"\"\"METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"binary\",\n",
    "    \"agree_correct\": \"binary\",\n",
    "    \"agree_wrong\": \"binary\",\n",
    "    \"agree_set\": \"binary\",\n",
    "    \"disagree_set\": \"binary\",\n",
    "\n",
    "    \"jaccard_@1\": \"binary\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\"\"\"\n",
    "METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"continuous\",\n",
    "    \"agree_correct\": \"continuous\",\n",
    "    \"agree_wrong\": \"continuous\",\n",
    "    \"agree_set\": \"continuous\",\n",
    "    \"disagree_set\": \"continuous\",\n",
    "\n",
    "    \"jaccard_@1\": \"continuous\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\n",
    "\n",
    "\"\"\"def _choose_corr_func_fixed(anchor, metric):\n",
    "    anchor_t = \"binary\" if \"disagree_correct\" in anchor else \"continuous\"\n",
    "    metric_t = METRIC_TYPES.get(metric, \"continuous\")\n",
    "\n",
    "    if anchor_t == \"binary\" and metric_t == \"binary\":\n",
    "        return \"phi\", _phi_coefficient\n",
    "    elif anchor_t == \"binary\" or metric_t == \"binary\":\n",
    "        return \"pointbiserial\", pointbiserialr\n",
    "    else:\n",
    "        return \"spearman\", spearmanr\"\"\"\n",
    "def _choose_corr_func_fixed(anchor, metric):\n",
    "    \"\"\"Deterministic correlation type based on known metric types.\"\"\"\n",
    "    anchor_t = METRIC_TYPES.get(anchor, \"continuous\") \n",
    "    metric_t = METRIC_TYPES.get(metric, \"continuous\")\n",
    "\n",
    "    if anchor_t == \"binary\" and metric_t == \"binary\":\n",
    "        return \"phi\", _phi_coefficient\n",
    "    elif anchor_t == \"binary\" or metric_t == \"binary\":\n",
    "        return \"pointbiserial\", pointbiserialr\n",
    "    else:\n",
    "        return \"spearman\", spearmanr\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation (per-prompt)\n",
    "# ============================================================\n",
    "def correlate_layers_by_anchor_perprompt(\n",
    "    df,\n",
    "    anchor,\n",
    "    metrics,\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk_levels=(1, 5, 10),\n",
    "    n_boot=10,\n",
    "    n_perm=10,\n",
    "    seed=42,\n",
    "    output_dir=None,\n",
    "    model=\"m_8bit\",\n",
    "    min_valid=3,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    results = []\n",
    "    df_out = df[df[\"layer_name\"].str.lower() == \"output\"]\n",
    "\n",
    "    # --- Build anchor map from output layer ---\n",
    "    anchor_map = {}\n",
    "    for _, row in df_out.iterrows():\n",
    "        key = (int(row[\"prompt_id\"]), int(row.get(\"batch_index\", 0)))\n",
    "        anchor_vecs = {}\n",
    "        for mode in modes:\n",
    "            col1 = f\"{anchor}_{mode}_@1\"\n",
    "            col2 = f\"{anchor}_{mode}\"\n",
    "            col = col1 if col1 in df.columns else col2\n",
    "            val = row.get(col)\n",
    "            if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
    "                anchor_vecs[mode] = np.array(val, float)\n",
    "        if anchor_vecs:\n",
    "            anchor_map[key] = anchor_vecs\n",
    "    print(f\"[anchor_map] built {len(anchor_map)} anchors\")\n",
    "\n",
    "    skip_const = 0\n",
    "    SHARED_METRICS = {\"cosine_sim\", \"l2_dist\"}\n",
    "\n",
    "    # --- Main correlation loop ---\n",
    "    for mode in modes:\n",
    "        for (lname, lidx), layer_df in tqdm(\n",
    "            df.groupby([\"layer_name\", \"layer_index\"]),\n",
    "            desc=f\"{anchor}_{mode}\"\n",
    "        ):\n",
    "            for metric in metrics:\n",
    "                is_topk_only = metric in {\"acc_A\", \"acc_B\", \"jaccard\"}\n",
    "                suffixes = [f\"_@{k}\" for k in topk_levels] if is_topk_only else [\"\"] + [f\"_@{k}\" for k in topk_levels]\n",
    "\n",
    "                for suffix in suffixes:\n",
    "                    src_mode = \"raw\" if metric in SHARED_METRICS else mode\n",
    "                    mcol = f\"{metric}_{src_mode}{suffix}\"\n",
    "                    if mcol not in layer_df.columns:\n",
    "                        continue\n",
    "\n",
    "                    per_prompt_rhos, per_prompt_ns = [], []\n",
    "\n",
    "                    for (pid, bid), group in layer_df.groupby([\"prompt_id\", \"batch_index\"]):\n",
    "                        key = (int(pid), int(bid))\n",
    "                        if key not in anchor_map or mode not in anchor_map[key]:\n",
    "                            continue\n",
    "\n",
    "                        a = anchor_map[key][mode]\n",
    "                        m = np.asarray(group[mcol].iloc[0], dtype=float).flatten()\n",
    "                        n = min(len(a), len(m))\n",
    "                        if n < min_valid:\n",
    "                            continue\n",
    "\n",
    "                        a, m = a[:n], m[:n]\n",
    "                        mask = np.isfinite(a) & np.isfinite(m)\n",
    "                        n_used = int(mask.sum())\n",
    "                        if n_used < min_valid:\n",
    "                            continue\n",
    "                        if np.std(a[mask]) == 0 or np.std(m[mask]) == 0:\n",
    "                            skip_const += 1\n",
    "                            continue\n",
    "\n",
    "                        cname, corr_func = _choose_corr_func_fixed(anchor, f\"{metric}{suffix}\")\n",
    "                        try:\n",
    "                            rho, _ = corr_func(a[mask], m[mask])\n",
    "                        except Exception:\n",
    "                            rho = np.nan\n",
    "                        if np.isfinite(rho):\n",
    "                            per_prompt_rhos.append(rho)\n",
    "                            per_prompt_ns.append(n_used)\n",
    "\n",
    "                    # --- handle missing prompts ---\n",
    "                    if not per_prompt_rhos:\n",
    "                        results.append({\n",
    "                            \"mode\": mode, \"anchor\": anchor, \"metric\": metric + suffix,\n",
    "                            \"corr_type\": \"undefined\", \"layer_name\": lname, \"layer_index\": lidx,\n",
    "                            \"rho\": np.nan, \"rho_boot_median\": np.nan, \"ci_low\": np.nan,\n",
    "                            \"ci_high\": np.nan, \"p_perm\": np.nan, \"n_prompts\": 0,\n",
    "                            \"n_used_mean\": np.nan, \"n_used_median\": np.nan,\n",
    "                            \"pooling\": \"per_prompt\", \"reason\": \"no_data\"\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    A, N = np.array(per_prompt_rhos), np.array(per_prompt_ns)\n",
    "                    if len(N) == 0 or np.all(np.isnan(A)):\n",
    "                        continue\n",
    "\n",
    "                    # --- fisher-Z weighted aggregation ---\n",
    "                    z = np.arctanh(np.clip(A, -0.999999, 0.999999))\n",
    "                    z_mean = np.average(z, weights=N)\n",
    "                    rho = np.tanh(z_mean)\n",
    "                    n_mean, n_median = np.nanmean(N), np.nanmedian(N)\n",
    "\n",
    "                    # --- bootstrap CI ---\n",
    "                    if len(A) >= min_valid:\n",
    "                        boot_rhos = []\n",
    "                        for _ in range(n_boot):\n",
    "                            idx = np.random.choice(len(A), len(A), replace=True)\n",
    "                            z_b = np.arctanh(np.clip(A[idx], -0.999999, 0.999999))\n",
    "                            boot_rhos.append(np.tanh(np.average(z_b, weights=N[idx])))\n",
    "                        ci_low, ci_high = np.percentile(boot_rhos, [2.5, 97.5])\n",
    "                        rho_boot = np.median(boot_rhos)\n",
    "                    else:\n",
    "                        ci_low = ci_high = rho_boot = np.nan\n",
    "\n",
    "                    # --- permutation test ---\n",
    "                    if len(A) >= min_valid:\n",
    "                        perm_rhos = []\n",
    "                        for _ in range(n_perm):\n",
    "                            perm_z = np.arctanh(np.clip(np.random.permutation(A), -0.999999, 0.999999))\n",
    "                            perm_rhos.append(np.tanh(np.average(perm_z, weights=N)))\n",
    "                        perm_rhos = np.array(perm_rhos)\n",
    "                        p_perm = (np.sum(np.abs(perm_rhos) >= abs(rho)) + 1) / (len(perm_rhos) + 1)\n",
    "                    else:\n",
    "                        p_perm = np.nan\n",
    "\n",
    "                    results.append({\n",
    "                        \"mode\": mode, \"anchor\": anchor, \"metric\": metric + suffix,\n",
    "                        \"corr_type\": cname, \"layer_name\": lname, \"layer_index\": lidx,\n",
    "                        \"rho\": rho, \"rho_boot_median\": rho_boot,\n",
    "                        \"ci_low\": ci_low, \"ci_high\": ci_high, \"p_perm\": p_perm,\n",
    "                        \"n_prompts\": len(A), \"n_used_mean\": n_mean, \"n_used_median\": n_median,\n",
    "                        \"pooling\": \"per_prompt\", \"reason\": \"ok\"\n",
    "                    })\n",
    "\n",
    "    df_corr = pd.DataFrame(results)\n",
    "    print(f\"[ok] {anchor} â†’ {len(df_corr)} per-prompt correlations\")\n",
    "    print(f\"[NaN correlations: {df_corr['rho'].isna().sum()}]\")\n",
    "    print(f\"[skipped constant={skip_const}]\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = Path(output_dir) / f\"lw_{model}_{anchor}_corr_perprompt.csv\"\n",
    "        df_corr.to_csv(out_path, index=False)\n",
    "        print(f\"[saved per-prompt correlations] {out_path}\")\n",
    "\n",
    "    # --- per-layer Fisher-Z summary ---\n",
    "    valid = df_corr[np.isfinite(df_corr[\"rho\"])]\n",
    "    if not valid.empty:\n",
    "        layer_stats = []\n",
    "        for (lname, lidx), g in valid.groupby([\"layer_name\", \"layer_index\"]):\n",
    "            z_vals = np.arctanh(np.clip(g[\"rho\"], -0.999999, 0.999999))\n",
    "            weights = g[\"n_prompts\"].fillna(1)\n",
    "            z_mean = np.average(z_vals, weights=weights)\n",
    "            rho_layer = np.tanh(z_mean)\n",
    "            layer_stats.append({\"layer_name\": lname, \"layer_index\": lidx, \"rho_layer\": rho_layer})\n",
    "        df_layer = pd.DataFrame(layer_stats)\n",
    "        rho_global = np.tanh(np.average(np.arctanh(df_layer[\"rho_layer\"]), weights=None))\n",
    "        print(f\"[global per-prompt Fisher-Z rho] {rho_global:.3f} over {len(df_layer)} layers\")\n",
    "    else:\n",
    "        df_layer = pd.DataFrame()\n",
    "        rho_global = np.nan\n",
    "        print(\"[global per-prompt Fisher-Z rho] not computable\")\n",
    "\n",
    "    return df_corr, df_layer, rho_global\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# summarize_correlations_perprompt (fix for single-row groups)\n",
    "# ============================================================\n",
    "def summarize_correlations_perprompt(df_corr, output_dir, model, ci_mode=\"both\"):\n",
    "    df = df_corr.copy()\n",
    "\n",
    "    df = df.dropna(subset=[\"rho\"]).copy()\n",
    "    if \"n_prompts\" in df.columns:\n",
    "        df[\"n\"] = df[\"n_prompts\"]\n",
    "    elif \"n\" not in df.columns:\n",
    "        df[\"n\"] = 1\n",
    "\n",
    "    # --- fisher-Z transform ---\n",
    "    df[\"z\"] = np.nan\n",
    "    mask_finite = np.isfinite(df[\"rho\"])\n",
    "    df.loc[mask_finite, \"z\"] = np.arctanh(np.clip(df.loc[mask_finite, \"rho\"], -0.999999, 0.999999))\n",
    "\n",
    "    drop_cols = [c for c in [\"prompt_id\", \"batch_index\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    group_cols = [\"mode\", \"anchor\", \"metric\", \"corr_type\", \"layer_name\", \"layer_index\"]\n",
    "\n",
    "    # --- weighted stats per layer ---\n",
    "    def _weighted_stats(g):\n",
    "        out = {}\n",
    "        if not np.any(np.isfinite(g[\"rho\"])) or g[\"n\"].sum() == 0:\n",
    "            for key in [\n",
    "                \"rho_mean\", \"rho_low\", \"rho_high\",\n",
    "                \"rho_boot_mean\", \"ci_low_emp\", \"ci_high_emp\", \"n_total\"\n",
    "            ]:\n",
    "                out[key] = np.nan\n",
    "            out[\"n_total\"] = 0\n",
    "        else:\n",
    "            w = g[\"n\"].fillna(1)\n",
    "            z = np.arctanh(np.clip(g[\"rho\"], -0.999999, 0.999999))\n",
    "            z_mean = np.average(z, weights=w)\n",
    "            z_std = np.sqrt(np.average((z - z_mean)**2, weights=w))\n",
    "            out[\"rho_mean\"] = np.tanh(z_mean)\n",
    "            out[\"rho_low\"] = np.tanh(z_mean - 1.96 * z_std)\n",
    "            out[\"rho_high\"] = np.tanh(z_mean + 1.96 * z_std)\n",
    "            out[\"n_total\"] = g[\"n\"].sum()\n",
    "            out[\"rho_boot_mean\"] = np.average(g[\"rho_boot_median\"].fillna(0), weights=w)\n",
    "            out[\"ci_low_emp\"] = np.average(g[\"ci_low\"].fillna(0), weights=w)\n",
    "            out[\"ci_high_emp\"] = np.average(g[\"ci_high\"].fillna(0), weights=w)\n",
    "\n",
    "        for k in group_cols:\n",
    "            out[k] = g[k].iloc[0]\n",
    "        return pd.DataFrame([out])\n",
    "\n",
    "    df_summary = pd.concat(\n",
    "        [_weighted_stats(g) for _, g in df.groupby(group_cols, group_keys=False)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # --- CI mode handling ---\n",
    "    if ci_mode in (\"empirical\", \"both\"):\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"ci_low_emp\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"ci_high_emp\"]\n",
    "    else:\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"rho_low\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"rho_high\"]\n",
    "\n",
    "    # --- compute global Spearman across layers ---\n",
    "    valid = df_summary[np.isfinite(df_summary[\"rho_mean\"])]\n",
    "    if not valid.empty:\n",
    "        z_vals = np.arctanh(valid[\"rho_mean\"].clip(-0.999999, 0.999999))\n",
    "        weights = valid[\"n_total\"].fillna(1)\n",
    "        z_mean = np.average(z_vals, weights=weights)\n",
    "        rho_global = np.tanh(z_mean)\n",
    "        n_valid_layers = len(valid)\n",
    "        print(f\"[global per-prompt Fisher-Z rho] {rho_global:.3f} (across {n_valid_layers} valid layers)\")\n",
    "    else:\n",
    "        rho_global = np.nan\n",
    "        n_valid_layers = 0\n",
    "        print(\"[global per-prompt Fisher-Z rho] not computable (no valid layers)\")\n",
    "\n",
    "    # --- report layers without variance or missing ---\n",
    "    if \"reason\" in df_corr.columns:\n",
    "        missing_info = df_corr[df_corr[\"reason\"].isin([\"no_variance\", \"no_data\"])]\n",
    "        if not missing_info.empty:\n",
    "            for metric in missing_info[\"metric\"].unique():\n",
    "                n_missing = len(missing_info[missing_info[\"metric\"] == metric])\n",
    "                print(f\"[info] {metric}: {n_missing} layers had no variance or missing data\")\n",
    "\n",
    "    # --- save ---\n",
    "    out_summary = Path(output_dir) / f\"lw_{model}_corr_perprompt_summary_{ci_mode}.csv\"\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "    print(f\"[saved summary] {out_summary}\")\n",
    "    print(f\"[diag] rows={len(df_summary)} groups={df[group_cols].drop_duplicates().shape[0]}\")\n",
    "    print(f\"[NaN summary rows: {df_summary['rho_mean'].isna().sum()}]\")\n",
    "\n",
    "    return df_summary, rho_global, n_valid_layers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run per-prompt correlation pipeline\n",
    "# ============================================================\n",
    "BASE = Path(\"saved_data\")\n",
    "model = \"m_quant\"\n",
    "output_dir = BASE / \"summary\" / model\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "topk_dir = BASE / \"topk\" / model\n",
    "df_topk = merge_parquet_files(topk_dir)\n",
    "df_topk = expand_topk_metrics(\n",
    "    df_topk,\n",
    "    metrics=[\"jaccard\", \"acc_A\", \"acc_B\", \"disagree_correct\"],\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\")\n",
    ")\n",
    "df_topk = preprocess_anchors(df_topk)\n",
    "\n",
    "for m in [\"cosine_sim\", \"l2_dist\"]:\n",
    "    if f\"{m}_raw\" in df_topk.columns:\n",
    "        for mode in [\"unit_rms\", \"norm_rms\"]:\n",
    "            col_src = f\"{m}_raw\"\n",
    "            col_dst = f\"{m}_{mode}\"\n",
    "            if col_dst not in df_topk.columns:\n",
    "                df_topk[col_dst] = df_topk[col_src]\n",
    "                print(f\"[copy] propagated {col_src} â†’ {col_dst}\")\n",
    "\n",
    "anchors = [\"disagree_correct\", \"p_diff\"]\n",
    "metrics = [\n",
    "    \"kl_ab\", \"kl_ba\", \"js_div\", \"js_dist\", \"tvd\",\n",
    "    \"entropy_A\", \"entropy_B\", \"cosine_sim\", \"l2_dist\",\n",
    "    \"ppl_diff\", \"jaccard\", \"acc_A\", \"acc_B\"\n",
    "]\n",
    "\n",
    "df_corr_perprompt_all = []\n",
    "for a in anchors:\n",
    "    df_corr_perprompt_all.append(\n",
    "        correlate_layers_by_anchor_perprompt(df_topk, a, metrics, model=model)\n",
    "    )\n",
    "df_corr_perprompt_all = pd.concat(df_corr_perprompt_all, ignore_index=True)\n",
    "\n",
    "perprompt_corr_path = output_dir / f\"lw_{model}_corr_perprompt.csv\"\n",
    "perprompt_summary_path = output_dir / f\"lw_{model}_corr_perprompt_summary.csv\"\n",
    "\n",
    "df_corr_perprompt_all.to_csv(perprompt_corr_path, index=False)\n",
    "print(f\"[saved per-prompt correlations] {perprompt_corr_path}\")\n",
    "\n",
    "df_summary_pp = summarize_correlations_perprompt(df_corr_perprompt_all, output_dir=output_dir, model=model)\n",
    "print(f\"[saved per-prompt summary] {perprompt_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_weight_differences(modelA, modelB, modelA_name=\"A\", modelB_name=\"B\"):\n",
    "    diffs = []\n",
    "    paramsB = dict(modelB.named_parameters())\n",
    "    for name, paramA in modelA.named_parameters():\n",
    "        if \"weight\" not in name or name not in paramsB:\n",
    "            continue\n",
    "        paramB = paramsB[name]\n",
    "        diff = torch.norm(paramA - paramB) / torch.norm(paramA)\n",
    "        layer_idx = -1\n",
    "        if \"layers.\" in name:\n",
    "            try:\n",
    "                layer_idx = int(name.split(\"layers.\")[1].split(\".\")[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "        diffs.append({\n",
    "            \"param_name\": name,\n",
    "            \"layer_index\": layer_idx,\n",
    "            \"weight_diff\": diff.item()\n",
    "        })\n",
    "    df_diff = pd.DataFrame(diffs)\n",
    "    out_path = Path(\"saved_data/weights\") / f\"diff_{modelA_name}_{modelB_name}.parquet\"\n",
    "    df_diff.to_parquet(out_path, index=False)\n",
    "    print(f\"[saved weight diffs] {out_path}\")\n",
    "    return df_diff\n",
    "\n",
    "df_diff = pd.read_parquet(\"saved_data/weights/diff_m_orig_m_quant.parquet\")\n",
    "df_summary = pd.read_csv(\"saved_data/summary/m_quant/lw_m_quant_corr_perprompt_summary_both.csv\")\n",
    "\n",
    "merged = pd.merge(df_summary, df_diff, on=\"layer_index\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def merge_weight_diffs_with_summary(summary_path, weight_path, output_dir=None, save_suffix=\"_weighted\"):\n",
    "    summary_path, weight_path = Path(summary_path), Path(weight_path)\n",
    "    if output_dir is None:\n",
    "        output_dir = summary_path.parent\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ”¹ Loading correlation summary from: {summary_path}\")\n",
    "    df_summary = pd.read_csv(summary_path)\n",
    "    print(f\"   Rows: {len(df_summary)}, Columns: {list(df_summary.columns)[:8]}...\")\n",
    "\n",
    "    print(f\"ðŸ”¹ Loading weight differences from: {weight_path}\")\n",
    "    df_w = pd.read_parquet(weight_path)\n",
    "    print(f\"   Rows: {len(df_w)}, Columns: {list(df_w.columns)}\")\n",
    "\n",
    "    merged = pd.merge(df_summary, df_w, on=\"layer_index\", how=\"left\")\n",
    "    if \"weight_diff\" not in merged.columns:\n",
    "        raise ValueError(\"Weight file must contain 'layer_index' and 'weight_diff' columns.\")\n",
    "\n",
    "    print(f\"Merged summary with weights â†’ {len(merged)} rows\")\n",
    "\n",
    "    results = []\n",
    "    for metric, g in merged.groupby(\"metric\"):\n",
    "        valid = g[[\"weight_diff\", \"rho_mean\"]].dropna()\n",
    "        if len(valid) < 3 or np.std(valid[\"weight_diff\"]) == 0 or np.std(valid[\"rho_mean\"]) == 0:\n",
    "            rho, p = np.nan, np.nan\n",
    "        else:\n",
    "            rho, p = spearmanr(valid[\"weight_diff\"], valid[\"rho_mean\"])\n",
    "        results.append({\"metric\": metric, \"rho_wcorr\": rho, \"p_wcorr\": p, \"n_layers\": len(valid)})\n",
    "\n",
    "    df_metric_corr = pd.DataFrame(results)\n",
    "    print(\"\\nWeight correlation summary per metric:\")\n",
    "    print(df_metric_corr)\n",
    "\n",
    "    merged = merged.merge(df_metric_corr[[\"metric\", \"rho_wcorr\", \"p_wcorr\"]], on=\"metric\", how=\"left\")\n",
    "\n",
    "    out_path = output_dir / f\"{summary_path.stem}{save_suffix}.csv\"\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved merged summary with weight correlations â†’ {out_path}\")\n",
    "\n",
    "    return merged, df_metric_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_perprompt_all.query(\"metric.str.contains('cosine_sim')\").groupby(\"mode\")[\"rho\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_perprompt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12453629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_perprompt_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ae5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_perprompt.csv\")\n",
    "df_sum = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_perprompt_summary_both.csv\")\n",
    "\n",
    "merged = df_sum.merge(df_raw, on=[\"mode\",\"anchor\",\"metric\",\"corr_type\",\"layer_index\"], suffixes=(\"_sum\",\"_raw\"))\n",
    "merged[\"diff\"] = merged[\"rho_mean\"] - merged[\"rho\"]\n",
    "\n",
    "print(merged[\"diff\"].abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae48d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_pooled.csv\")\n",
    "df_sum = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_pooled_summary_both.csv\")\n",
    "\n",
    "merged = df_sum.merge(df_raw, on=[\"mode\",\"anchor\",\"metric\",\"corr_type\",\"layer_index\"], suffixes=(\"_sum\",\"_raw\"))\n",
    "merged[\"diff\"] = merged[\"rho_mean\"] - merged[\"rho\"]\n",
    "\n",
    "print(merged[\"diff\"].abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    for f in (BASE_DIR / model).glob(\"*_summary*.csv\"):\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    print(f\"\\n=== {pooling.upper()} ===\")\n",
    "    d = df[df[\"pooling\"] == pooling]\n",
    "    all_layers = sorted(df[\"layer_index\"].unique())\n",
    "    print(f\"Alle mulige lag: {all_layers[:10]}... ({len(all_layers)} total)\")\n",
    "    for model in MODELS:\n",
    "        layers = sorted(d[d[\"model\"] == model][\"layer_index\"].unique())\n",
    "        missing = sorted(set(all_layers) - set(layers))\n",
    "        print(f\"{model}: {len(layers)} lag fundet, mangler {len(missing)} â†’ {missing[:10] if missing else 'ingen mangler'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[\n",
    "    (df[\"anchor\"] == \"disagree_correct\") &\n",
    "    (df[\"corr_type\"] == \"pointbiserial\")\n",
    "]\n",
    "\n",
    "print(\"Unique layers per model (per_prompt):\")\n",
    "for model in df[\"model\"].unique():\n",
    "    d = subset[(subset[\"model\"] == model) & (df[\"pooling\"] == \"per_prompt\")]\n",
    "    print(f\"{model}: {sorted(d['layer_index'].unique())[:10]} ... total {d['layer_index'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1fd30",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Plot TopK Correlations =======================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a89039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# === STYLE ===\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_rawcorr_all\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"rho_smooth\"] = df[\"rho\"]  # raw values only\n",
    "\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "print(df.groupby([\"anchor\",\"corr_type\",\"pooling\"])[\"rho\"].describe().round(3))\n",
    "\n",
    "# === HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def _mark_nans(ax, dsub):\n",
    "    d_nan = dsub[dsub[\"rho\"].isna()]\n",
    "    if not d_nan.empty:\n",
    "        for i, model in enumerate(sorted(d_nan[\"model\"].unique())):\n",
    "            d_m = d_nan[d_nan[\"model\"] == model]\n",
    "            ax.scatter(\n",
    "                d_m[\"layer_index\"],\n",
    "                [-0.95 + 0.05*i] * len(d_m),\n",
    "                color=\"red\", marker=\"x\", s=50, label=f\"{model} NaN\"\n",
    "            )\n",
    "\n",
    "# === PLOT FUNCTIONS ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmode)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmet)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            _mark_nans(ax, dmode)\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} â€” {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial & Phi)\n",
    "    for corr_type in [\"pointbiserial\", \"phi\"]:\n",
    "        for anchor in ANCHORS_BINARY:\n",
    "            df_b = df[\n",
    "                (df[\"anchor\"] == anchor)\n",
    "                & (df[\"corr_type\"] == corr_type)\n",
    "                & (df[\"pooling\"] == pooling)\n",
    "                & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "            ]\n",
    "            if df_b.empty:\n",
    "                continue\n",
    "            if LAYOUT_MODE == \"mode\":\n",
    "                plot_by_mode(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"metric\":\n",
    "                plot_by_metric(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"group\":\n",
    "                plot_by_group(df_b, corr_type, anchor, pooling, out_dir)\n",
    "\n",
    "# === MIXED ALL-COMBINATION PLOTS ===\n",
    "print(\"\\n[mixed plotting] Generating full cross-anchor Ã— metric plots...\")\n",
    "\n",
    "ALL_CORR_TYPES = [\"phi\", \"pointbiserial\", \"spearman\"]\n",
    "\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for anchor in ANCHORS_BINARY + ANCHORS_CONTINUOUS:\n",
    "        df_all = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"corr_type\"].isin(ALL_CORR_TYPES))\n",
    "        ]\n",
    "        if df_all.empty:\n",
    "            print(f\"[skip] no data for {anchor} ({pooling})\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[mixed] {anchor} ({pooling}) â†’ {df_all['metric'].nunique()} metrics plotted\")\n",
    "        plot_by_metric(df_all, \"mixed_all\", anchor, pooling, out_dir)\n",
    "        plot_by_group(df_all, \"mixed_all\", anchor, pooling, out_dir)\n",
    "\n",
    "# === QUICK TOPLIST ===\n",
    "top = (\n",
    "    df.groupby([\"anchor\", \"metric\", \"corr_type\", \"model\"])[\"rho\"]\n",
    "      .mean().reset_index()\n",
    "      .sort_values(\"rho\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop correlations:\")\n",
    "print(top.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"metric == 'ppl_diff'\")[[\"rho\",\"corr_type\",\"anchor\",\"pooling\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd887014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# === STYLE ===\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"rho_smooth\"] = df[\"rho\"]  # raw values only\n",
    "\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "print(df.groupby([\"anchor\",\"corr_type\",\"pooling\"])[\"rho\"].describe().round(3))\n",
    "\n",
    "# === HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def _mark_nans(ax, dsub):\n",
    "    d_nan = dsub[dsub[\"rho\"].isna()]\n",
    "    if not d_nan.empty:\n",
    "        for i, model in enumerate(sorted(d_nan[\"model\"].unique())):\n",
    "            d_m = d_nan[d_nan[\"model\"] == model]\n",
    "            ax.scatter(\n",
    "                d_m[\"layer_index\"],\n",
    "                [-0.95 + 0.05*i] * len(d_m),  # lidt forskudt pr. model\n",
    "                color=\"red\", marker=\"x\", s=50, label=f\"{model} NaN\"\n",
    "            )\n",
    "\n",
    "# === PLOT FUNCTIONS ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmode)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmet)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            _mark_nans(ax, dmode)\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} â€” {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial & Phi)\n",
    "    for corr_type in [\"pointbiserial\", \"phi\"]:\n",
    "        for anchor in ANCHORS_BINARY:\n",
    "            df_b = df[\n",
    "                (df[\"anchor\"] == anchor)\n",
    "                & (df[\"corr_type\"] == corr_type)\n",
    "                & (df[\"pooling\"] == pooling)\n",
    "                & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "            ]\n",
    "            if df_b.empty:\n",
    "                continue\n",
    "            if LAYOUT_MODE == \"mode\":\n",
    "                plot_by_mode(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"metric\":\n",
    "                plot_by_metric(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"group\":\n",
    "                plot_by_group(df_b, corr_type, anchor, pooling, out_dir)\n",
    "\n",
    "# === QUICK TOPLIST ===\n",
    "top = (\n",
    "    df.groupby([\"anchor\", \"metric\", \"corr_type\", \"model\"])[\"rho\"]\n",
    "      .mean().reset_index()\n",
    "      .sort_values(\"rho\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop correlations:\")\n",
    "print(top.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f88821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# === STYLE ===\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_flexible_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# === RAW VALUES (no smoothing) ===\n",
    "df[\"rho_smooth\"] = df[\"rho\"]\n",
    "\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "print(df.groupby([\"anchor\",\"corr_type\",\"pooling\"])[\"rho\"].describe().round(3))\n",
    "\n",
    "# === HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# === ADD NAN MARKERS ===\n",
    "def _mark_nans(ax, dsub):\n",
    "    d_nan = dsub[dsub[\"rho\"].isna()]\n",
    "    if not d_nan.empty:\n",
    "        ax.scatter(\n",
    "            d_nan[\"layer_index\"],\n",
    "            [0] * len(d_nan),\n",
    "            color=\"red\", marker=\"x\", s=60, label=\"NaN / no corr\"\n",
    "        )\n",
    "\n",
    "# === PLOT FUNCTIONS ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmode)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmet)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            _mark_nans(ax, dmode)\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} â€” {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial & Phi)\n",
    "    for corr_type in [\"pointbiserial\", \"phi\"]:\n",
    "        for anchor in ANCHORS_BINARY:\n",
    "            df_b = df[\n",
    "                (df[\"anchor\"] == anchor)\n",
    "                & (df[\"corr_type\"] == corr_type)\n",
    "                & (df[\"pooling\"] == pooling)\n",
    "                & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "            ]\n",
    "            if df_b.empty:\n",
    "                continue\n",
    "            if LAYOUT_MODE == \"mode\":\n",
    "                plot_by_mode(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"metric\":\n",
    "                plot_by_metric(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"group\":\n",
    "                plot_by_group(df_b, corr_type, anchor, pooling, out_dir)\n",
    "\n",
    "# === QUICK TOPLIST ===\n",
    "top = (\n",
    "    df.groupby([\"anchor\", \"metric\", \"corr_type\", \"model\"])[\"rho\"]\n",
    "      .mean().reset_index()\n",
    "      .sort_values(\"rho\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop correlations:\")\n",
    "print(top.head(20).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a84c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_flexible_clean\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*_summary*.csv\"): \n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No summary files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho_mean\",\"rho_low\",\"rho_high\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "\n",
    "\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    \"\"\"Return dynamic rows/cols for a clean subplot grid.\"\"\"\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    \"\"\"Uniform cleanup for figure layout.\"\"\"\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    modes = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "\n",
    "    for ax, mode in zip(axes, modes):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_mean\",\n",
    "            hue=\"model\", style=\"metric\", markers=False,\n",
    "            lw=2, ax=ax\n",
    "        )\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Mean Ï\" if mode == \"raw\" else \"\")\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_mean\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_mean\",\n",
    "                hue=\"model\", style=\"metric\", lw=2,\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} â€” {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous anchors Spearman\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary anchors Point-biserial\n",
    "    for anchor in ANCHORS_BINARY:\n",
    "        df_b = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"pointbiserial\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "        ]\n",
    "        if df_b.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_flexible_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "\n",
    "# === LOAD RAW CORR FILES (not summaries) ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue  # skip summaries\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "print(f\"[ok] merged {len(df)} raw correlation rows from {df['model'].nunique()} models.\")\n",
    "\n",
    "\n",
    "# === PLOTTING HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# === PLOT FUNCTIONS (identiske men y='rho') ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    modes = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "    for ax, mode in zip(axes, modes):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes_raw.png\")\n",
    "\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Ï\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} â€” {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics_raw.png\")\n",
    "\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"Ï\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} â€” {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}_raw.png\")\n",
    "\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial)\n",
    "    for anchor in ANCHORS_BINARY:\n",
    "        df_b = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"pointbiserial\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "        ]\n",
    "        if df_b.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b619394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_heatmaps_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df[\"rho\"] = pd.to_numeric(df[\"rho\"], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "\n",
    "\n",
    "# === HEATMAP FUNCTION ===\n",
    "def plot_heatmap(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    \"\"\"\n",
    "    Plots rho values as a heatmap (metric x layer) for each model/mode combo.\n",
    "    \"\"\"\n",
    "    modes = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "    for mode in modes:\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            continue\n",
    "\n",
    "        for model in dmode[\"model\"].unique():\n",
    "            dmodel = dmode[dmode[\"model\"] == model]\n",
    "            if dmodel.empty:\n",
    "                continue\n",
    "\n",
    "            # Pivot so that rows = metric, columns = layer_index\n",
    "            heat = dmodel.pivot_table(\n",
    "                index=\"metric\", columns=\"layer_index\", values=\"rho\", aggfunc=\"mean\"\n",
    "            ).sort_index()\n",
    "\n",
    "            plt.figure(figsize=(12, max(6, 0.4 * len(heat))))\n",
    "            sns.heatmap(\n",
    "                heat, cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\",\n",
    "                cbar_kws={\"label\": \"Ï (correlation)\"}, linewidths=0.4\n",
    "            )\n",
    "            plt.title(f\"{model} â€” {anchor} ({pooling}, {corr_type}, {mode})\", fontsize=13, weight=\"bold\")\n",
    "            plt.xlabel(\"Layer index\")\n",
    "            plt.ylabel(\"Metric\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            out_path = out_dir / f\"{model}_{corr_type}_{anchor}_{pooling}_{mode}_heatmap.png\"\n",
    "            plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"[saved] {out_path}\")\n",
    "\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous anchors (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if not df_s.empty:\n",
    "            plot_heatmap(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary anchors (Point-biserial)\n",
    "    for anchor in ANCHORS_BINARY:\n",
    "        df_b = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"pointbiserial\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "        ]\n",
    "        if not df_b.empty:\n",
    "            plot_heatmap(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit-lens-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
