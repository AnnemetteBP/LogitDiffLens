{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6294ae",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Notebook for Logit Lens and Hidden Acts ======\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, DownloadMode\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from old_lens.mi_utils.quant_configs.bnb_configs import load_bnb_in_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f24fe2",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Models & Dataset =============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340878a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\LogitLensData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:1000]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")\n",
    "\n",
    "nq_queries = nq_dataset['train']['query']\n",
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_1000 = nq_queries[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_500 = nq_queries[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Models(Enum):\n",
    "    LAIN8B = \"Models/LLaMA3Instruct\"\n",
    "    HF100B = \"Models/HF1BitLLM100Btokens\"\n",
    "\n",
    "\n",
    "class Names(Enum):\n",
    "    LAIN8B = \"Meta-Llama-3-8B-Instruct-fp\"\n",
    "    HF100B = \"Llama3-8B-1.58-100B-tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tok(\n",
    "    model_name: str,\n",
    "    low_cpu_mem_usage: bool = True,\n",
    "    local_files_only: bool = True,\n",
    "    device_map: str = \"cpu\",\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    load_in_8bit: bool = False\n",
    "):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        return_dict=True,\n",
    "        output_attentions=True,\n",
    "        low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "        local_files_only=local_files_only,\n",
    "        device_map=device_map,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        torch_dtype=dtype,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    return model, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig, orig_tokenizer = load_model_and_tok(Models.LAIN8B.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = orig_tokenizer.eos_token_id\n",
    "bos_token_id = orig_tokenizer.bos_token_id\n",
    "print(f\"eos: {eos_token_id}\\nbos: {bos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50589c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8bit, orig_tokenizer = load_model_and_tok(Models.LAIN8B.value, dtype=torch.float16, load_in_8bit=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit, orig_tokenizer = load_bnb_in_4bit(Models.LAIN8B.value, double_quant=False, dtype=torch.float16, device_map=\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant, quant_tokenizer = load_model_and_tok(Models.HF100B.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdeb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for q in nq_500:\n",
    "    ids = orig_tokenizer.encode(q, add_special_tokens=True)\n",
    "    lengths.append(len(ids))\n",
    "\n",
    "print(\"=== Token Length Statistics (LLaMA-3-8B-Instruct tokenizer) ===\")\n",
    "print(f\"Samples analyzed: {len(lengths)}\")\n",
    "print(f\"Mean length:       {np.mean(lengths):.2f}\")\n",
    "print(f\"Median length:     {np.median(lengths):.0f}\")\n",
    "print(f\"90th percentile:   {np.percentile(lengths, 90):.0f}\")\n",
    "print(f\"95th percentile:   {np.percentile(lengths, 95):.0f}\")\n",
    "print(f\"Max observed len:  {np.max(lengths)}\")\n",
    "print(f\"Min observed len:  {np.min(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "sig = inspect.signature(model_orig.model.layers[0].forward)\n",
    "print(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL = \"\"\n",
    "SAVE_DIR = \"weights_extracted\" \n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_and_save_weights(model_name: str, model_cfg: dict):\n",
    "    print(f\"\\n🔹 Extracting weights from {model_name} ...\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_cfg[\"path\"],\n",
    "        quantization_config=model_cfg[\"quant\"],\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    out_dir = os.path.join(SAVE_DIR, model_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        w = param.detach().clone().to(torch.float32).cpu()\n",
    "\n",
    "        fname = name.replace(\".\", \"_\") + \".pt\"\n",
    "        torch.save(w, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"Saved float32 weights for {model_name} to {out_dir}\")\n",
    "\n",
    "    # small sanity check\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Example tensor dtype: {w.dtype}, shape: {tuple(w.shape)}\")\n",
    "\n",
    "\n",
    "\n",
    "for model_name, model_cfg in MODEL.items():\n",
    "    extract_and_save_weights(model_name, model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, glob\n",
    "\n",
    "base_w = torch.load(\"weights_extracted/base_fp32/model_layers_0_self_attn_q_proj_weight.pt\")\n",
    "bnb_w = torch.load(\"weights_extracted/bnb_8bit/model_layers_0_self_attn_q_proj_weight.pt\")\n",
    "\n",
    "print(base_w.dtype, bnb_w.dtype)\n",
    "print(\"Mean abs diff:\", (base_w - bnb_w).abs().mean().item())\n",
    "print(\"Cosine similarity:\", torch.nn.functional.cosine_similarity(\n",
    "    base_w.flatten(), bnb_w.flatten(), dim=0\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b729d26",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Logit Lens with Normaliztion =================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37206e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"MatMul8bitLt: inputs will be cast\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Normalization modes\n",
    "# ============================================================\n",
    "def apply_normalization(x, model, normalize_mode=\"raw\", block=None, layer_index=None):\n",
    "    x = x.to(torch.float32) \n",
    "    \n",
    "    if normalize_mode == \"raw\":\n",
    "        return x\n",
    "\n",
    "    elif normalize_mode == \"unit_rms\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            eps = 1e-5\n",
    "            return x / (x.pow(2).mean(dim=-1, keepdim=True).add(eps).sqrt())\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    elif normalize_mode == \"rms_layer\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            return block.post_attention_layernorm(x.to(torch.float32))\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    elif normalize_mode == \"norm_rms\":\n",
    "        if layer_index == -1:\n",
    "            return x\n",
    "        elif block is not None:\n",
    "            return model.model.norm(x).to(torch.float32)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization_mode: {normalize_mode}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper: causal + padding mask for LLaMA blocks\n",
    "# ============================================================\n",
    "def build_full_attention_mask(input_ids, attention_mask, device, model=None):\n",
    "    bsz, seq_len = input_ids.shape\n",
    "\n",
    "    causal = torch.triu(\n",
    "        torch.ones((seq_len, seq_len), device=device, dtype=torch.bool), diagonal=1\n",
    "    ).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        padding_mask = torch.zeros((bsz, 1, 1, seq_len), device=device, dtype=torch.bool)\n",
    "    else:\n",
    "        padding_mask = (attention_mask[:, None, None, :] == 0)\n",
    "\n",
    "    full = causal | padding_mask\n",
    "\n",
    "    attn_impl = getattr(getattr(model, \"config\", None), \"attn_implementation\", None)\n",
    "    attn_impl = attn_impl or \"eager\" \n",
    "\n",
    "    if attn_impl in [\"flash_attention_2\", \"sdpa\"]:\n",
    "        return full.to(torch.bool)\n",
    "    else:\n",
    "        return full.to(torch.float32) * -1e9\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Collector with multiple normalization variants + attention weights (optional storage)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def collect_logit_lens_full(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    batch_index=0,\n",
    "    max_len=17,\n",
    "    device=None,\n",
    "    clamp_logits=False,         \n",
    "    clamp_value=100.0,         \n",
    "    save_path=None,\n",
    "    collect_attn=True,\n",
    "    save_attn=True,\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "):\n",
    "\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    bnb_layer_types = (\"Linear4bit\", \"Linear8bitLt\")\n",
    "    is_quantized = any(any(name in type(m).__name__ for name in bnb_layer_types)\n",
    "                       for m in model.modules())\n",
    "\n",
    "    if is_quantized:\n",
    "        try:\n",
    "            first_param_device = next(model.parameters()).device\n",
    "        except StopIteration:\n",
    "            first_param_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[info] Detected quantized model → device {first_param_device}\")\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = model.to(device).eval()\n",
    "\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # Tokenization\n",
    "    # ============================================================\n",
    "    encoded = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    for p in prompts:\n",
    "        ids = tokenizer.encode(p, add_special_tokens=False)\n",
    "        content = ids[: max_len - 2]\n",
    "        ids = torch.tensor([bos_id] + content + [eos_id], dtype=torch.long)\n",
    "        if len(ids) < max_len:\n",
    "            ids = F.pad(ids, (0, max_len - len(ids)), value=pad_id)\n",
    "        encoded.append(ids)\n",
    "\n",
    "    input_ids = torch.stack(encoded, dim=0).to(device)\n",
    "\n",
    "    # ============================================================\n",
    "    # Build attention mask (stop after first EOS)\n",
    "    # ============================================================\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        eos_positions = (ids == eos_id).nonzero(as_tuple=True)[0]\n",
    "        if len(eos_positions) > 0:\n",
    "            eos_pos = eos_positions[0].item()\n",
    "            attention_mask[i, eos_pos + 1:] = 0\n",
    "\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    full_mask = build_full_attention_mask(input_ids, attention_mask, device)\n",
    "    #print(f\"[mask] dtype={full_mask.dtype}, shape={tuple(full_mask.shape)}, example={full_mask.flatten()[0].item()}\")\n",
    "    print(\n",
    "        f\"[mask] dtype={full_mask.dtype}, shape={tuple(full_mask.shape)}, \"\n",
    "        f\"min={full_mask.min().item()}, max={full_mask.max().item()}, \"\n",
    "        f\"unique={torch.unique(full_mask)}\"\n",
    "    )\n",
    "    assert (full_mask == 0).sum() < full_mask.numel(), \"Mask seems to contain only zeros — check logic!\"\n",
    "\n",
    "    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "    vocab_size = model.lm_head.out_features\n",
    "\n",
    "    print(f\"[info] Tokenized {batch_size} prompts | seq_len={seq_len}\")\n",
    "    print(f\"[info] Collecting from {len(model.model.layers)} layers | quantized={is_quantized}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Projection helper (with consistent upcasting and sanitization)\n",
    "    # ============================================================\n",
    "    def project(x):\n",
    "        x_fp32 = x.to(torch.float32)\n",
    "\n",
    "        head_dtype = next(model.lm_head.parameters()).dtype\n",
    "        x_cast = x_fp32.to(head_dtype)\n",
    "\n",
    "        logits = model.lm_head(x_cast)\n",
    "\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if clamp_logits and is_quantized:\n",
    "            logits = logits.clamp(-clamp_value, clamp_value)\n",
    "\n",
    "        return logits.to(torch.float32)\n",
    "\n",
    "    # ============================================================\n",
    "    # Collection containers\n",
    "    # ============================================================\n",
    "    rows = []\n",
    "    all_hidden, all_logits, all_attn = {}, {}, {}\n",
    "\n",
    "    # ============================================================\n",
    "    # Embedding layer\n",
    "    # ============================================================\n",
    "    x = model.model.embed_tokens(input_ids).to(torch.float32)\n",
    "    hidden_variants = {mode: apply_normalization(x.clone(), model, mode, layer_index=-1)\n",
    "                       for mode in norm_modes}\n",
    "    logits_variants = {mode: project(hidden_variants[mode]) for mode in norm_modes}\n",
    "\n",
    "    for mode in norm_modes:\n",
    "        all_hidden[f\"embed_tokens_{mode}\"] = hidden_variants[mode].cpu()\n",
    "        all_logits[f\"embed_tokens_{mode}\"] = logits_variants[mode].cpu()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rows.append({\n",
    "            \"prompt_id\": i,\n",
    "            \"prompt_text\": prompts[i],\n",
    "            \"batch_index\": batch_index,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_index\": -1,\n",
    "            \"layer_name\": \"embed_tokens\",\n",
    "            \"input_ids\": input_ids[i].cpu(),\n",
    "            \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "            \"attention_mask\": attention_mask[i].cpu(),\n",
    "            **{f\"hidden_{m}\": hidden_variants[m][i, :-1].cpu() for m in norm_modes},\n",
    "            **{f\"logits_{m}\": logits_variants[m][i, :-1].cpu() for m in norm_modes},\n",
    "        })\n",
    "\n",
    "    # ============================================================\n",
    "    # Transformer layers\n",
    "    # ============================================================\n",
    "    for li, block in enumerate(model.model.layers):\n",
    "        out = block(\n",
    "            x, position_ids=position_ids,\n",
    "            attention_mask=full_mask,\n",
    "            output_attentions=collect_attn,\n",
    "        )\n",
    "        x = out[0]\n",
    "        attn = out[1] if collect_attn else None\n",
    "\n",
    "        layer_output = x.detach().clone().to(torch.float32)\n",
    "        hidden_variants = {\n",
    "            mode: apply_normalization(layer_output.clone(), model, mode, block=block, layer_index=li)\n",
    "            for mode in norm_modes\n",
    "        }\n",
    "        logits_variants = {mode: project(hidden_variants[mode]) for mode in norm_modes}\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            hidden_variants[mode] = hidden_variants[mode][:, :-1, :]\n",
    "            logits_variants[mode] = logits_variants[mode][:, :-1, :]\n",
    "\n",
    "        for mode in norm_modes:\n",
    "            all_hidden[f\"layer.{li}_{mode}\"] = hidden_variants[mode].cpu()\n",
    "            all_logits[f\"layer.{li}_{mode}\"] = logits_variants[mode].cpu()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            record = {\n",
    "                \"prompt_id\": i,\n",
    "                \"prompt_text\": prompts[i],\n",
    "                \"batch_index\": batch_index,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"layer_index\": li,\n",
    "                \"layer_name\": f\"layer.{li}\",\n",
    "                \"input_ids\": input_ids[i].cpu(),\n",
    "                \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "                \"attention_mask\": attention_mask[i].cpu(),\n",
    "                **{f\"hidden_{m}\": hidden_variants[m][i].cpu() for m in norm_modes},\n",
    "                **{f\"logits_{m}\": logits_variants[m][i].cpu() for m in norm_modes},\n",
    "            }\n",
    "            if save_attn and attn is not None:\n",
    "                record[\"attn\"] = attn[i].cpu()\n",
    "            rows.append(record)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # ============================================================\n",
    "    # Final RMSNorm output\n",
    "    # ============================================================\n",
    "    x = model.model.norm(x.to(torch.float32))\n",
    "    h = x\n",
    "    l = project(h)\n",
    "    h, l = h[:, :-1, :], l[:, :-1, :]\n",
    "\n",
    "    all_hidden[\"output_true\"] = h.cpu()\n",
    "    all_logits[\"output_true\"] = l.cpu()\n",
    "\n",
    "    for mode in norm_modes:\n",
    "        all_hidden[f\"output_{mode}\"] = h.cpu()\n",
    "        all_logits[f\"output_{mode}\"] = l.cpu()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rows.append({\n",
    "            \"prompt_id\": i,\n",
    "            \"prompt_text\": prompts[i],\n",
    "            \"batch_index\": batch_index,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_index\": len(model.model.layers),\n",
    "            \"layer_name\": \"output\",\n",
    "            \"input_ids\": input_ids[i].cpu(),\n",
    "            \"target_ids\": input_ids[i, 1:].cpu(),\n",
    "            \"attention_mask\": attention_mask[i].cpu(),\n",
    "            **{f\"hidden_{m}\": h[i].cpu() for m in norm_modes},\n",
    "            **{f\"logits_{m}\": l[i].cpu() for m in norm_modes},\n",
    "        })\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Save and finish \n",
    "    # ============================================================\n",
    "    if save_path:\n",
    "        torch.save(rows, save_path)\n",
    "        print(f\"[saved] Logit-lens data → {save_path}\")\n",
    "\n",
    "\n",
    "    print(f\"[info] Model has {len(model.model.layers)} transformer blocks (plus embedding + output).\")\n",
    "\n",
    "    return rows, all_hidden, all_logits, all_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8653a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_logit_lens_in_batches(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    all_prompts,\n",
    "    batch_size=20,\n",
    "    save_prefix=\"logitlens_batch\",\n",
    "    max_len=17,\n",
    "    normalize_mode=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    device=None,\n",
    "    clamp_logits=False,\n",
    "    collect_attn=False,\n",
    "    save_attn=False \n",
    "):\n",
    "\n",
    "    num_batches = (len(all_prompts) + batch_size - 1) // batch_size\n",
    "    print(f\"[run] Processing {len(all_prompts)} prompts in {num_batches} batches of {batch_size}\")\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Running logit lens batches\"):\n",
    "        start = batch_idx * batch_size\n",
    "        end = min((batch_idx + 1) * batch_size, len(all_prompts))\n",
    "        batch_prompts = all_prompts[start:end]\n",
    "\n",
    "        save_path = f\"{save_prefix}_batch{batch_idx:03d}.pt\"\n",
    "\n",
    "        print(f\"\\n[batch {batch_idx+1}/{num_batches}] {len(batch_prompts)} prompts → {save_path}\")\n",
    "\n",
    "        try:\n",
    "            rows, hidden_dict, logits_dict, all_attn = collect_logit_lens_full(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                prompts=batch_prompts,\n",
    "                max_len=max_len,\n",
    "                device=device,\n",
    "                norm_modes=normalize_mode,\n",
    "                save_path=save_path,\n",
    "                clamp_logits=clamp_logits,\n",
    "                collect_attn=collect_attn,\n",
    "                save_attn=save_attn,\n",
    "            )\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[error] Batch {batch_idx} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        del rows, hidden_dict, logits_dict, all_attn, batch_prompts\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\n[done] All batches processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logit_lens_in_batches(\n",
    "    model=model_8bit,\n",
    "    tokenizer=orig_tokenizer,\n",
    "    all_prompts=nq_500,\n",
    "    batch_size=10,\n",
    "    max_len=17,\n",
    "    normalize_mode=(\"raw\", \"unit_rms\", \"norm_rms\"), \n",
    "    save_prefix=\"saved_data/lens_data/m_8bit/m_8bit_modes\",\n",
    "    device=\"cpu\",\n",
    "    clamp_logits=False,\n",
    "    collect_attn=False,\n",
    "    save_attn=False  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data = torch.load(\"saved_data/lens_data/m_orig/m_orig_modes_batch000.pt\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39183c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df = pd.DataFrame(ll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591013aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df[\"layer_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ef638",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_tokenizer.decode([128000]))\n",
    "print(orig_tokenizer.decode([128009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bee5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_tokenizer.decode([128000, 9906, 1917, 128009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37103286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_tokenizer.bos_token_id, orig_tokenizer.eos_token_id)\n",
    "print(model_8bit.config.bos_token_id, model_8bit.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = orig_tokenizer.encode(\"Hello world\", add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f33a9",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# TopK Comparison ==============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Cache BOS–EOS valid indices per prompt\n",
    "# ============================================================\n",
    "_mask_cache = {}\n",
    "\n",
    "\n",
    "def preprocess_metrics(metrics, lens_type=\"raw\"):\n",
    "    \"\"\"Trim logits/hidden/targets to BOS–EOS span, cached per prompt_id.\"\"\"\n",
    "    processed = []\n",
    "    for row in metrics:\n",
    "        pid = row.get(\"prompt_id\")\n",
    "        logits = row.get(f\"logits_{lens_type}\")\n",
    "        hidden = row.get(f\"hidden_{lens_type}\")\n",
    "        attn_mask = row.get(\"attention_mask\")\n",
    "        targets = row.get(\"target_ids\")\n",
    "\n",
    "        if logits is None or targets is None or attn_mask is None:\n",
    "            continue\n",
    "\n",
    "        # reuse cached BOS–EOS mask\n",
    "        if pid in _mask_cache:\n",
    "            valid_pos = _mask_cache[pid]\n",
    "        else:\n",
    "            if not isinstance(attn_mask, torch.Tensor):\n",
    "                attn_mask = torch.tensor(attn_mask)\n",
    "            if attn_mask.ndim == 4:\n",
    "                attn_mask = attn_mask[:, 0, 0, :]\n",
    "            elif attn_mask.ndim == 1:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "            mask_1d = attn_mask[0]\n",
    "            true_pos = mask_1d.nonzero(as_tuple=True)[0]\n",
    "            if true_pos.numel() < 2:\n",
    "                continue\n",
    "\n",
    "            bos_idx, eos_idx = int(true_pos[0]), int(true_pos[-1])\n",
    "            eval_mask = torch.zeros_like(mask_1d, dtype=torch.bool)\n",
    "            if eos_idx > bos_idx + 1:\n",
    "                eval_mask[bos_idx + 1:eos_idx] = True\n",
    "            valid_pos = eval_mask.nonzero(as_tuple=True)[0]\n",
    "            if valid_pos.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            _mask_cache[pid] = valid_pos\n",
    "\n",
    "        if logits.ndim == 2:\n",
    "            logits = logits.unsqueeze(0)\n",
    "        if targets.ndim == 1:\n",
    "            targets = targets.unsqueeze(0)\n",
    "        if hidden is not None and hidden.ndim == 2:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "\n",
    "        logits_trim = logits[:, valid_pos, :].contiguous()\n",
    "        targets_trim = targets[:, valid_pos].contiguous()\n",
    "        hidden_trim = hidden[:, valid_pos, :].contiguous() if hidden is not None else None\n",
    "\n",
    "        row_out = dict(row)\n",
    "        row_out[f\"logits_{lens_type}\"] = logits_trim\n",
    "        row_out[f\"hidden_{lens_type}\"] = hidden_trim\n",
    "        row_out[\"target_ids\"] = targets_trim\n",
    "        processed.append(row_out)\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute metrics and top-k similarities for A vs B\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def compute_topk(\n",
    "    metrics_A,\n",
    "    metrics_B,\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk=(1, 5, 10, 20),\n",
    "    device=\"cpu\",\n",
    "    eps = 1e-12,\n",
    "    output_dir=\"logs/new_summary\",\n",
    "    run_name=None,\n",
    "    batch_idx=None,\n",
    "    debug=False,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- safe clean helper ---\n",
    "    \"\"\"def clean(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().to(\"cpu\", copy=False).float()\n",
    "            x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return x.tolist()\n",
    "        return x\"\"\"\n",
    "    def clean(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().to(\"cpu\", copy=False).float()\n",
    "            x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return x.view(-1).tolist()\n",
    "        elif isinstance(x, (list, np.ndarray)):\n",
    "            return [float(v) if np.isfinite(v) else 0.0 for v in x]\n",
    "        else:\n",
    "            return float(x) if np.isfinite(x) else 0.0\n",
    "\n",
    "    # --- preprocess all normalization modes ---\n",
    "    proc_modes = {\n",
    "        m: (preprocess_metrics(metrics_A, m), preprocess_metrics(metrics_B, m))\n",
    "        for m in norm_modes\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[mask cache] built for {len(_mask_cache)} prompt_ids\")\n",
    "        for pid, mask in list(_mask_cache.items())[:10]: \n",
    "            print(f\"  prompt_id={pid:<5} → len={len(mask)}  positions={mask.tolist()}\")\n",
    "        lengths = [len(v) for v in _mask_cache.values()]\n",
    "        print(f\"  unique mask lengths: {sorted(set(lengths))}\")\n",
    "\n",
    "    rec_map = {}\n",
    "\n",
    "    # --- main computation ---\n",
    "    for mode in norm_modes:\n",
    "        A_trim, B_trim = proc_modes[mode]\n",
    "        if not A_trim or not B_trim:\n",
    "            if debug:\n",
    "                print(f\"[skip] no valid rows for mode={mode}\")\n",
    "            continue\n",
    "\n",
    "        for rA in A_trim:\n",
    "            pid, lid = rA[\"prompt_id\"], rA[\"layer_index\"]\n",
    "            rB = next((r for r in B_trim if r.get(\"prompt_id\") == pid and r.get(\"layer_index\") == lid), None)\n",
    "            if rB is None:\n",
    "                continue\n",
    "\n",
    "            key = (pid, lid)\n",
    "            record = rec_map.setdefault(\n",
    "                key,\n",
    "                dict(\n",
    "                    prompt_id=pid,\n",
    "                    batch_index=rA.get(\"batch_index\", batch_idx),\n",
    "                    layer_index=lid,\n",
    "                    layer_name=rA.get(\"layer_name\"),\n",
    "                    prompt_text=rA.get(\"prompt_text\"),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # --- tensor setup ---\n",
    "            logits_A, logits_B = rA[f\"logits_{mode}\"], rB[f\"logits_{mode}\"]\n",
    "            targets = rA[\"target_ids\"]\n",
    "            hidden_A, hidden_B = rA.get(f\"hidden_{mode}\"), rB.get(f\"hidden_{mode}\")\n",
    "\n",
    "            if logits_A.ndim == 2: logits_A = logits_A.unsqueeze(0)\n",
    "            if logits_B.ndim == 2: logits_B = logits_B.unsqueeze(0)\n",
    "            if targets.ndim == 1: targets = targets.unsqueeze(0)\n",
    "\n",
    "            logits_A, logits_B = logits_A.to(device).float(), logits_B.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            if hidden_A is not None: hidden_A = hidden_A.to(device).float()\n",
    "            if hidden_B is not None: hidden_B = hidden_B.to(device).float()\n",
    "\n",
    "            L = targets.size(1)\n",
    "            logits_A, logits_B = logits_A[:, :L, :], logits_B[:, :L, :]\n",
    "            if hidden_A is not None: hidden_A = hidden_A[:, :L, :]\n",
    "            if hidden_B is not None: hidden_B = hidden_B[:, :L, :]\n",
    "\n",
    "            vocab = max(logits_A.size(-1), logits_B.size(-1))\n",
    "            if logits_A.size(-1) < vocab:\n",
    "                logits_A = F.pad(logits_A, (0, vocab - logits_A.size(-1)))\n",
    "            if logits_B.size(-1) < vocab:\n",
    "                logits_B = F.pad(logits_B, (0, vocab - logits_B.size(-1)))\n",
    "\n",
    "            # --- probability space ---\n",
    "            logp_A = F.log_softmax(logits_A, dim=-1)\n",
    "            logp_B = F.log_softmax(logits_B, dim=-1)\n",
    "\n",
    "            # Compute probabilities cleanly and normalize to avoid drift\n",
    "            pA = torch.exp(logp_A)\n",
    "            pB = torch.exp(logp_B)\n",
    "\n",
    "            # Explicit renormalization (prevents underflow/rounding issues)\n",
    "            pA = pA / (pA.sum(-1, keepdim=True) + eps)\n",
    "            pB = pB / (pB.sum(-1, keepdim=True) + eps)\n",
    "\n",
    "            # unpack first dimension (batch=1)\n",
    "            logp_A, logp_B = logp_A[0], logp_B[0]\n",
    "            pA, pB, tgt = pA[0], pB[0], targets[0]\n",
    "\n",
    "            # --- basic metrics ---\n",
    "            kl_ab = torch.sum(pA * (logp_A - logp_B), dim=-1).clamp_min(0.0)\n",
    "            kl_ba = torch.sum(pB * (logp_B - logp_A), dim=-1).clamp_min(0.0)\n",
    "            js_div = 0.5 * (kl_ab + kl_ba)\n",
    "            js_dist = torch.sqrt(torch.clamp(js_div, min=0.0) + eps)\n",
    "\n",
    "            # move to cpu and clean\n",
    "            kl_ab = clean(kl_ab)\n",
    "            kl_ba = clean(kl_ba)\n",
    "            js_div = clean(js_div)\n",
    "            js_dist = clean(js_dist)\n",
    "\n",
    "            # TVD and entropy\n",
    "            tvd = clean(0.5 * torch.sum(torch.abs(pA - pB), dim=-1))\n",
    "            entropy_A = clean(-torch.sum(pA * logp_A, dim=-1))\n",
    "            entropy_B = clean(-torch.sum(pB * logp_B, dim=-1))\n",
    "\n",
    "\n",
    "            # === per-position L2 ===\n",
    "            if hidden_A is not None and hidden_B is not None:\n",
    "                cosine = clean(F.cosine_similarity(hidden_A[0], hidden_B[0], dim=-1))\n",
    "                l2_tensor = torch.sqrt(torch.sum((hidden_A[0] - hidden_B[0]) ** 2, dim=-1))\n",
    "                l2 = clean(l2_tensor)\n",
    "                #if debug:\n",
    "                    #print(f\"[debug] pid={pid} lid={lid} L2 shape={l2_tensor.shape} mean={l2_tensor.mean().item():.3g}\")\n",
    "            else:\n",
    "                cosine, l2 = [0.0] * L, [0.0] * L\n",
    "\n",
    "            # log-likelihood difference for ground-truth tokens\n",
    "            logp_A_gt = torch.gather(logp_A, -1, tgt.unsqueeze(-1)).squeeze(-1)\n",
    "            logp_B_gt = torch.gather(logp_B, -1, tgt.unsqueeze(-1)).squeeze(-1)\n",
    "            logp_diff = logp_A_gt - logp_B_gt\n",
    "\n",
    "            # probability assignments for the same ground-truth tokens\n",
    "            p_A_gt = torch.exp(logp_A_gt)\n",
    "            p_B_gt = torch.exp(logp_B_gt)\n",
    "            p_diff = p_A_gt - p_B_gt\n",
    "\n",
    "            # clean everything for storage\n",
    "            logp_A_gt = clean(logp_A_gt)\n",
    "            logp_B_gt = clean(logp_B_gt)\n",
    "            logp_diff = clean(logp_diff)\n",
    "            p_A_gt = clean(p_A_gt)\n",
    "            p_B_gt = clean(p_B_gt)\n",
    "            p_diff = clean(p_diff)\n",
    "\n",
    "            # === per-position cross-entropy and perplexity ===\n",
    "            ce_A_pos = F.cross_entropy(\n",
    "                logits_A.view(-1, vocab), targets.view(-1), reduction=\"none\"\n",
    "            ).view(targets.shape)\n",
    "            ce_B_pos = F.cross_entropy(\n",
    "                logits_B.view(-1, vocab), targets.view(-1), reduction=\"none\"\n",
    "            ).view(targets.shape)\n",
    "\n",
    "            # Compute per-position perplexity\n",
    "            ppl_A_pos = torch.exp(ce_A_pos)\n",
    "            ppl_B_pos = torch.exp(ce_B_pos)\n",
    "\n",
    "            # Convert everything safely to Python lists\n",
    "            ppl_A_pos = clean(ppl_A_pos)\n",
    "            ppl_B_pos = clean(ppl_B_pos)\n",
    "\n",
    "            # Compute per-position difference as list comprehension to avoid tensor ops\n",
    "            ppl_diff = [a - b for a, b in zip(ppl_A_pos, ppl_B_pos)]\n",
    "\n",
    "            record.update(\n",
    "                {\n",
    "                    f\"kl_ab_{mode}\": kl_ab,\n",
    "                    f\"kl_ba_{mode}\": kl_ba,\n",
    "                    f\"js_div_{mode}\": js_div,\n",
    "                    f\"js_dist_{mode}\": js_dist,\n",
    "                    f\"tvd_{mode}\": tvd,\n",
    "                    f\"entropy_A_{mode}\": entropy_A,\n",
    "                    f\"entropy_B_{mode}\": entropy_B,\n",
    "                    f\"cosine_sim_{mode}\": cosine,\n",
    "                    f\"l2_dist_{mode}\": l2,   \n",
    "                    f\"logp_diff_{mode}\": logp_diff,\n",
    "                    f\"logp_A_gt_{mode}\": logp_A_gt,\n",
    "                    f\"logp_B_gt_{mode}\": logp_B_gt,\n",
    "                    f\"logp_diff_{mode}\": logp_diff,\n",
    "                    f\"p_A_gt_{mode}\": p_A_gt,\n",
    "                    f\"p_B_gt_{mode}\": p_B_gt,\n",
    "                    f\"p_diff_{mode}\": p_diff,\n",
    "                    f\"ppl_A_{mode}\": ppl_A_pos,\n",
    "                    f\"ppl_B_{mode}\": ppl_B_pos,\n",
    "                    f\"ppl_diff_{mode}\": ppl_diff,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # === top-k metrics ===\n",
    "            fams = {key: {} for key in [\n",
    "                f\"acc_A_{mode}\", f\"acc_B_{mode}\",\n",
    "                f\"jaccard_{mode}\", f\"disagree_set_{mode}\", f\"agree_set_{mode}\",\n",
    "                f\"agree_correct_{mode}\", f\"disagree_correct_{mode}\",\n",
    "                f\"agree_wrong_{mode}\", f\"prob_overlap_{mode}\",\n",
    "                f\"top_pred_ids_A_{mode}\", f\"top_pred_vals_A_{mode}\",\n",
    "                f\"top_pred_ids_B_{mode}\", f\"top_pred_vals_B_{mode}\"\n",
    "            ]}\n",
    "\n",
    "            max_k = max(topk)\n",
    "            top_vals_A, top_idx_A = torch.topk(pA, max_k, -1)\n",
    "            top_vals_B, top_idx_B = torch.topk(pB, max_k, -1)\n",
    "\n",
    "            top_vals_A, top_vals_B = top_vals_A.cpu(), top_vals_B.cpu()\n",
    "            top_idx_A, top_idx_B = top_idx_A.cpu(), top_idx_B.cpu()\n",
    "            tgt_cpu = tgt.cpu()\n",
    "\n",
    "            for k in topk:\n",
    "                tkA, tkB = top_idx_A[:, :k], top_idx_B[:, :k]\n",
    "                tvA, tvB = top_vals_A[:, :k], top_vals_B[:, :k]\n",
    "\n",
    "                acc_A = (tkA == tgt_cpu.unsqueeze(1)).any(1).float()\n",
    "                acc_B = (tkB == tgt_cpu.unsqueeze(1)).any(1).float()\n",
    "\n",
    "                # --- set overlap ---\n",
    "                inter = torch.tensor(\n",
    "                    [len(set(tkA[i].tolist()) & set(tkB[i].tolist())) for i in range(L)],\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "                jaccard = inter / (2 * k - inter + eps)\n",
    "                disagree_set = 1.0 - jaccard\n",
    "                agree_set = (inter > 0).float()\n",
    "\n",
    "                # --- correctness relations ---\n",
    "                agree_correct = acc_A * acc_B\n",
    "                disagree_correct = ((acc_A + acc_B).round() == 1).float()  # XOR\n",
    "                agree_wrong = ((1 - acc_A) * (1 - acc_B)).float()\n",
    "\n",
    "                # --- probability mass overlap ---\n",
    "                pmA, pmB = tvA.sum(1), tvB.sum(1)\n",
    "                shared_mass = torch.zeros_like(pmA)\n",
    "                for i in range(L):\n",
    "                    shared = set(tkA[i].tolist()) & set(tkB[i].tolist())\n",
    "                    if shared:\n",
    "                        shared_mass[i] = 0.5 * (\n",
    "                            pA[i, list(shared)].sum().cpu() + pB[i, list(shared)].sum().cpu()\n",
    "                        )\n",
    "                prob_overlap = shared_mass / (0.5 * (pmA + pmB) + eps)\n",
    "\n",
    "                # --- save results ---\n",
    "                fams[f\"acc_A_{mode}\"][f\"@{k}\"] = acc_A.tolist()\n",
    "                fams[f\"acc_B_{mode}\"][f\"@{k}\"] = acc_B.tolist()\n",
    "                fams[f\"jaccard_{mode}\"][f\"@{k}\"] = jaccard.tolist()\n",
    "                fams[f\"disagree_set_{mode}\"][f\"@{k}\"] = disagree_set.tolist()\n",
    "                fams[f\"agree_set_{mode}\"][f\"@{k}\"] = agree_set.tolist()\n",
    "                fams[f\"agree_correct_{mode}\"][f\"@{k}\"] = agree_correct.tolist()\n",
    "                fams[f\"disagree_correct_{mode}\"][f\"@{k}\"] = disagree_correct.tolist()\n",
    "                fams[f\"agree_wrong_{mode}\"][f\"@{k}\"] = agree_wrong.tolist()\n",
    "                fams[f\"prob_overlap_{mode}\"][f\"@{k}\"] = prob_overlap.tolist()\n",
    "\n",
    "                # top predictions\n",
    "                if k == 1:\n",
    "                    fams[f\"top_pred_ids_A_{mode}\"][f\"@{k}\"] = [int(x) for x in tkA[:, 0].tolist()]\n",
    "                    fams[f\"top_pred_vals_A_{mode}\"][f\"@{k}\"] = [float(x) for x in tvA[:, 0].tolist()]\n",
    "                    fams[f\"top_pred_ids_B_{mode}\"][f\"@{k}\"] = [int(x) for x in tkB[:, 0].tolist()]\n",
    "                    fams[f\"top_pred_vals_B_{mode}\"][f\"@{k}\"] = [float(x) for x in tvB[:, 0].tolist()]\n",
    "                else:\n",
    "                    fams[f\"top_pred_ids_A_{mode}\"][f\"@{k}\"] = [[int(x) for x in arr] for arr in tkA.tolist()]\n",
    "                    fams[f\"top_pred_vals_A_{mode}\"][f\"@{k}\"] = [[float(x) for x in arr] for arr in tvA.tolist()]\n",
    "                    fams[f\"top_pred_ids_B_{mode}\"][f\"@{k}\"] = [[int(x) for x in arr] for arr in tkB.tolist()]\n",
    "                    fams[f\"top_pred_vals_B_{mode}\"][f\"@{k}\"] = [[float(x) for x in arr] for arr in tvB.tolist()]\n",
    "\n",
    "\n",
    "            record.update(fams)\n",
    "\n",
    "            # --- cleanup per iteration ---\n",
    "            del logits_A, logits_B, logp_A, logp_B, pA, pB, hidden_A, hidden_B, targets\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # --- finalize and save ---\n",
    "    df = pd.DataFrame(list(rec_map.values()), dtype=object)\n",
    "    out_path = os.path.join(output_dir, f\"{run_name or 'run'}_batch{int(batch_idx or 0):03d}.parquet\")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[saved] {len(df)} rows → {out_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f667ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, torch, psutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def mem_report(note=\"\"):\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"[mem] {note} used {mem.used/1e9:.1f} / {mem.total/1e9:.1f} GB\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_topk_streaming(\n",
    "    dir_A,\n",
    "    dir_B,\n",
    "    output_dir=\"saved_data/topk\",\n",
    "    norm_modes=(\"raw\",\"unit_rms\",\"norm_rms\"),\n",
    "    topk=(1,5,10,20),\n",
    "    device=None,\n",
    "    run_name=\"run\",\n",
    "    debug=True\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    files_A = sorted([f for f in os.listdir(dir_A) if f.endswith(\".pt\")])\n",
    "    files_B = sorted([f for f in os.listdir(dir_B) if f.endswith(\".pt\")])\n",
    "    assert len(files_A) == len(files_B), \"Mismatch in number of files\"\n",
    "\n",
    "    print(f\"[info] Found {len(files_A)} file pairs to process\")\n",
    "\n",
    "    for batch_idx, (fa, fb) in enumerate(tqdm(zip(files_A, files_B), total=len(files_A))):\n",
    "        path_A = os.path.join(dir_A, fa)\n",
    "        path_B = os.path.join(dir_B, fb)\n",
    "        print(f\"\\n[batch {batch_idx}] {fa} vs {fb}\")\n",
    "        mem_report(\"before loading\")\n",
    "\n",
    "        metrics_A = torch.load(path_A, map_location=\"cpu\")\n",
    "        metrics_B = torch.load(path_B, map_location=\"cpu\")\n",
    "\n",
    "        print(\"  [compute] running compute_topk ...\")\n",
    "        df = compute_topk(\n",
    "            metrics_A,\n",
    "            metrics_B,\n",
    "            norm_modes=norm_modes,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "            output_dir=output_dir,\n",
    "            run_name=run_name,\n",
    "            batch_idx=batch_idx,\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "        print(f\"  [saved] {run_name}_batch{batch_idx}.parquet\")\n",
    "\n",
    "        del df, metrics_A, metrics_B\n",
    "        _mask_cache.clear()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        mem_report(\"after cleanup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Found 50 file pairs to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch 0] m_orig_modes_batch000.pt vs m_8bit_modes_batch000.pt\n",
      "[mem] before loading used 6.2 / 66.6 GB\n",
      "  [compute] running compute_topk ...\n",
      "[mask cache] built for 10 prompt_ids\n",
      "  prompt_id=0     → len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=1     → len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=2     → len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=3     → len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=4     → len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  prompt_id=5     → len=10  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  prompt_id=6     → len=11  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "  prompt_id=7     → len=8  positions=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "  prompt_id=8     → len=15  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  prompt_id=9     → len=9  positions=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  unique mask lengths: [8, 9, 10, 11, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7291ee4b7070>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/am/AM/LogitDiffLens/logit-lens-env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "run_name = \"m_orig_m_8bit\"\n",
    "\n",
    "run_topk_streaming(\n",
    "    dir_A=\"saved_data/lens_data/m_orig\",\n",
    "    dir_B=\"saved_data/lens_data/m_8bit\",\n",
    "    output_dir=\"saved_data/topk/m_8bit\",\n",
    "    norm_modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk=(1, 5, 10),\n",
    "    #eos_token_id=128009,\n",
    "    #bos_token_id=128000,\n",
    "    run_name=run_name,\n",
    "    device=\"cpu\",\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fdf070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 83)\n",
      "['prompt_id', 'batch_index', 'layer_index', 'layer_name', 'prompt_text', 'kl_ab_raw', 'kl_ba_raw', 'js_div_raw', 'js_dist_raw', 'tvd_raw', 'entropy_A_raw', 'entropy_B_raw', 'cosine_sim_raw', 'l2_dist_raw', 'logp_diff_raw', 'ppl_A_raw', 'ppl_B_raw', 'ppl_diff_raw', 'acc_A_raw', 'acc_B_raw', 'jaccard_raw', 'disagree_set_raw', 'agree_set_raw', 'agree_correct_raw', 'disagree_correct_raw', 'agree_wrong_raw', 'prob_overlap_raw', 'top_pred_ids_A_raw', 'top_pred_vals_A_raw', 'top_pred_ids_B_raw', 'top_pred_vals_B_raw', 'kl_ab_unit_rms', 'kl_ba_unit_rms', 'js_div_unit_rms', 'js_dist_unit_rms', 'tvd_unit_rms', 'entropy_A_unit_rms', 'entropy_B_unit_rms', 'cosine_sim_unit_rms', 'l2_dist_unit_rms', 'logp_diff_unit_rms', 'ppl_A_unit_rms', 'ppl_B_unit_rms', 'ppl_diff_unit_rms', 'acc_A_unit_rms', 'acc_B_unit_rms', 'jaccard_unit_rms', 'disagree_set_unit_rms', 'agree_set_unit_rms', 'agree_correct_unit_rms', 'disagree_correct_unit_rms', 'agree_wrong_unit_rms', 'prob_overlap_unit_rms', 'top_pred_ids_A_unit_rms', 'top_pred_vals_A_unit_rms', 'top_pred_ids_B_unit_rms', 'top_pred_vals_B_unit_rms', 'kl_ab_norm_rms', 'kl_ba_norm_rms', 'js_div_norm_rms', 'js_dist_norm_rms', 'tvd_norm_rms', 'entropy_A_norm_rms', 'entropy_B_norm_rms', 'cosine_sim_norm_rms', 'l2_dist_norm_rms', 'logp_diff_norm_rms', 'ppl_A_norm_rms', 'ppl_B_norm_rms', 'ppl_diff_norm_rms', 'acc_A_norm_rms', 'acc_B_norm_rms', 'jaccard_norm_rms', 'disagree_set_norm_rms', 'agree_set_norm_rms', 'agree_correct_norm_rms', 'disagree_correct_norm_rms', 'agree_wrong_norm_rms', 'prob_overlap_norm_rms', 'top_pred_ids_A_norm_rms', 'top_pred_vals_A_norm_rms', 'top_pred_ids_B_norm_rms', 'top_pred_vals_B_norm_rms']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"saved_data/topk/4bit_test/m_orig_m_4bit_batch000.parquet\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc96c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2849.375    , -1397.8828125,  -421.671875 , -2650.46875  ,\n",
       "        2668.390625 , -2862.546875 ,  2088.28125  ,  1438.09375  ,\n",
       "        1142.90625  ,  -997.3515625])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ppl_diff_raw\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31ba581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt_id                   0\n",
       "batch_index                 0\n",
       "layer_index                 0\n",
       "layer_name                  0\n",
       "prompt_text                 0\n",
       "                           ..\n",
       "prob_overlap_norm_rms       0\n",
       "top_pred_ids_A_norm_rms     0\n",
       "top_pred_vals_A_norm_rms    0\n",
       "top_pred_ids_B_norm_rms     0\n",
       "top_pred_vals_B_norm_rms    0\n",
       "Length: 83, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efba6a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw {'@1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '@10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '@5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "unit_rms {'@1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '@10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '@5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "norm_rms {'@1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '@10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '@5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "for m in [\"raw\", \"unit_rms\", \"norm_rms\"]:\n",
    "    print(m, df.iloc[0][f\"acc_A_{m}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072ca179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>batch_index</th>\n",
       "      <th>layer_index</th>\n",
       "      <th>layer_name</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>kl_ab_raw</th>\n",
       "      <th>kl_ba_raw</th>\n",
       "      <th>js_div_raw</th>\n",
       "      <th>js_dist_raw</th>\n",
       "      <th>tvd_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>disagree_set_norm_rms</th>\n",
       "      <th>agree_set_norm_rms</th>\n",
       "      <th>agree_correct_norm_rms</th>\n",
       "      <th>disagree_correct_norm_rms</th>\n",
       "      <th>agree_wrong_norm_rms</th>\n",
       "      <th>prob_overlap_norm_rms</th>\n",
       "      <th>top_pred_ids_A_norm_rms</th>\n",
       "      <th>top_pred_vals_A_norm_rms</th>\n",
       "      <th>top_pred_ids_B_norm_rms</th>\n",
       "      <th>top_pred_vals_B_norm_rms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>when did richmond last play in a preliminary f...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.5604879877173516e-07, 0.0, 0...</td>\n",
       "      <td>[3.295296551186766e-07, 7.95488574567571e-07, ...</td>\n",
       "      <td>[1.647648275593383e-07, 3.977442872837855e-07,...</td>\n",
       "      <td>[0.00040591356810182333, 0.0006306705181486905...</td>\n",
       "      <td>[4.759094736073166e-07, 4.540966074273456e-07,...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.9999998807907104, 0.9999998807907104...</td>\n",
       "      <td>{'@1': [106240, 50974, 15631, 30986, 125312, 4...</td>\n",
       "      <td>{'@1': [8.04836872703163e-06, 8.04786759545095...</td>\n",
       "      <td>{'@1': [106240, 50974, 15631, 30986, 125312, 4...</td>\n",
       "      <td>{'@1': [8.048412382777315e-06, 8.0478621384827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>who sang what in the world's come over you</td>\n",
       "      <td>[0.0, 7.059189215397055e-07, 0.0, 4.4279076405...</td>\n",
       "      <td>[6.355795676427078e-07, 0.0, 1.700919511904430...</td>\n",
       "      <td>[3.177897838213539e-07, 3.5295946076985274e-07...</td>\n",
       "      <td>[0.0005637293797917664, 0.0005941047566011548,...</td>\n",
       "      <td>[5.015654096496291e-07, 5.726046765630599e-07,...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.9999998807907104, 0.9999998807907104...</td>\n",
       "      <td>{'@1': [16527, 15739, 775, 27253, 22832, 98940...</td>\n",
       "      <td>{'@1': [8.05455601948779e-06, 8.06495881988667...</td>\n",
       "      <td>{'@1': [16527, 15739, 775, 27253, 22832, 98940...</td>\n",
       "      <td>{'@1': [8.054635145526845e-06, 8.0649260780774...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>who produces the most wool in the world</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 5.433208798422129e-07, 4....</td>\n",
       "      <td>[6.355795676427078e-07, 1.2710876262644888e-07...</td>\n",
       "      <td>[3.177897838213539e-07, 6.355438131322444e-08,...</td>\n",
       "      <td>[0.0005637293797917664, 0.0002521019196137786,...</td>\n",
       "      <td>[5.015654096496291e-07, 6.046816451998893e-07,...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.9999998807907104, 0.9999998807907104...</td>\n",
       "      <td>{'@1': [16527, 88215, 22832, 1818, 88443, 2725...</td>\n",
       "      <td>{'@1': [8.05455601948779e-06, 8.08752793091116...</td>\n",
       "      <td>{'@1': [16527, 88215, 22832, 1818, 88443, 2725...</td>\n",
       "      <td>{'@1': [8.054635145526845e-06, 8.0874269769992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>where does alaska the last frontier take place</td>\n",
       "      <td>[3.2161079843717744e-07, 0.0, 1.22883203612644...</td>\n",
       "      <td>[0.0, 8.771570492172032e-07, 0.0, 0.0, 2.18893...</td>\n",
       "      <td>[1.6080539921858872e-07, 4.385785246086016e-07...</td>\n",
       "      <td>[0.00040100672049447894, 0.0006622533546760678...</td>\n",
       "      <td>[4.811638518731343e-07, 4.230280410411069e-07,...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.9999998807907104, 0.9999998807907104...</td>\n",
       "      <td>{'@1': [93655, 38008, 8376, 598, 22832, 125312...</td>\n",
       "      <td>{'@1': [8.043350135267247e-06, 8.0232348409481...</td>\n",
       "      <td>{'@1': [93655, 38008, 8376, 598, 22832, 125312...</td>\n",
       "      <td>{'@1': [8.043406523938756e-06, 8.0232284744852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>a day to remember all i want cameos</td>\n",
       "      <td>[4.754832048092794e-07, 0.0, 1.210236035831258...</td>\n",
       "      <td>[0.0, 3.4339532817284635e-07, 0.0, 3.753275450...</td>\n",
       "      <td>[2.377416024046397e-07, 1.7169766408642317e-07...</td>\n",
       "      <td>[0.0004875885497312993, 0.00041436534957028925...</td>\n",
       "      <td>[3.634875156421913e-07, 4.2826491153391544e-07...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.9999998807907104, 0.9999998807907104...</td>\n",
       "      <td>{'@1': [6472, 12791, 880, 65420, 58182, 21162,...</td>\n",
       "      <td>{'@1': [7.964791620906908e-06, 8.0265626820619...</td>\n",
       "      <td>{'@1': [6472, 12791, 880, 65420, 58182, 21162,...</td>\n",
       "      <td>{'@1': [7.964817086758558e-06, 8.0265299402526...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id  batch_index  layer_index    layer_name  \\\n",
       "0          0            0           -1  embed_tokens   \n",
       "1          1            0           -1  embed_tokens   \n",
       "2          2            0           -1  embed_tokens   \n",
       "3          3            0           -1  embed_tokens   \n",
       "4          4            0           -1  embed_tokens   \n",
       "\n",
       "                                         prompt_text  \\\n",
       "0  when did richmond last play in a preliminary f...   \n",
       "1         who sang what in the world's come over you   \n",
       "2            who produces the most wool in the world   \n",
       "3     where does alaska the last frontier take place   \n",
       "4                a day to remember all i want cameos   \n",
       "\n",
       "                                           kl_ab_raw  \\\n",
       "0  [0.0, 0.0, 0.0, 1.5604879877173516e-07, 0.0, 0...   \n",
       "1  [0.0, 7.059189215397055e-07, 0.0, 4.4279076405...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 5.433208798422129e-07, 4....   \n",
       "3  [3.2161079843717744e-07, 0.0, 1.22883203612644...   \n",
       "4  [4.754832048092794e-07, 0.0, 1.210236035831258...   \n",
       "\n",
       "                                           kl_ba_raw  \\\n",
       "0  [3.295296551186766e-07, 7.95488574567571e-07, ...   \n",
       "1  [6.355795676427078e-07, 0.0, 1.700919511904430...   \n",
       "2  [6.355795676427078e-07, 1.2710876262644888e-07...   \n",
       "3  [0.0, 8.771570492172032e-07, 0.0, 0.0, 2.18893...   \n",
       "4  [0.0, 3.4339532817284635e-07, 0.0, 3.753275450...   \n",
       "\n",
       "                                          js_div_raw  \\\n",
       "0  [1.647648275593383e-07, 3.977442872837855e-07,...   \n",
       "1  [3.177897838213539e-07, 3.5295946076985274e-07...   \n",
       "2  [3.177897838213539e-07, 6.355438131322444e-08,...   \n",
       "3  [1.6080539921858872e-07, 4.385785246086016e-07...   \n",
       "4  [2.377416024046397e-07, 1.7169766408642317e-07...   \n",
       "\n",
       "                                         js_dist_raw  \\\n",
       "0  [0.00040591356810182333, 0.0006306705181486905...   \n",
       "1  [0.0005637293797917664, 0.0005941047566011548,...   \n",
       "2  [0.0005637293797917664, 0.0002521019196137786,...   \n",
       "3  [0.00040100672049447894, 0.0006622533546760678...   \n",
       "4  [0.0004875885497312993, 0.00041436534957028925...   \n",
       "\n",
       "                                             tvd_raw  ...  \\\n",
       "0  [4.759094736073166e-07, 4.540966074273456e-07,...  ...   \n",
       "1  [5.015654096496291e-07, 5.726046765630599e-07,...  ...   \n",
       "2  [5.015654096496291e-07, 6.046816451998893e-07,...  ...   \n",
       "3  [4.811638518731343e-07, 4.230280410411069e-07,...  ...   \n",
       "4  [3.634875156421913e-07, 4.2826491153391544e-07...  ...   \n",
       "\n",
       "                               disagree_set_norm_rms  \\\n",
       "0  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "3  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "4  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                  agree_set_norm_rms  \\\n",
       "0  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "1  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "2  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "3  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "4  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "\n",
       "                              agree_correct_norm_rms  \\\n",
       "0  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "3  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "4  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                           disagree_correct_norm_rms  \\\n",
       "0  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "3  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "4  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                agree_wrong_norm_rms  \\\n",
       "0  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "1  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "2  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "3  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "4  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "\n",
       "                               prob_overlap_norm_rms  \\\n",
       "0  {'@1': [0.9999998807907104, 0.9999998807907104...   \n",
       "1  {'@1': [0.9999998807907104, 0.9999998807907104...   \n",
       "2  {'@1': [0.9999998807907104, 0.9999998807907104...   \n",
       "3  {'@1': [0.9999998807907104, 0.9999998807907104...   \n",
       "4  {'@1': [0.9999998807907104, 0.9999998807907104...   \n",
       "\n",
       "                             top_pred_ids_A_norm_rms  \\\n",
       "0  {'@1': [106240, 50974, 15631, 30986, 125312, 4...   \n",
       "1  {'@1': [16527, 15739, 775, 27253, 22832, 98940...   \n",
       "2  {'@1': [16527, 88215, 22832, 1818, 88443, 2725...   \n",
       "3  {'@1': [93655, 38008, 8376, 598, 22832, 125312...   \n",
       "4  {'@1': [6472, 12791, 880, 65420, 58182, 21162,...   \n",
       "\n",
       "                            top_pred_vals_A_norm_rms  \\\n",
       "0  {'@1': [8.04836872703163e-06, 8.04786759545095...   \n",
       "1  {'@1': [8.05455601948779e-06, 8.06495881988667...   \n",
       "2  {'@1': [8.05455601948779e-06, 8.08752793091116...   \n",
       "3  {'@1': [8.043350135267247e-06, 8.0232348409481...   \n",
       "4  {'@1': [7.964791620906908e-06, 8.0265626820619...   \n",
       "\n",
       "                             top_pred_ids_B_norm_rms  \\\n",
       "0  {'@1': [106240, 50974, 15631, 30986, 125312, 4...   \n",
       "1  {'@1': [16527, 15739, 775, 27253, 22832, 98940...   \n",
       "2  {'@1': [16527, 88215, 22832, 1818, 88443, 2725...   \n",
       "3  {'@1': [93655, 38008, 8376, 598, 22832, 125312...   \n",
       "4  {'@1': [6472, 12791, 880, 65420, 58182, 21162,...   \n",
       "\n",
       "                            top_pred_vals_B_norm_rms  \n",
       "0  {'@1': [8.048412382777315e-06, 8.0478621384827...  \n",
       "1  {'@1': [8.054635145526845e-06, 8.0649260780774...  \n",
       "2  {'@1': [8.054635145526845e-06, 8.0874269769992...  \n",
       "3  {'@1': [8.043406523938756e-06, 8.0232284744852...  \n",
       "4  {'@1': [7.964817086758558e-06, 8.0265299402526...  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93f7372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>batch_index</th>\n",
       "      <th>layer_index</th>\n",
       "      <th>layer_name</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>kl_ab_raw</th>\n",
       "      <th>kl_ba_raw</th>\n",
       "      <th>js_div_raw</th>\n",
       "      <th>js_dist_raw</th>\n",
       "      <th>tvd_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>disagree_set_norm_rms</th>\n",
       "      <th>agree_set_norm_rms</th>\n",
       "      <th>agree_correct_norm_rms</th>\n",
       "      <th>disagree_correct_norm_rms</th>\n",
       "      <th>agree_wrong_norm_rms</th>\n",
       "      <th>prob_overlap_norm_rms</th>\n",
       "      <th>top_pred_ids_A_norm_rms</th>\n",
       "      <th>top_pred_vals_A_norm_rms</th>\n",
       "      <th>top_pred_ids_B_norm_rms</th>\n",
       "      <th>top_pred_vals_B_norm_rms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>output</td>\n",
       "      <td>what does the red stripes mean on the american...</td>\n",
       "      <td>[0.00931896548718214, 0.02275301143527031, 0.0...</td>\n",
       "      <td>[0.00875139981508255, 0.020966731011867523, 0....</td>\n",
       "      <td>[0.009035183116793633, 0.021859871223568916, 0...</td>\n",
       "      <td>[0.09505358338356018, 0.14785084128379822, 0.2...</td>\n",
       "      <td>[0.04915468022227287, 0.07283315807580948, 0.1...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [374, 279, 3492, 5425, 389, 389, 264, 3...</td>\n",
       "      <td>{'@1': [0.4756481945514679, 0.2824018895626068...</td>\n",
       "      <td>{'@1': [374, 279, 61301, 5425, 389, 304, 279, ...</td>\n",
       "      <td>{'@1': [0.4565294086933136, 0.2929996550083160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>output</td>\n",
       "      <td>where did they film diary of a wimpy kid</td>\n",
       "      <td>[0.02899564430117607, 0.016912903636693954, 0....</td>\n",
       "      <td>[0.029145263135433197, 0.017903849482536316, 0...</td>\n",
       "      <td>[0.029070453718304634, 0.017408376559615135, 0...</td>\n",
       "      <td>[0.17050059139728546, 0.1319408118724823, 0.36...</td>\n",
       "      <td>[0.09413482248783112, 0.07486601918935776, 0.2...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [374, 499, 733, 279, 315, 264, 289, 318...</td>\n",
       "      <td>{'@1': [0.14577332139015198, 0.451576769351959...</td>\n",
       "      <td>{'@1': [374, 499, 636, 279, 315, 264, 289, 318...</td>\n",
       "      <td>{'@1': [0.15400931239128113, 0.506159126758575...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>output</td>\n",
       "      <td>where was beasts of the southern wild filmed</td>\n",
       "      <td>[0.02899564430117607, 0.014131640084087849, 0....</td>\n",
       "      <td>[0.029145263135433197, 0.01317090354859829, 0....</td>\n",
       "      <td>[0.029070453718304634, 0.013651272282004356, 0...</td>\n",
       "      <td>[0.17050059139728546, 0.11683866381645203, 0.1...</td>\n",
       "      <td>[0.09413482248783112, 0.03487376123666763, 0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [374, 279, 315, 279, 18561, 8545, 42508...</td>\n",
       "      <td>{'@1': [0.14577332139015198, 0.772456526756286...</td>\n",
       "      <td>{'@1': [374, 279, 315, 279, 18561, 8545, 42508...</td>\n",
       "      <td>{'@1': [0.15400931239128113, 0.780452430248260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>output</td>\n",
       "      <td>what part of the country are you likely to fin...</td>\n",
       "      <td>[0.00931893102824688, 0.002987947780638933, 0....</td>\n",
       "      <td>[0.008751489222049713, 0.0038406294770538807, ...</td>\n",
       "      <td>[0.009035210125148296, 0.003414288628846407, 0...</td>\n",
       "      <td>[0.09505372494459152, 0.05843191593885422, 0.2...</td>\n",
       "      <td>[0.04915442690253258, 0.011744649149477482, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "      <td>{'@1': [374, 315, 279, 8271, 374, 499, 505, 31...</td>\n",
       "      <td>{'@1': [0.4756476581096649, 0.9758495092391968...</td>\n",
       "      <td>{'@1': [374, 315, 279, 8271, 374, 499, 505, 31...</td>\n",
       "      <td>{'@1': [0.4565294086933136, 0.964128315448761,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>output</td>\n",
       "      <td>when did fosters home for imaginary friends start</td>\n",
       "      <td>[0.026482293382287025, 0.01734291948378086, 0....</td>\n",
       "      <td>[0.026023251935839653, 0.017519818618893623, 0...</td>\n",
       "      <td>[0.02625277265906334, 0.017431369051337242, 0....</td>\n",
       "      <td>[0.16202707588672638, 0.13202790915966034, 0.1...</td>\n",
       "      <td>[0.09061356633901596, 0.07100653648376465, 0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...</td>\n",
       "      <td>{'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...</td>\n",
       "      <td>{'@1': [499, 279, 388, 2162, 369, 51052, 4885,...</td>\n",
       "      <td>{'@1': [0.12614338099956512, 0.323676854372024...</td>\n",
       "      <td>{'@1': [499, 279, 388, 2162, 369, 51052, 4885,...</td>\n",
       "      <td>{'@1': [0.14103320240974426, 0.300641596317291...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id  batch_index  layer_index layer_name  \\\n",
       "335          5            0           32     output   \n",
       "336          6            0           32     output   \n",
       "337          7            0           32     output   \n",
       "338          8            0           32     output   \n",
       "339          9            0           32     output   \n",
       "\n",
       "                                           prompt_text  \\\n",
       "335  what does the red stripes mean on the american...   \n",
       "336           where did they film diary of a wimpy kid   \n",
       "337       where was beasts of the southern wild filmed   \n",
       "338  what part of the country are you likely to fin...   \n",
       "339  when did fosters home for imaginary friends start   \n",
       "\n",
       "                                             kl_ab_raw  \\\n",
       "335  [0.00931896548718214, 0.02275301143527031, 0.0...   \n",
       "336  [0.02899564430117607, 0.016912903636693954, 0....   \n",
       "337  [0.02899564430117607, 0.014131640084087849, 0....   \n",
       "338  [0.00931893102824688, 0.002987947780638933, 0....   \n",
       "339  [0.026482293382287025, 0.01734291948378086, 0....   \n",
       "\n",
       "                                             kl_ba_raw  \\\n",
       "335  [0.00875139981508255, 0.020966731011867523, 0....   \n",
       "336  [0.029145263135433197, 0.017903849482536316, 0...   \n",
       "337  [0.029145263135433197, 0.01317090354859829, 0....   \n",
       "338  [0.008751489222049713, 0.0038406294770538807, ...   \n",
       "339  [0.026023251935839653, 0.017519818618893623, 0...   \n",
       "\n",
       "                                            js_div_raw  \\\n",
       "335  [0.009035183116793633, 0.021859871223568916, 0...   \n",
       "336  [0.029070453718304634, 0.017408376559615135, 0...   \n",
       "337  [0.029070453718304634, 0.013651272282004356, 0...   \n",
       "338  [0.009035210125148296, 0.003414288628846407, 0...   \n",
       "339  [0.02625277265906334, 0.017431369051337242, 0....   \n",
       "\n",
       "                                           js_dist_raw  \\\n",
       "335  [0.09505358338356018, 0.14785084128379822, 0.2...   \n",
       "336  [0.17050059139728546, 0.1319408118724823, 0.36...   \n",
       "337  [0.17050059139728546, 0.11683866381645203, 0.1...   \n",
       "338  [0.09505372494459152, 0.05843191593885422, 0.2...   \n",
       "339  [0.16202707588672638, 0.13202790915966034, 0.1...   \n",
       "\n",
       "                                               tvd_raw  ...  \\\n",
       "335  [0.04915468022227287, 0.07283315807580948, 0.1...  ...   \n",
       "336  [0.09413482248783112, 0.07486601918935776, 0.2...  ...   \n",
       "337  [0.09413482248783112, 0.03487376123666763, 0.0...  ...   \n",
       "338  [0.04915442690253258, 0.011744649149477482, 0....  ...   \n",
       "339  [0.09061356633901596, 0.07100653648376465, 0.0...  ...   \n",
       "\n",
       "                                 disagree_set_norm_rms  \\\n",
       "335  {'@1': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0...   \n",
       "336  {'@1': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "337  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "338  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "339  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...   \n",
       "\n",
       "                                    agree_set_norm_rms  \\\n",
       "335  {'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0...   \n",
       "336  {'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "337  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "338  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "339  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...   \n",
       "\n",
       "                                agree_correct_norm_rms  \\\n",
       "335  {'@1': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...   \n",
       "336  {'@1': [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "337  {'@1': [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...   \n",
       "338  {'@1': [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0...   \n",
       "339  {'@1': [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...   \n",
       "\n",
       "                             disagree_correct_norm_rms  \\\n",
       "335  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0...   \n",
       "336  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "337  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "338  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "339  {'@1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                  agree_wrong_norm_rms  \\\n",
       "335  {'@1': [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0...   \n",
       "336  {'@1': [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "337  {'@1': [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...   \n",
       "338  {'@1': [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0...   \n",
       "339  {'@1': [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...   \n",
       "\n",
       "                                 prob_overlap_norm_rms  \\\n",
       "335  {'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0...   \n",
       "336  {'@1': [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "337  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "338  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...   \n",
       "339  {'@1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0...   \n",
       "\n",
       "                               top_pred_ids_A_norm_rms  \\\n",
       "335  {'@1': [374, 279, 3492, 5425, 389, 389, 264, 3...   \n",
       "336  {'@1': [374, 499, 733, 279, 315, 264, 289, 318...   \n",
       "337  {'@1': [374, 279, 315, 279, 18561, 8545, 42508...   \n",
       "338  {'@1': [374, 315, 279, 8271, 374, 499, 505, 31...   \n",
       "339  {'@1': [499, 279, 388, 2162, 369, 51052, 4885,...   \n",
       "\n",
       "                              top_pred_vals_A_norm_rms  \\\n",
       "335  {'@1': [0.4756481945514679, 0.2824018895626068...   \n",
       "336  {'@1': [0.14577332139015198, 0.451576769351959...   \n",
       "337  {'@1': [0.14577332139015198, 0.772456526756286...   \n",
       "338  {'@1': [0.4756476581096649, 0.9758495092391968...   \n",
       "339  {'@1': [0.12614338099956512, 0.323676854372024...   \n",
       "\n",
       "                               top_pred_ids_B_norm_rms  \\\n",
       "335  {'@1': [374, 279, 61301, 5425, 389, 304, 279, ...   \n",
       "336  {'@1': [374, 499, 636, 279, 315, 264, 289, 318...   \n",
       "337  {'@1': [374, 279, 315, 279, 18561, 8545, 42508...   \n",
       "338  {'@1': [374, 315, 279, 8271, 374, 499, 505, 31...   \n",
       "339  {'@1': [499, 279, 388, 2162, 369, 51052, 4885,...   \n",
       "\n",
       "                              top_pred_vals_B_norm_rms  \n",
       "335  {'@1': [0.4565294086933136, 0.2929996550083160...  \n",
       "336  {'@1': [0.15400931239128113, 0.506159126758575...  \n",
       "337  {'@1': [0.15400931239128113, 0.780452430248260...  \n",
       "338  {'@1': [0.4565294086933136, 0.964128315448761,...  \n",
       "339  {'@1': [0.14103320240974426, 0.300641596317291...  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cosine_sim_norm_rms\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ec611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cosine_sim_raw\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"l2_dist_raw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = [\"kl_ab_raw\", \"js_div_raw\", \"disagree_correct_raw\"]\n",
    "def arr_len(x):\n",
    "    if isinstance(x, dict) and \"@1\" in x: \n",
    "        return len(x[\"@1\"])\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return len(x)\n",
    "    return np.nan\n",
    "\n",
    "for c in cols:\n",
    "    df[f\"len_{c}\"] = df[c].apply(arr_len)\n",
    "\n",
    "print(df[[\"prompt_id\",\"layer_index\",\"len_kl_ab_raw\",\"len_js_div_raw\",\"len_disagree_correct_raw\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d06d96",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Plot TopK Summaries ==========================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge] merged 50 parquet files → 17000 rows\n",
      "[diag] unique batch_index=50 | unique prompt_id=10 | unique (prompt,batch)=500\n",
      "[ok] prompt–batch pairs look good\n",
      "[ok] merged all models → 816000 rows total\n",
      "mode              norm_rms    raw  unit_rms\n",
      "metric                                     \n",
      "acc_A                51000  51000     51000\n",
      "acc_B                51000  51000     51000\n",
      "cosine_sim           17000  17000     17000\n",
      "disagree_correct     51000  51000     51000\n",
      "jaccard              51000  51000     51000\n",
      "l2_dist              17000  17000     17000\n",
      "ppl_diff             17000  17000     17000\n",
      "tvd                  17000  17000     17000\n",
      "[saved] saved_data/figures_topk_modes_fixed_full/overview_m_4bit.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "BASE = Path(\"saved_data\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "OUT_DIR = BASE / \"figures_topk_modes_fixed_full\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPK_METRICS = [\"acc_A\", \"acc_B\", \"jaccard\", \"disagree_correct\"]\n",
    "CONT_METRICS = [\"cosine_sim\", \"l2_dist\", \"tvd\", \"ppl_diff\"]\n",
    "MODES = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "TOPK = [1, 5, 10]\n",
    "\n",
    "\n",
    "def merge_parquet_files(input_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(input_dir).glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {input_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        d = pd.read_parquet(f)\n",
    "        d[\"batch_index\"] = i\n",
    "        dfs.append(d)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merge] merged {len(files)} parquet files → {len(df)} rows\")\n",
    "\n",
    "    n_batches = df[\"batch_index\"].nunique()\n",
    "    n_prompts = df[\"prompt_id\"].nunique()\n",
    "    n_pairs = df.groupby([\"prompt_id\", \"batch_index\"]).ngroups\n",
    "    print(f\"[diag] unique batch_index={n_batches} | unique prompt_id={n_prompts} | unique (prompt,batch)={n_pairs}\")\n",
    "\n",
    "    if n_pairs < len(files) * 10:\n",
    "        print(\"[warn] fewer prompt–batch pairs than expected — possible ID overlap?\")\n",
    "    else:\n",
    "        print(\"[ok] prompt–batch pairs look good\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_topk(df, metric, mode, k):\n",
    "    base = f\"{metric}_{mode}\"\n",
    "    if base not in df.columns:\n",
    "        return np.full(len(df), np.nan)\n",
    "    return df[base].apply(\n",
    "        lambda d: np.mean(d.get(f\"@{k}\", [])) if isinstance(d, dict) and f\"@{k}\" in d else np.nan\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_flat(df, metric, mode):\n",
    "    col = f\"{metric}_{mode}\"\n",
    "    if col not in df.columns:\n",
    "        return np.full(len(df), np.nan)\n",
    "    return df[col].apply(\n",
    "        lambda v: np.mean(v) if isinstance(v, (list, np.ndarray)) and len(v) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    topk_dir = BASE / \"topk\" / model\n",
    "    df = merge_parquet_files(topk_dir)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for metric in TOPK_METRICS:\n",
    "        for mode in MODES:\n",
    "            for k in TOPK:\n",
    "                vals = extract_topk(df, metric, mode, k)\n",
    "                dsub = pd.DataFrame({\n",
    "                    \"model\": model,\n",
    "                    \"metric\": metric,\n",
    "                    \"mode\": mode,\n",
    "                    \"topk\": k,\n",
    "                    \"layer_index\": df.get(\"layer_index\", pd.Series(np.zeros(len(df)))),\n",
    "                    \"value\": vals\n",
    "                })\n",
    "                all_rows.append(dsub)\n",
    "\n",
    "    for metric in CONT_METRICS:\n",
    "        for mode in MODES:\n",
    "            vals = extract_flat(df, metric, mode)\n",
    "            dsub = pd.DataFrame({\n",
    "                \"model\": model,\n",
    "                \"metric\": metric,\n",
    "                \"mode\": mode,\n",
    "                \"topk\": 1,  \n",
    "                \"layer_index\": df.get(\"layer_index\", pd.Series(np.zeros(len(df)))),\n",
    "                \"value\": vals\n",
    "            })\n",
    "            all_rows.append(dsub)\n",
    "\n",
    "    dfs.append(pd.concat(all_rows, ignore_index=True))\n",
    "\n",
    "df_long = pd.concat(dfs, ignore_index=True).dropna(subset=[\"value\"])\n",
    "print(f\"[ok] merged all models → {len(df_long)} rows total\")\n",
    "print(df_long.groupby([\"metric\", \"mode\"])[\"value\"].count().unstack(fill_value=0))\n",
    "\n",
    "\n",
    "for model in MODELS:\n",
    "    dsub = df_long[df_long[\"model\"] == model]\n",
    "    if dsub.empty:\n",
    "        print(f\"[skip] no data for {model}\")\n",
    "        continue\n",
    "\n",
    "    metrics_unique = dsub[\"metric\"].unique().tolist()\n",
    "    nrows = int(np.ceil(len(metrics_unique) / 2))\n",
    "    fig, axes = plt.subplots(nrows, 2, figsize=(14, 4 * nrows))\n",
    "    fig.suptitle(f\"Top-K + Continuous Metrics — {model}\", fontsize=16, weight=\"bold\")\n",
    "\n",
    "    for ax, metric in zip(axes.flatten(), metrics_unique):\n",
    "        d = dsub[dsub[\"metric\"] == metric]\n",
    "        sns.lineplot(\n",
    "            data=d,\n",
    "            x=\"layer_index\",\n",
    "            y=\"value\",\n",
    "            hue=\"mode\",\n",
    "            style=\"topk\" if metric in TOPK_METRICS else None,\n",
    "            markers=True,\n",
    "            err_style=\"band\",\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(metric.upper())\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Mean value\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    out_path = OUT_DIR / f\"overview_{model}.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[saved] {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b15b55",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# TopK Correlations ============================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b62baa",
   "metadata": {},
   "source": [
    "### ==============================================\n",
    "### Correlation with Pooling =====================\n",
    "### =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea4a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge] merged 50 parquet files → 17000 rows\n",
      "[diag] unique batch_index=50 | unique prompt_id=10 | unique (prompt,batch)=500\n",
      "[ok] prompt–batch pairs look good\n",
      "[expand] added 36 flattened top-k columns\n",
      "[ok] normalized disagree_correct_raw_@1 → len=15\n",
      "[ok] normalized disagree_correct_unit_rms_@1 → len=15\n",
      "[ok] normalized disagree_correct_norm_rms_@1 → len=15\n",
      "[ok] normalized logp_diff_raw → len=15\n",
      "[ok] normalized logp_diff_unit_rms → len=15\n",
      "[ok] normalized logp_diff_norm_rms → len=15\n",
      "[done] Anchors preprocessed (binary + continuous, unified padding)]\n",
      "[anchor_map] built 500 anchors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "disagree_correct_raw: 100%|██████████| 34/34 [00:37<00:00,  1.11s/it]\n",
      "disagree_correct_unit_rms: 100%|██████████| 34/34 [00:37<00:00,  1.11s/it]\n",
      "disagree_correct_norm_rms: 100%|██████████| 34/34 [00:38<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] disagree_correct → 1929 pooled correlations\n",
      "[nan check] 102 NaN correlations out of 1929 total\n",
      "[anchor_map] built 500 anchors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp_diff_raw:   0%|          | 0/34 [00:00<?, ?it/s]/media/am/AM/LogitDiffLens/logit-lens-env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "logp_diff_raw: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s]\n",
      "logp_diff_unit_rms:   0%|          | 0/34 [00:00<?, ?it/s]/media/am/AM/LogitDiffLens/logit-lens-env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "logp_diff_unit_rms: 100%|██████████| 34/34 [00:32<00:00,  1.05it/s]\n",
      "logp_diff_norm_rms:   0%|          | 0/34 [00:00<?, ?it/s]/media/am/AM/LogitDiffLens/logit-lens-env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "logp_diff_norm_rms: 100%|██████████| 34/34 [00:31<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] logp_diff → 1929 pooled correlations\n",
      "[nan check] 102 NaN correlations out of 1929 total\n",
      "[saved pooled correlations] saved_data/summary/m_quant/lw_m_quant_corr_pooled.csv\n",
      "[saved summary] saved_data/summary/m_quant/lw_m_quant_corr_pooled_summary_both.csv\n",
      "[diag] rows=3858  groups=3858\n",
      "[NaN summary rows: 204]\n",
      "[saved pooled summary] saved_data/summary/m_quant/lw_m_quant_corr_pooled_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, pointbiserialr, chi2_contingency\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Merge parquet files\n",
    "# ============================================================\n",
    "def merge_parquet_files(input_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(input_dir).glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {input_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        d = pd.read_parquet(f)\n",
    "        d[\"batch_index\"] = i\n",
    "        dfs.append(d)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merge] merged {len(files)} parquet files → {len(df)} rows\")\n",
    "\n",
    "    n_batches = df[\"batch_index\"].nunique()\n",
    "    n_prompts = df[\"prompt_id\"].nunique()\n",
    "    n_pairs = df.groupby([\"prompt_id\", \"batch_index\"]).ngroups\n",
    "    print(f\"[diag] unique batch_index={n_batches} | unique prompt_id={n_prompts} | unique (prompt,batch)={n_pairs}\")\n",
    "\n",
    "    if n_pairs < len(files) * 10:\n",
    "        print(\"[warn] fewer prompt–batch pairs than expected — possible ID overlap?\")\n",
    "    else:\n",
    "        print(\"[ok] prompt–batch pairs look good\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Expand nested top-k metrics (safe names)\n",
    "# ============================================================\n",
    "def expand_topk_metrics(df, metrics, modes=(\"raw\",\"unit_rms\",\"norm_rms\"), topk_levels=(1,5,10)):\n",
    "    new_cols=[]\n",
    "    for metric in metrics:\n",
    "        for mode in modes:\n",
    "            base=f\"{metric}_{mode}\"\n",
    "            if base not in df.columns:\n",
    "                continue\n",
    "            for k in topk_levels:\n",
    "                new=f\"{base}_@{k}\"\n",
    "                df[new]=df[base].apply(\n",
    "                    lambda d: np.array(d.get(f\"@{k}\",[]),float)\n",
    "                    if isinstance(d,dict) and f\"@{k}\" in d else np.array([])\n",
    "                )\n",
    "                new_cols.append(new)\n",
    "    print(f\"[expand] added {len(new_cols)} flattened top-k columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper\n",
    "# ============================================================\n",
    "\"\"\"def safe_flatten(x):\n",
    "    if isinstance(x,(list,np.ndarray)):\n",
    "        return np.array(x,float).flatten()\n",
    "    return np.array([],float)\"\"\"\n",
    "def safe_flatten(v):\n",
    "    \"\"\"Ensure flattening of nested lists/arrays into 1D np.array.\"\"\"\n",
    "    if isinstance(v, (list, np.ndarray)):\n",
    "        return np.asarray(v, dtype=float).flatten()\n",
    "    try:\n",
    "        return np.array([float(v)], dtype=float)\n",
    "    except Exception:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# Extract anchors \n",
    "# ============================================================\n",
    "def preprocess_anchors(df, modes=(\"raw\", \"unit_rms\", \"norm_rms\")):\n",
    "    # === BINARY ANCHOR ===\n",
    "    for mode in modes:\n",
    "        col = f\"disagree_correct_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        # find max sequence length (@1-level)\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda d: len(d.get(\"@1\", [])) if isinstance(d, dict) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(d):\n",
    "            if isinstance(d, dict) and \"@1\" in d:\n",
    "                arr = np.asarray(d[\"@1\"], dtype=float)\n",
    "            else:\n",
    "                arr = np.full(max_len, np.nan, dtype=float)\n",
    "            # pad to consistent length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[f\"{col}_@1\"] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col}_@1 → len={max_len}\")\n",
    "\n",
    "    # === CONTINUOUS ANCHOR ===\n",
    "    for mode in modes:\n",
    "        col = f\"logp_diff_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        # find max length across all prompts\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda v: len(v) if isinstance(v, (list, np.ndarray)) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(v):\n",
    "            if isinstance(v, (list, np.ndarray)):\n",
    "                arr = np.asarray(v, dtype=float)\n",
    "            else:\n",
    "                try:\n",
    "                    arr = np.array([float(v)], dtype=float)\n",
    "                except Exception:\n",
    "                    arr = np.array([np.nan], dtype=float)\n",
    "            # pad to uniform length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[col] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col} → len={max_len}\")\n",
    "\n",
    "    print(\"[done] Anchors preprocessed (binary + continuous, unified padding)]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation \n",
    "# ============================================================\n",
    "def _is_binary(arr, tol=1e-6):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if len(arr) == 0:\n",
    "        return False\n",
    "    u = np.unique(np.round(arr, 6))\n",
    "    return np.all((np.abs(u - 0) < tol) | (np.abs(u - 1) < tol))\n",
    "\n",
    "\n",
    "def _phi_coefficient(x, y):\n",
    "    \"\"\"Phi coefficient for two binary arrays (0/1).\"\"\"\n",
    "    x, y = np.asarray(x).astype(int), np.asarray(y).astype(int)\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        table = pd.crosstab(x, y)\n",
    "        if table.shape != (2, 2):\n",
    "            return np.nan\n",
    "        chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
    "        sign = np.sign((table.loc[1,1]*table.loc[0,0]) - (table.loc[1,0]*table.loc[0,1]))\n",
    "        return float(sign * np.sqrt(chi2 / len(x)))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- Deterministic metric-type mapping ---\n",
    "\"\"\"METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"binary\",\n",
    "    \"agree_correct\": \"binary\",\n",
    "    \"agree_wrong\": \"binary\",\n",
    "    \"agree_set\": \"binary\",\n",
    "    \"disagree_set\": \"binary\",\n",
    "\n",
    "    \"jaccard_@1\": \"binary\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\"\"\"\n",
    "METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"continuous\",\n",
    "    \"agree_correct\": \"continuous\",\n",
    "    \"agree_wrong\": \"continuous\",\n",
    "    \"agree_set\": \"continuous\",\n",
    "    \"disagree_set\": \"continuous\",\n",
    "\n",
    "    \"jaccard_@1\": \"continuous\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\n",
    "\n",
    "def _choose_corr_func_fixed(anchor, metric):\n",
    "    \"\"\"Deterministisk valg af korrelationstype ud fra kendt datanatur.\"\"\"\n",
    "    # Anchors: vi kender dem\n",
    "    anchor_t = \"binary\" if \"disagree_correct\" in anchor else \"continuous\"\n",
    "    metric_t = METRIC_TYPES.get(metric, \"continuous\")\n",
    "\n",
    "    # Klassiske statistiske regler\n",
    "    if anchor_t == \"binary\" and metric_t == \"binary\":\n",
    "        return \"phi\", _phi_coefficient\n",
    "    elif anchor_t == \"binary\" or metric_t == \"binary\":\n",
    "        return \"pointbiserial\", pointbiserialr\n",
    "    else:\n",
    "        return \"spearman\", spearmanr\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation (pooled version)\n",
    "# ============================================================\n",
    "def correlate_layers_by_anchor(\n",
    "    df,\n",
    "    anchor,\n",
    "    metrics,\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk_levels=(1, 5, 10),\n",
    "    n_boot=10,\n",
    "    n_perm=10,\n",
    "    seed=42,\n",
    "    output_dir=None,\n",
    "    model=\"m_8bit\",\n",
    "    min_valid=3,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    results = []\n",
    "    df_out = df[df[\"layer_name\"].str.lower() == \"output\"]\n",
    "\n",
    "    # --- Build anchor map ---\n",
    "    anchor_map = {}\n",
    "    for _, row in df_out.iterrows():\n",
    "        key = (int(row[\"prompt_id\"]), int(row.get(\"batch_index\", 0)))\n",
    "        anchor_vecs = {}\n",
    "        for mode in modes:\n",
    "            col1 = f\"{anchor}_{mode}_@1\"\n",
    "            col2 = f\"{anchor}_{mode}\"\n",
    "            col = col1 if col1 in df.columns else col2\n",
    "            val = row.get(col)\n",
    "            if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
    "                anchor_vecs[mode] = np.array(val, float)\n",
    "        if anchor_vecs:\n",
    "            anchor_map[key] = anchor_vecs\n",
    "    print(f\"[anchor_map] built {len(anchor_map)} anchors\")\n",
    "\n",
    "    # --- Force raw for these metrics ---\n",
    "    SHARED_METRICS = {\"cosine_sim\", \"l2_dist\"}\n",
    "\n",
    "    # ============================================================\n",
    "    # MAIN LOOP\n",
    "    # ============================================================\n",
    "    for mode in modes:\n",
    "        for (lname, lidx), layer_df in tqdm(df.groupby([\"layer_name\", \"layer_index\"]), desc=f\"{anchor}_{mode}\"):\n",
    "            for metric in metrics:\n",
    "                # Only have top-k variants for accuracy/Jaccard\n",
    "                is_topk_only = metric in {\"acc_A\", \"acc_B\", \"jaccard\"}\n",
    "                suffixes = [f\"_@{k}\" for k in topk_levels] if is_topk_only else [\"\"] + [f\"_@{k}\" for k in topk_levels]\n",
    "\n",
    "                for suffix in suffixes:\n",
    "                    src_mode = \"raw\" if metric in SHARED_METRICS else mode\n",
    "                    mcol = f\"{metric}_{src_mode}{suffix}\"\n",
    "                    if mcol not in layer_df.columns:\n",
    "                        continue\n",
    "\n",
    "                    anchor_vals, metric_vals = [], []\n",
    "                    for (pid, bid), group in layer_df.groupby([\"prompt_id\", \"batch_index\"]):\n",
    "                        key = (int(pid), int(bid))\n",
    "                        if key not in anchor_map or mode not in anchor_map[key]:\n",
    "                            continue\n",
    "                        a = anchor_map[key][mode]\n",
    "                        m = np.asarray(group[mcol].iloc[0], dtype=float).flatten()\n",
    "                        n = min(len(a), len(m))\n",
    "                        if n < min_valid:\n",
    "                            continue\n",
    "                        a, m = a[:n], m[:n]\n",
    "                        mask = np.isfinite(a) & np.isfinite(m)\n",
    "                        if mask.sum() < min_valid:\n",
    "                            continue\n",
    "                        anchor_vals.append(a[mask])\n",
    "                        metric_vals.append(m[mask])\n",
    "\n",
    "                    # --- NaN ---\n",
    "                    if not anchor_vals:\n",
    "                        results.append({\n",
    "                            \"mode\": mode,\n",
    "                            \"anchor\": anchor,\n",
    "                            \"metric\": f\"{metric}{suffix}\",\n",
    "                            \"corr_type\": \"undefined\",\n",
    "                            \"layer_name\": lname,\n",
    "                            \"layer_index\": lidx,\n",
    "                            \"rho\": np.nan,\n",
    "                            \"rho_boot_median\": np.nan,\n",
    "                            \"ci_low\": np.nan,\n",
    "                            \"ci_high\": np.nan,\n",
    "                            \"p_val\": np.nan,\n",
    "                            \"p_perm\": np.nan,\n",
    "                            \"n\": 0,\n",
    "                            \"pooling\": \"pooled\"\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    A = np.concatenate(anchor_vals)\n",
    "                    M = np.concatenate(metric_vals)\n",
    "                    mask = np.isfinite(A) & np.isfinite(M)\n",
    "                    n_used = int(mask.sum())\n",
    "                    if n_used < min_valid:\n",
    "                        continue\n",
    "                    A, M = A[mask], M[mask]\n",
    "\n",
    "                    if np.std(A) == 0 or np.std(M) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # --- correlation type (fixed deterministic rule) ---\n",
    "                    cname, func = _choose_corr_func_fixed(anchor, f\"{metric}{suffix}\")\n",
    "\n",
    "                    # fallback hvis alt ender binært i praksis\n",
    "                    \"\"\"if cname != \"phi\" and _is_binary(A) and _is_binary(M):\n",
    "                        cname, func = \"phi\", _phi_coefficient\"\"\"\n",
    "\n",
    "                    # --- rho ---\n",
    "                    try:\n",
    "                        if cname == \"phi\":\n",
    "                            rho = func(A, M)\n",
    "                            pval = np.nan\n",
    "                        else:\n",
    "                            rho, pval = func(A, M)\n",
    "                    except Exception:\n",
    "                        rho, pval = np.nan, np.nan\n",
    "                    if not np.isfinite(rho):\n",
    "                        continue\n",
    "\n",
    "                    # --- bootstrap CI ---\n",
    "                    boot_rhos = []\n",
    "                    for _ in range(n_boot):\n",
    "                        idx = np.random.choice(n_used, n_used, replace=True)\n",
    "                        try:\n",
    "                            if cname == \"phi\":\n",
    "                                rho_b = func(A[idx], M[idx])\n",
    "                            else:\n",
    "                                rho_b, _ = func(A[idx], M[idx])\n",
    "                        except Exception:\n",
    "                            rho_b = np.nan\n",
    "                        if np.isfinite(rho_b):\n",
    "                            boot_rhos.append(rho_b)\n",
    "                    if len(boot_rhos) > 20:\n",
    "                        ci_low, ci_high = np.percentile(boot_rhos, [2.5, 97.5])\n",
    "                        rho_boot = np.median(boot_rhos)\n",
    "                    else:\n",
    "                        ci_low = ci_high = rho_boot = np.nan\n",
    "\n",
    "                    # --- permutation test ---\n",
    "                    perm_rhos = []\n",
    "                    for _ in range(n_perm):\n",
    "                        try:\n",
    "                            if cname == \"phi\":\n",
    "                                rho_p = func(A, np.random.permutation(M))\n",
    "                            else:\n",
    "                                rho_p, _ = func(A, np.random.permutation(M))\n",
    "                        except Exception:\n",
    "                            rho_p = np.nan\n",
    "                        if np.isfinite(rho_p):\n",
    "                            perm_rhos.append(rho_p)\n",
    "                    if len(perm_rhos) > 20:\n",
    "                        perm_rhos = np.array(perm_rhos)\n",
    "                        p_perm = (np.sum(np.abs(perm_rhos) >= abs(rho)) + 1) / (len(perm_rhos) + 1)\n",
    "                    else:\n",
    "                        p_perm = np.nan\n",
    "\n",
    "                    results.append({\n",
    "                        \"mode\": mode,\n",
    "                        \"anchor\": anchor,\n",
    "                        \"metric\": f\"{metric}{suffix}\",\n",
    "                        \"corr_type\": cname,\n",
    "                        \"layer_name\": lname,\n",
    "                        \"layer_index\": lidx,\n",
    "                        \"rho\": rho,\n",
    "                        \"rho_boot_median\": rho_boot,\n",
    "                        \"ci_low\": ci_low,\n",
    "                        \"ci_high\": ci_high,\n",
    "                        \"p_val\": pval,\n",
    "                        \"p_perm\": p_perm,\n",
    "                        \"n\": n_used,\n",
    "                        \"pooling\": \"pooled\"\n",
    "                    })\n",
    "\n",
    "    # ============================================================\n",
    "    # Wrap up\n",
    "    # ============================================================\n",
    "    df_corr = pd.DataFrame(results)\n",
    "    print(f\"[ok] {anchor} → {len(df_corr)} pooled correlations\")\n",
    "    if not df_corr.empty:\n",
    "        print(f\"[nan check] {df_corr['rho'].isna().sum()} NaN correlations out of {len(df_corr)} total\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = Path(output_dir) / f\"lw_{model}_{anchor}_corr_pooled.csv\"\n",
    "        df_corr.to_csv(out_path, index=False)\n",
    "        print(f\"[saved correlations] {out_path}\")\n",
    "\n",
    "    return df_corr\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Summarize correlations\n",
    "# ============================================================\n",
    "def summarize_correlations(df_corr, output_dir, model, ci_mode=\"both\"):\n",
    "    df = df_corr.copy()\n",
    "\n",
    "    if \"n\" not in df.columns:\n",
    "        df[\"n\"] = 1\n",
    "\n",
    "    # z (only where rho is finite)\n",
    "    df[\"z\"] = np.nan\n",
    "    mask_finite = np.isfinite(df[\"rho\"])\n",
    "    df.loc[mask_finite, \"z\"] = np.arctanh(np.clip(df.loc[mask_finite, \"rho\"], -0.999999, 0.999999))\n",
    "\n",
    "    drop_cols = [c for c in [\"prompt_id\", \"batch_index\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    group_cols = [\"mode\", \"anchor\", \"metric\", \"corr_type\", \"layer_name\", \"layer_index\"]\n",
    "\n",
    "    def _weighted_stats(g):\n",
    "        out = {}\n",
    "\n",
    "        if not np.any(np.isfinite(g[\"rho\"])) or g[\"n\"].sum() == 0:\n",
    "            # keep but mark as NaN\n",
    "            for key in [\n",
    "                \"rho_mean\", \"rho_low\", \"rho_high\",\n",
    "                \"rho_boot_mean\", \"ci_low_emp\", \"ci_high_emp\", \"n_total\"\n",
    "            ]:\n",
    "                out[key] = np.nan\n",
    "            out[\"n_total\"] = 0\n",
    "        else:\n",
    "            # Fisher-Z mean + CI\n",
    "            z = g[\"z\"].dropna()\n",
    "            w = g.loc[z.index, \"n\"]\n",
    "            z_mean = np.average(z, weights=w)\n",
    "            z_std = np.sqrt(np.average((z - z_mean)**2, weights=w))\n",
    "            out[\"rho_mean\"] = np.tanh(z_mean)\n",
    "            out[\"rho_low\"] = np.tanh(z_mean - 1.96 * z_std)\n",
    "            out[\"rho_high\"] = np.tanh(z_mean + 1.96 * z_std)\n",
    "            out[\"n_total\"] = g[\"n\"].sum()\n",
    "\n",
    "            # Empirical (bootstrap) weighted\n",
    "            out[\"rho_boot_mean\"] = np.average(g[\"rho_boot_median\"].fillna(0), weights=w)\n",
    "            out[\"ci_low_emp\"] = np.average(g[\"ci_low\"].fillna(0), weights=w)\n",
    "            out[\"ci_high_emp\"] = np.average(g[\"ci_high\"].fillna(0), weights=w)\n",
    "\n",
    "        for k in group_cols:\n",
    "            out[k] = g[k].iloc[0]\n",
    "        return pd.DataFrame([out])\n",
    "\n",
    "    df_summary = pd.concat(\n",
    "        [_weighted_stats(g) for _, g in df.groupby(group_cols, group_keys=False)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    if ci_mode == \"empirical\":\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"ci_low_emp\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"ci_high_emp\"]\n",
    "    elif ci_mode == \"fisherz\":\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"rho_low\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"rho_high\"]\n",
    "    else:\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"rho_low\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"rho_high\"]\n",
    "\n",
    "    out_summary = Path(output_dir) / f\"lw_{model}_corr_pooled_summary_{ci_mode}.csv\"\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    print(f\"[saved summary] {out_summary}\")\n",
    "    print(f\"[diag] rows={len(df_summary)}  groups={df[group_cols].drop_duplicates().shape[0]}\")\n",
    "    print(f\"[NaN summary rows: {df_summary['rho_mean'].isna().sum()}]\")\n",
    "    return df_summary\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run pooled correlation pipeline \n",
    "# ============================================================\n",
    "BASE = Path(\"saved_data\")\n",
    "model = \"m_quant\"\n",
    "output_dir = BASE / \"summary\" / model\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "topk_dir = BASE / \"topk\" / model\n",
    "df_topk = merge_parquet_files(topk_dir)\n",
    "df_topk = expand_topk_metrics(\n",
    "    df_topk,\n",
    "    metrics=[\"jaccard\", \"acc_A\", \"acc_B\", \"disagree_correct\"],\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\")\n",
    ")\n",
    "df_topk = preprocess_anchors(df_topk)\n",
    "\n",
    "for m in [\"cosine_sim\", \"l2_dist\"]:\n",
    "    if f\"{m}_raw\" in df_topk.columns:\n",
    "        for mode in [\"unit_rms\", \"norm_rms\"]:\n",
    "            col_src = f\"{m}_raw\"\n",
    "            col_dst = f\"{m}_{mode}\"\n",
    "            if col_dst not in df_topk.columns:\n",
    "                df_topk[col_dst] = df_topk[col_src]\n",
    "                print(f\"[copy] propagated {col_src} → {col_dst}\")\n",
    "\n",
    "anchors = [\"disagree_correct\", \"logp_diff\"]\n",
    "metrics = [\n",
    "    \"kl_ab\", \"kl_ba\", \"js_div\", \"js_dist\", \"tvd\",\n",
    "    \"entropy_A\", \"entropy_B\", \"cosine_sim\", \"l2_dist\",\n",
    "    \"ppl_diff\", \"jaccard\", \"acc_A\", \"acc_B\"\n",
    "]\n",
    "\n",
    "df_corr_all = []\n",
    "for a in anchors:\n",
    "    df_corr_all.append(correlate_layers_by_anchor(df_topk, a, metrics, model=model))\n",
    "df_corr_all = pd.concat(df_corr_all, ignore_index=True)\n",
    "\n",
    "pooled_corr_path = output_dir / f\"lw_{model}_corr_pooled.csv\"\n",
    "pooled_summary_path = output_dir / f\"lw_{model}_corr_pooled_summary.csv\"\n",
    "\n",
    "df_corr_all.to_csv(pooled_corr_path, index=False)\n",
    "print(f\"[saved pooled correlations] {pooled_corr_path}\")\n",
    "\n",
    "df_summary = summarize_correlations(df_corr_all, output_dir=output_dir, model=model)\n",
    "print(f\"[saved pooled summary] {pooled_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efabf6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric\n",
       "ppl_diff       1.0\n",
       "acc_A_@10      0.0\n",
       "acc_A_@1       0.0\n",
       "acc_A_@5       0.0\n",
       "acc_B_@1       0.0\n",
       "acc_B_@5       0.0\n",
       "acc_B_@10      0.0\n",
       "entropy_A      0.0\n",
       "entropy_B      0.0\n",
       "jaccard_@1     0.0\n",
       "cosine_sim     0.0\n",
       "jaccard_@10    0.0\n",
       "jaccard_@5     0.0\n",
       "js_div         0.0\n",
       "js_dist        0.0\n",
       "kl_ab          0.0\n",
       "kl_ba          0.0\n",
       "l2_dist        0.0\n",
       "tvd            0.0\n",
       "Name: rho, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_all[df_corr_all[\"rho\"].isna()].groupby(\"layer_name\").size()\n",
    "\n",
    "df_corr_all.groupby(\"metric\")[\"rho\"].apply(lambda s: s.isna().mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513a1168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode\n",
       "norm_rms   -0.040288\n",
       "raw        -0.040288\n",
       "unit_rms   -0.040288\n",
       "Name: rho, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_all.query(\"metric.str.contains('cosine_sim')\").groupby(\"mode\")[\"rho\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9e4281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rho  rho_mean\n",
      "rho       1.0       1.0\n",
      "rho_mean  1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "merged = pd.merge(\n",
    "    df_corr_all, df_summary,\n",
    "    on=[\"mode\",\"anchor\",\"metric\",\"corr_type\",\"layer_name\",\"layer_index\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_corr\",\"_sum\")\n",
    ")\n",
    "print(merged[[\"rho\",\"rho_mean\"]].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bf0388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>anchor</th>\n",
       "      <th>metric</th>\n",
       "      <th>corr_type</th>\n",
       "      <th>layer_name</th>\n",
       "      <th>layer_index</th>\n",
       "      <th>rho</th>\n",
       "      <th>rho_boot_median</th>\n",
       "      <th>ci_low</th>\n",
       "      <th>ci_high</th>\n",
       "      <th>p_val</th>\n",
       "      <th>p_perm</th>\n",
       "      <th>n</th>\n",
       "      <th>pooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>kl_ab</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.759996e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>kl_ba</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.018672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.848729e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>js_div</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.115474e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>js_dist</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.002287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.709944e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>tvd</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.012805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.632298e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_A_@5</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.160517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.829454e-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_A_@10</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.181674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.103509e-38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_B_@1</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.064232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.985200e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_B_@5</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.125667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.296032e-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_B_@10</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.156016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.490267e-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5044</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3834 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mode            anchor     metric      corr_type    layer_name  \\\n",
       "0          raw  disagree_correct      kl_ab  pointbiserial  embed_tokens   \n",
       "1          raw  disagree_correct      kl_ba  pointbiserial  embed_tokens   \n",
       "2          raw  disagree_correct     js_div  pointbiserial  embed_tokens   \n",
       "3          raw  disagree_correct    js_dist  pointbiserial  embed_tokens   \n",
       "4          raw  disagree_correct        tvd  pointbiserial  embed_tokens   \n",
       "...        ...               ...        ...            ...           ...   \n",
       "3829  norm_rms         logp_diff   acc_A_@5  pointbiserial        output   \n",
       "3830  norm_rms         logp_diff  acc_A_@10  pointbiserial        output   \n",
       "3831  norm_rms         logp_diff   acc_B_@1  pointbiserial        output   \n",
       "3832  norm_rms         logp_diff   acc_B_@5  pointbiserial        output   \n",
       "3833  norm_rms         logp_diff  acc_B_@10  pointbiserial        output   \n",
       "\n",
       "      layer_index       rho  rho_boot_median  ci_low  ci_high         p_val  \\\n",
       "0              -1  0.023328              NaN     NaN      NaN  9.759996e-02   \n",
       "1              -1 -0.018672              NaN     NaN      NaN  1.848729e-01   \n",
       "2              -1  0.001565              NaN     NaN      NaN  9.115474e-01   \n",
       "3              -1 -0.002287              NaN     NaN      NaN  8.709944e-01   \n",
       "4              -1 -0.012805              NaN     NaN      NaN  3.632298e-01   \n",
       "...           ...       ...              ...     ...      ...           ...   \n",
       "3829           32  0.160517              NaN     NaN      NaN  1.829454e-30   \n",
       "3830           32  0.181674              NaN     NaN      NaN  1.103509e-38   \n",
       "3831           32  0.064232              NaN     NaN      NaN  4.985200e-06   \n",
       "3832           32  0.125667              NaN     NaN      NaN  3.296032e-19   \n",
       "3833           32  0.156016              NaN     NaN      NaN  7.490267e-29   \n",
       "\n",
       "      p_perm     n pooling  \n",
       "0        NaN  5044  pooled  \n",
       "1        NaN  5044  pooled  \n",
       "2        NaN  5044  pooled  \n",
       "3        NaN  5044  pooled  \n",
       "4        NaN  5044  pooled  \n",
       "...      ...   ...     ...  \n",
       "3829     NaN  5044  pooled  \n",
       "3830     NaN  5044  pooled  \n",
       "3831     NaN  5044  pooled  \n",
       "3832     NaN  5044  pooled  \n",
       "3833     NaN  5044  pooled  \n",
       "\n",
       "[3834 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66dbc9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho_mean</th>\n",
       "      <th>rho_low</th>\n",
       "      <th>rho_high</th>\n",
       "      <th>n_total</th>\n",
       "      <th>rho_boot_mean</th>\n",
       "      <th>ci_low_emp</th>\n",
       "      <th>ci_high_emp</th>\n",
       "      <th>mode</th>\n",
       "      <th>anchor</th>\n",
       "      <th>metric</th>\n",
       "      <th>corr_type</th>\n",
       "      <th>layer_name</th>\n",
       "      <th>layer_index</th>\n",
       "      <th>rho_low_plot</th>\n",
       "      <th>rho_high_plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.10</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.11</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.12</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>-0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004381</td>\n",
       "      <td>-0.004381</td>\n",
       "      <td>-0.004381</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.13</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.004381</td>\n",
       "      <td>-0.004381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.017870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.031248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>0.036456</td>\n",
       "      <td>0.036456</td>\n",
       "      <td>0.036456</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.036456</td>\n",
       "      <td>0.036456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>0.036417</td>\n",
       "      <td>0.036417</td>\n",
       "      <td>0.036417</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.036417</td>\n",
       "      <td>0.036417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>5044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.019022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3834 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rho_mean   rho_low  rho_high  n_total  rho_boot_mean  ci_low_emp  \\\n",
       "0    -0.001788 -0.001788 -0.001788     5044            0.0         0.0   \n",
       "1    -0.002528 -0.002528 -0.002528     5044            0.0         0.0   \n",
       "2    -0.002528 -0.002528 -0.002528     5044            0.0         0.0   \n",
       "3    -0.001788 -0.001788 -0.001788     5044            0.0         0.0   \n",
       "4    -0.004381 -0.004381 -0.004381     5044            0.0         0.0   \n",
       "...        ...       ...       ...      ...            ...         ...   \n",
       "3829  0.017870  0.017870  0.017870     5044            0.0         0.0   \n",
       "3830  0.031248  0.031248  0.031248     5044            0.0         0.0   \n",
       "3831  0.036456  0.036456  0.036456     5044            0.0         0.0   \n",
       "3832  0.036417  0.036417  0.036417     5044            0.0         0.0   \n",
       "3833  0.019022  0.019022  0.019022     5044            0.0         0.0   \n",
       "\n",
       "      ci_high_emp      mode            anchor    metric corr_type layer_name  \\\n",
       "0             0.0  norm_rms  disagree_correct  acc_A_@1       phi    layer.1   \n",
       "1             0.0  norm_rms  disagree_correct  acc_A_@1       phi   layer.10   \n",
       "2             0.0  norm_rms  disagree_correct  acc_A_@1       phi   layer.11   \n",
       "3             0.0  norm_rms  disagree_correct  acc_A_@1       phi   layer.12   \n",
       "4             0.0  norm_rms  disagree_correct  acc_A_@1       phi   layer.13   \n",
       "...           ...       ...               ...       ...       ...        ...   \n",
       "3829          0.0  unit_rms         logp_diff       tvd  spearman    layer.6   \n",
       "3830          0.0  unit_rms         logp_diff       tvd  spearman    layer.7   \n",
       "3831          0.0  unit_rms         logp_diff       tvd  spearman    layer.8   \n",
       "3832          0.0  unit_rms         logp_diff       tvd  spearman    layer.9   \n",
       "3833          0.0  unit_rms         logp_diff       tvd  spearman     output   \n",
       "\n",
       "      layer_index  rho_low_plot  rho_high_plot  \n",
       "0               1     -0.001788      -0.001788  \n",
       "1              10     -0.002528      -0.002528  \n",
       "2              11     -0.002528      -0.002528  \n",
       "3              12     -0.001788      -0.001788  \n",
       "4              13     -0.004381      -0.004381  \n",
       "...           ...           ...            ...  \n",
       "3829            6      0.017870       0.017870  \n",
       "3830            7      0.031248       0.031248  \n",
       "3831            8      0.036456       0.036456  \n",
       "3832            9      0.036417       0.036417  \n",
       "3833           32      0.019022       0.019022  \n",
       "\n",
       "[3834 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e4b07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode                  0\n",
       "anchor                0\n",
       "metric                0\n",
       "corr_type             0\n",
       "layer_name            0\n",
       "layer_index           0\n",
       "rho                 204\n",
       "rho_boot_median    3834\n",
       "ci_low             3834\n",
       "ci_high            3834\n",
       "p_val               897\n",
       "p_perm             3834\n",
       "n                     0\n",
       "pooling               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2add52da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rho_mean         204\n",
       "rho_low          204\n",
       "rho_high         204\n",
       "n_total            0\n",
       "rho_boot_mean    204\n",
       "ci_low_emp       204\n",
       "ci_high_emp      204\n",
       "mode               0\n",
       "anchor             0\n",
       "metric             0\n",
       "corr_type          0\n",
       "layer_name         0\n",
       "layer_index        0\n",
       "rho_low_plot     204\n",
       "rho_high_plot    204\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ae653",
   "metadata": {},
   "source": [
    "### ==============================================\n",
    "### Correlation Per Prompt =======================\n",
    "### =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge] merged 50 parquet files → 17000 rows\n",
      "[diag] unique batch_index=50 | unique prompt_id=10 | unique (prompt,batch)=500\n",
      "[ok] prompt–batch pairs look good\n",
      "[expand] added 36 flattened top-k columns\n",
      "[ok] normalized disagree_correct_raw_@1 → len=15\n",
      "[ok] normalized disagree_correct_unit_rms_@1 → len=15\n",
      "[ok] normalized disagree_correct_norm_rms_@1 → len=15\n",
      "[ok] normalized logp_diff_raw → len=15\n",
      "[ok] normalized logp_diff_unit_rms → len=15\n",
      "[ok] normalized logp_diff_norm_rms → len=15\n",
      "[done] Anchors preprocessed (binary + continuous, unified padding)]\n",
      "[anchor_map] built 500 anchors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "disagree_correct_raw: 100%|██████████| 34/34 [02:16<00:00,  4.00s/it]\n",
      "disagree_correct_unit_rms: 100%|██████████| 34/34 [02:14<00:00,  3.96s/it]\n",
      "disagree_correct_norm_rms: 100%|██████████| 34/34 [02:15<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] disagree_correct → 1938 per-prompt correlations\n",
      "[NaN correlations: 111]\n",
      "[skipped constant=291515]\n",
      "[anchor_map] built 500 anchors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp_diff_raw: 100%|██████████| 34/34 [01:09<00:00,  2.05s/it]\n",
      "logp_diff_unit_rms: 100%|██████████| 34/34 [01:10<00:00,  2.07s/it]\n",
      "logp_diff_norm_rms: 100%|██████████| 34/34 [01:10<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] logp_diff → 1938 per-prompt correlations\n",
      "[NaN correlations: 111]\n",
      "[skipped constant=237054]\n",
      "[saved per-prompt correlations] saved_data/summary/m_quant/lw_m_quant_corr_perprompt.csv\n",
      "[saved summary] saved_data/summary/m_quant/lw_m_quant_corr_perprompt_summary_both.csv\n",
      "[diag] rows=3654  groups=3654\n",
      "[saved per-prompt summary] saved_data/summary/m_quant/lw_m_quant_corr_perprompt_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, pointbiserialr, chi2_contingency\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Merge parquet files\n",
    "# ============================================================\n",
    "def merge_parquet_files(input_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(input_dir).glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {input_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        d = pd.read_parquet(f)\n",
    "        d[\"batch_index\"] = i\n",
    "        dfs.append(d)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[merge] merged {len(files)} parquet files → {len(df)} rows\")\n",
    "\n",
    "    n_batches = df[\"batch_index\"].nunique()\n",
    "    n_prompts = df[\"prompt_id\"].nunique()\n",
    "    n_pairs = df.groupby([\"prompt_id\", \"batch_index\"]).ngroups\n",
    "    print(f\"[diag] unique batch_index={n_batches} | unique prompt_id={n_prompts} | unique (prompt,batch)={n_pairs}\")\n",
    "\n",
    "    if n_pairs < len(files) * 10:\n",
    "        print(\"[warn] fewer prompt–batch pairs than expected — possible ID overlap?\")\n",
    "    else:\n",
    "        print(\"[ok] prompt–batch pairs look good\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Expand nested top-k metrics \n",
    "# ============================================================\n",
    "def expand_topk_metrics(df, metrics, modes=(\"raw\",\"unit_rms\",\"norm_rms\"), topk_levels=(1,5,10)):\n",
    "    new_cols=[]\n",
    "    for metric in metrics:\n",
    "        for mode in modes:\n",
    "            base=f\"{metric}_{mode}\"\n",
    "            if base not in df.columns:\n",
    "                continue\n",
    "            for k in topk_levels:\n",
    "                new=f\"{base}_@{k}\"\n",
    "                df[new]=df[base].apply(\n",
    "                    lambda d: np.array(d.get(f\"@{k}\",[]),float)\n",
    "                    if isinstance(d,dict) and f\"@{k}\" in d else np.array([])\n",
    "                )\n",
    "                new_cols.append(new)\n",
    "    print(f\"[expand] added {len(new_cols)} flattened top-k columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper\n",
    "# ============================================================\n",
    "\"\"\"def safe_flatten(x):\n",
    "    if isinstance(x,(list,np.ndarray)):\n",
    "        return np.array(x,float).flatten()\n",
    "    return np.array([],float)\"\"\"\n",
    "def safe_flatten(v):\n",
    "    \"\"\"Ensure flattening of nested lists/arrays into 1D np.array.\"\"\"\n",
    "    if isinstance(v, (list, np.ndarray)):\n",
    "        return np.asarray(v, dtype=float).flatten()\n",
    "    try:\n",
    "        return np.array([float(v)], dtype=float)\n",
    "    except Exception:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# Extract anchors \n",
    "# ============================================================\n",
    "def extract_top1_disagreement_anchor(df, modes=(\"raw\",\"unit_rms\",\"norm_rms\")):\n",
    "    for mode in modes:\n",
    "        col=f\"disagree_correct_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "        max_len=int(df[col].apply(\n",
    "            lambda d: len(d.get(\"@1\",[])) if isinstance(d,dict) else 0\n",
    "        ).max() or 0)\n",
    "        def to_array(d):\n",
    "            if isinstance(d,dict) and \"@1\" in d:\n",
    "                arr=np.array(d[\"@1\"],float)\n",
    "            else:\n",
    "                arr=np.full(max_len,np.nan)\n",
    "            return arr\n",
    "        df[f\"{col}_@1\"]=df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col}_@1 → len={max_len}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_anchors(df, modes=(\"raw\", \"unit_rms\", \"norm_rms\")):\n",
    "    # === BINARY ANCHOR ===\n",
    "    for mode in modes:\n",
    "        col = f\"disagree_correct_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        # find max sequence length (@1-level)\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda d: len(d.get(\"@1\", [])) if isinstance(d, dict) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(d):\n",
    "            if isinstance(d, dict) and \"@1\" in d:\n",
    "                arr = np.asarray(d[\"@1\"], dtype=float)\n",
    "            else:\n",
    "                arr = np.full(max_len, np.nan, dtype=float)\n",
    "            # pad to consistent length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[f\"{col}_@1\"] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col}_@1 → len={max_len}\")\n",
    "\n",
    "    # === CONTINUOUS ANCHOR ===\n",
    "    for mode in modes:\n",
    "        col = f\"logp_diff_{mode}\"\n",
    "        if col not in df.columns:\n",
    "            print(f\"[warn] missing {col}\")\n",
    "            continue\n",
    "\n",
    "        # find max length across all prompts\n",
    "        max_len = int(df[col].apply(\n",
    "            lambda v: len(v) if isinstance(v, (list, np.ndarray)) else 0\n",
    "        ).max() or 0)\n",
    "\n",
    "        def to_array(v):\n",
    "            if isinstance(v, (list, np.ndarray)):\n",
    "                arr = np.asarray(v, dtype=float)\n",
    "            else:\n",
    "                try:\n",
    "                    arr = np.array([float(v)], dtype=float)\n",
    "                except Exception:\n",
    "                    arr = np.array([np.nan], dtype=float)\n",
    "            # pad to uniform length\n",
    "            if len(arr) < max_len:\n",
    "                arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "            return arr\n",
    "\n",
    "        df[col] = df[col].apply(to_array)\n",
    "        print(f\"[ok] normalized {col} → len={max_len}\")\n",
    "\n",
    "    print(\"[done] Anchors preprocessed (binary + continuous, unified padding)]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation \n",
    "# ============================================================\n",
    "def _is_binary(arr, tol=1e-6):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if len(arr) == 0:\n",
    "        return False\n",
    "    u = np.unique(np.round(arr, 6))\n",
    "    return np.all((np.abs(u - 0) < tol) | (np.abs(u - 1) < tol))\n",
    "\n",
    "\n",
    "def _phi_coefficient(x, y):\n",
    "    \"\"\"Phi coefficient for two binary arrays (0/1).\"\"\"\n",
    "    x, y = np.asarray(x).astype(int), np.asarray(y).astype(int)\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        table = pd.crosstab(x, y)\n",
    "        if table.shape != (2, 2):\n",
    "            return np.nan\n",
    "        chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
    "        sign = np.sign((table.loc[1, 1] * table.loc[0, 0]) - (table.loc[1, 0] * table.loc[0, 1]))\n",
    "        return float(sign * np.sqrt(chi2 / len(x)))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- Deterministic metric-type mapping ---\n",
    "\"\"\"METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"binary\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"binary\",\n",
    "    \"agree_correct\": \"binary\",\n",
    "    \"agree_wrong\": \"binary\",\n",
    "    \"agree_set\": \"binary\",\n",
    "    \"disagree_set\": \"binary\",\n",
    "\n",
    "    \"jaccard_@1\": \"binary\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\"\"\"\n",
    "METRIC_TYPES = {\n",
    "    **{f\"acc_A_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    **{f\"acc_B_@{k}\": \"continuous\" for k in [1, 5, 10]},\n",
    "    \"disagree_correct\": \"continuous\",\n",
    "    \"agree_correct\": \"continuous\",\n",
    "    \"agree_wrong\": \"continuous\",\n",
    "    \"agree_set\": \"continuous\",\n",
    "    \"disagree_set\": \"continuous\",\n",
    "\n",
    "    \"jaccard_@1\": \"continuous\",\n",
    "    \"jaccard_@5\": \"continuous\",\n",
    "    \"jaccard_@10\": \"continuous\",\n",
    "}\n",
    "\n",
    "def _choose_corr_func_fixed(anchor, metric):\n",
    "    \"\"\"Deterministic correlation type based on known data nature.\"\"\"\n",
    "    anchor_t = \"binary\" if \"disagree_correct\" in anchor else \"continuous\"\n",
    "    metric_t = METRIC_TYPES.get(metric, \"continuous\")\n",
    "\n",
    "    if anchor_t == \"binary\" and metric_t == \"binary\":\n",
    "        return \"phi\", _phi_coefficient\n",
    "    elif anchor_t == \"binary\" or metric_t == \"binary\":\n",
    "        return \"pointbiserial\", pointbiserialr\n",
    "    else:\n",
    "        return \"spearman\", spearmanr\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Correlation (per-prompt)\n",
    "# ============================================================\n",
    "def correlate_layers_by_anchor_perprompt(\n",
    "    df,\n",
    "    anchor,\n",
    "    metrics,\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\"),\n",
    "    topk_levels=(1, 5, 10),\n",
    "    n_boot=10,\n",
    "    n_perm=10,\n",
    "    seed=42,\n",
    "    output_dir=None,\n",
    "    model=\"m_8bit\",\n",
    "    min_valid=3,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    results = []\n",
    "    df_out = df[df[\"layer_name\"].str.lower() == \"output\"]\n",
    "\n",
    "    # --- Build anchor map from output layer ---\n",
    "    anchor_map = {}\n",
    "    for _, row in df_out.iterrows():\n",
    "        key = (int(row[\"prompt_id\"]), int(row.get(\"batch_index\", 0)))\n",
    "        anchor_vecs = {}\n",
    "        for mode in modes:\n",
    "            col1 = f\"{anchor}_{mode}_@1\"\n",
    "            col2 = f\"{anchor}_{mode}\"\n",
    "            col = col1 if col1 in df.columns else col2\n",
    "            val = row.get(col)\n",
    "            if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
    "                anchor_vecs[mode] = np.array(val, float)\n",
    "        if anchor_vecs:\n",
    "            anchor_map[key] = anchor_vecs\n",
    "    print(f\"[anchor_map] built {len(anchor_map)} anchors\")\n",
    "\n",
    "    skip_const = 0\n",
    "    SHARED_METRICS = {\"cosine_sim\", \"l2_dist\"}\n",
    "\n",
    "    # --- Main correlation loop ---\n",
    "    for mode in modes:\n",
    "        for (lname, lidx), layer_df in tqdm(\n",
    "            df.groupby([\"layer_name\", \"layer_index\"]),\n",
    "            desc=f\"{anchor}_{mode}\"\n",
    "        ):\n",
    "            for metric in metrics:\n",
    "                is_topk_only = metric in {\"acc_A\", \"acc_B\", \"jaccard\"}\n",
    "                suffixes = [f\"_@{k}\" for k in topk_levels] if is_topk_only else [\"\"] + [f\"_@{k}\" for k in topk_levels]\n",
    "\n",
    "                for suffix in suffixes:\n",
    "                    src_mode = \"raw\" if metric in SHARED_METRICS else mode\n",
    "                    mcol = f\"{metric}_{src_mode}{suffix}\"\n",
    "                    if mcol not in layer_df.columns:\n",
    "                        continue\n",
    "\n",
    "                    per_prompt_rhos, per_prompt_ns = [], []\n",
    "\n",
    "                    for (pid, bid), group in layer_df.groupby([\"prompt_id\", \"batch_index\"]):\n",
    "                        key = (int(pid), int(bid))\n",
    "                        if key not in anchor_map or mode not in anchor_map[key]:\n",
    "                            continue\n",
    "\n",
    "                        a = anchor_map[key][mode]\n",
    "                        m = safe_flatten(group[mcol].iloc[0])\n",
    "                        n = min(len(a), len(m))\n",
    "                        if n < min_valid:\n",
    "                            continue\n",
    "\n",
    "                        a, m = a[:n], m[:n]\n",
    "                        mask = np.isfinite(a) & np.isfinite(m)\n",
    "                        n_used = int(mask.sum())\n",
    "                        if n_used < min_valid:\n",
    "                            continue\n",
    "\n",
    "                        if np.std(a[mask]) == 0 or np.std(m[mask]) == 0:\n",
    "                            skip_const += 1\n",
    "                            continue\n",
    "\n",
    "                        cname, corr_func = _choose_corr_func_fixed(anchor, f\"{metric}{suffix}\")\n",
    "\n",
    "                        try:\n",
    "                            if cname == \"phi\":\n",
    "                                rho = corr_func(a[mask], m[mask])\n",
    "                            else:\n",
    "                                rho, _ = corr_func(a[mask], m[mask])\n",
    "                        except Exception:\n",
    "                            rho = np.nan\n",
    "\n",
    "                        if np.isfinite(rho):\n",
    "                            per_prompt_rhos.append(rho)\n",
    "                            per_prompt_ns.append(n_used)\n",
    "\n",
    "                    # --- Handle missing prompts ---\n",
    "                    if not per_prompt_rhos:\n",
    "                        results.append({\n",
    "                            \"mode\": mode,\n",
    "                            \"anchor\": anchor,\n",
    "                            \"metric\": metric + suffix,\n",
    "                            \"corr_type\": \"undefined\",\n",
    "                            \"layer_name\": lname,\n",
    "                            \"layer_index\": lidx,\n",
    "                            \"rho\": np.nan,\n",
    "                            \"rho_boot_median\": np.nan,\n",
    "                            \"ci_low\": np.nan,\n",
    "                            \"ci_high\": np.nan,\n",
    "                            \"p_perm\": np.nan,\n",
    "                            \"n_prompts\": 0,\n",
    "                            \"n_used_mean\": np.nan,\n",
    "                            \"n_used_median\": np.nan,\n",
    "                            \"pooling\": \"per_prompt\"\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    # --- Aggregate correlations across prompts ---\n",
    "                    A = np.array(per_prompt_rhos)\n",
    "                    N = np.array(per_prompt_ns)\n",
    "                    rho = np.average(A, weights=N) if len(N) > 0 else np.nan\n",
    "                    \n",
    "                    \"\"\"z = np.arctanh(A)            # Fisher transform\n",
    "                    rho = np.tanh(np.average(z, weights=N))\"\"\"\n",
    "\n",
    "                    n_mean, n_median = np.nanmean(N), np.nanmedian(N)\n",
    "\n",
    "                    # --- Bootstrap CI ---\n",
    "                    if len(A) >= min_valid:\n",
    "                        boot_rhos = []\n",
    "                        for _ in range(n_boot):\n",
    "                            idx = np.random.choice(len(A), len(A), replace=True)\n",
    "                            boot_rhos.append(np.average(A[idx], weights=N[idx]))\n",
    "                        if len(boot_rhos) > 20:\n",
    "                            ci_low, ci_high = np.percentile(boot_rhos, [2.5, 97.5])\n",
    "                            rho_boot = np.median(boot_rhos)\n",
    "                        else:\n",
    "                            ci_low = ci_high = rho_boot = np.nan\n",
    "                    else:\n",
    "                        ci_low = ci_high = rho_boot = np.nan\n",
    "\n",
    "                    # --- Permutation test ---\n",
    "                    if len(A) >= min_valid:\n",
    "                        perm_rhos = []\n",
    "                        for _ in range(n_perm):\n",
    "                            perm_rhos.append(np.average(np.random.permutation(A), weights=N))\n",
    "                        if len(perm_rhos) > 20:\n",
    "                            perm_rhos = np.array(perm_rhos)\n",
    "                            p_perm = (np.sum(np.abs(perm_rhos) >= abs(rho)) + 1) / (len(perm_rhos) + 1)\n",
    "                        else:\n",
    "                            p_perm = np.nan\n",
    "                    else:\n",
    "                        p_perm = np.nan\n",
    "\n",
    "                    # --- Save results ---\n",
    "                    results.append({\n",
    "                        \"mode\": mode,\n",
    "                        \"anchor\": anchor,\n",
    "                        \"metric\": metric + suffix,\n",
    "                        \"corr_type\": cname,\n",
    "                        \"layer_name\": lname,\n",
    "                        \"layer_index\": lidx,\n",
    "                        \"rho\": rho,\n",
    "                        \"rho_boot_median\": rho_boot,\n",
    "                        \"ci_low\": ci_low,\n",
    "                        \"ci_high\": ci_high,\n",
    "                        \"p_perm\": p_perm,\n",
    "                        \"n_prompts\": len(A),\n",
    "                        \"n_used_mean\": n_mean,\n",
    "                        \"n_used_median\": n_median,\n",
    "                        \"pooling\": \"per_prompt\"\n",
    "                    })\n",
    "\n",
    "    # --- Final output ---\n",
    "    df_corr = pd.DataFrame(results)\n",
    "    print(f\"[ok] {anchor} → {len(df_corr)} per-prompt correlations\")\n",
    "    print(f\"[NaN correlations: {df_corr['rho'].isna().sum()}]\")\n",
    "    print(f\"[skipped constant={skip_const}]\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = Path(output_dir) / f\"lw_{model}_{anchor}_corr_perprompt.csv\"\n",
    "        df_corr.to_csv(out_path, index=False)\n",
    "        print(f\"[saved per-prompt correlations] {out_path}\")\n",
    "\n",
    "    return df_corr\n",
    "\n",
    "# ============================================================\n",
    "# summarize_correlations_perprompt (fix for single-row groups)\n",
    "# ============================================================\n",
    "def summarize_correlations_perprompt(df_corr, output_dir, model, ci_mode=\"both\"):\n",
    "    df = df_corr.dropna(subset=[\"rho\"]).copy()\n",
    "    if \"n\" not in df.columns:\n",
    "        df[\"n\"] = 1\n",
    "    df[\"z\"] = np.arctanh(np.clip(df[\"rho\"], -0.999999, 0.999999))\n",
    "\n",
    "    drop_cols = [c for c in [\"prompt_id\", \"batch_index\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    group_cols = [\"mode\", \"anchor\", \"metric\", \"corr_type\", \"layer_name\", \"layer_index\"]\n",
    "\n",
    "    def _weighted_stats(g):\n",
    "        out = {}\n",
    "        if len(g) < 2:\n",
    "            out[\"rho_mean\"] = g[\"rho\"].iloc[0]\n",
    "            out[\"rho_low\"] = out[\"rho_high\"] = g[\"rho\"].iloc[0]\n",
    "            out[\"rho_boot_mean\"] = g[\"rho_boot_median\"].iloc[0]\n",
    "            out[\"ci_low_emp\"] = out[\"ci_high_emp\"] = np.nan\n",
    "            out[\"n_total\"] = g[\"n\"].sum()\n",
    "            for k in group_cols:\n",
    "                out[k] = g[k].iloc[0]\n",
    "            return pd.DataFrame([out])\n",
    "\n",
    "        w = g[\"n\"]\n",
    "        z = np.arctanh(np.clip(g[\"rho\"], -0.999999, 0.999999))\n",
    "        z_mean = np.average(z, weights=w)\n",
    "        z_std = np.sqrt(np.average((z - z_mean)**2, weights=w))\n",
    "        out[\"rho_mean\"] = np.tanh(z_mean)\n",
    "        out[\"rho_low\"] = np.tanh(z_mean - 1.96 * z_std)\n",
    "        out[\"rho_high\"] = np.tanh(z_mean + 1.96 * z_std)\n",
    "        out[\"n_total\"] = g[\"n\"].sum()\n",
    "\n",
    "        out[\"rho_boot_mean\"] = np.average(g[\"rho_boot_median\"], weights=w)\n",
    "        out[\"ci_low_emp\"] = np.average(g[\"ci_low\"], weights=w)\n",
    "        out[\"ci_high_emp\"] = np.average(g[\"ci_high\"], weights=w)\n",
    "\n",
    "        for k in group_cols:\n",
    "            out[k] = g[k].iloc[0]\n",
    "        return pd.DataFrame([out])\n",
    "\n",
    "    df_summary = pd.concat(\n",
    "        [_weighted_stats(g) for _, g in df.groupby(group_cols, group_keys=False)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    if ci_mode in (\"empirical\", \"both\"):\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"ci_low_emp\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"ci_high_emp\"]\n",
    "    else:\n",
    "        df_summary[\"rho_low_plot\"] = df_summary[\"rho_low\"]\n",
    "        df_summary[\"rho_high_plot\"] = df_summary[\"rho_high\"]\n",
    "\n",
    "    out_summary = Path(output_dir) / f\"lw_{model}_corr_perprompt_summary_{ci_mode}.csv\"\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "    print(f\"[saved summary] {out_summary}\")\n",
    "    print(f\"[diag] rows={len(df_summary)}  groups={df[group_cols].drop_duplicates().shape[0]}\")\n",
    "    return df_summary\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run per-prompt correlation pipeline\n",
    "# ============================================================\n",
    "BASE = Path(\"saved_data\")\n",
    "model = \"m_quant\"\n",
    "output_dir = BASE / \"summary\" / model\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "topk_dir = BASE / \"topk\" / model\n",
    "df_topk = merge_parquet_files(topk_dir)\n",
    "df_topk = expand_topk_metrics(\n",
    "    df_topk,\n",
    "    metrics=[\"jaccard\", \"acc_A\", \"acc_B\", \"disagree_correct\"],\n",
    "    modes=(\"raw\", \"unit_rms\", \"norm_rms\")\n",
    ")\n",
    "df_topk = preprocess_anchors(df_topk)\n",
    "\n",
    "for m in [\"cosine_sim\", \"l2_dist\"]:\n",
    "    if f\"{m}_raw\" in df_topk.columns:\n",
    "        for mode in [\"unit_rms\", \"norm_rms\"]:\n",
    "            col_src = f\"{m}_raw\"\n",
    "            col_dst = f\"{m}_{mode}\"\n",
    "            if col_dst not in df_topk.columns:\n",
    "                df_topk[col_dst] = df_topk[col_src]\n",
    "                print(f\"[copy] propagated {col_src} → {col_dst}\")\n",
    "\n",
    "anchors = [\"disagree_correct\", \"logp_diff\"]\n",
    "metrics = [\n",
    "    \"kl_ab\", \"kl_ba\", \"js_div\", \"js_dist\", \"tvd\",\n",
    "    \"entropy_A\", \"entropy_B\", \"cosine_sim\", \"l2_dist\",\n",
    "    \"ppl_diff\", \"jaccard\", \"acc_A\", \"acc_B\"\n",
    "]\n",
    "\n",
    "df_corr_perprompt_all = []\n",
    "for a in anchors:\n",
    "    df_corr_perprompt_all.append(\n",
    "        correlate_layers_by_anchor_perprompt(df_topk, a, metrics, model=model)\n",
    "    )\n",
    "df_corr_perprompt_all = pd.concat(df_corr_perprompt_all, ignore_index=True)\n",
    "\n",
    "perprompt_corr_path = output_dir / f\"lw_{model}_corr_perprompt.csv\"\n",
    "perprompt_summary_path = output_dir / f\"lw_{model}_corr_perprompt_summary.csv\"\n",
    "\n",
    "df_corr_perprompt_all.to_csv(perprompt_corr_path, index=False)\n",
    "print(f\"[saved per-prompt correlations] {perprompt_corr_path}\")\n",
    "\n",
    "df_summary_pp = summarize_correlations_perprompt(df_corr_perprompt_all, output_dir=output_dir, model=model)\n",
    "print(f\"[saved per-prompt summary] {perprompt_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce7dcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode\n",
       "norm_rms   -0.034492\n",
       "raw        -0.034492\n",
       "unit_rms   -0.034492\n",
       "Name: rho, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_perprompt_all.query(\"metric.str.contains('cosine_sim')\").groupby(\"mode\")[\"rho\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19e8453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>anchor</th>\n",
       "      <th>metric</th>\n",
       "      <th>corr_type</th>\n",
       "      <th>layer_name</th>\n",
       "      <th>layer_index</th>\n",
       "      <th>rho</th>\n",
       "      <th>rho_boot_median</th>\n",
       "      <th>ci_low</th>\n",
       "      <th>ci_high</th>\n",
       "      <th>p_perm</th>\n",
       "      <th>n_prompts</th>\n",
       "      <th>n_used_mean</th>\n",
       "      <th>n_used_median</th>\n",
       "      <th>pooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>kl_ab</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.046042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>10.935065</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>kl_ba</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.051954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>10.935065</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>js_div</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>10.935065</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>js_dist</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>10.935065</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>tvd</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>embed_tokens</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.014360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>10.935065</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_A_@5</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.192738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10.088000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_A_@10</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.215255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10.088000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_B_@1</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.092933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494</td>\n",
       "      <td>10.107287</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_B_@5</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.161631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10.088000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>norm_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>acc_B_@10</td>\n",
       "      <td>pointbiserial</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>0.190724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10.088000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>per_prompt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3876 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mode            anchor     metric      corr_type    layer_name  \\\n",
       "0          raw  disagree_correct      kl_ab  pointbiserial  embed_tokens   \n",
       "1          raw  disagree_correct      kl_ba  pointbiserial  embed_tokens   \n",
       "2          raw  disagree_correct     js_div  pointbiserial  embed_tokens   \n",
       "3          raw  disagree_correct    js_dist  pointbiserial  embed_tokens   \n",
       "4          raw  disagree_correct        tvd  pointbiserial  embed_tokens   \n",
       "...        ...               ...        ...            ...           ...   \n",
       "3871  norm_rms         logp_diff   acc_A_@5  pointbiserial        output   \n",
       "3872  norm_rms         logp_diff  acc_A_@10  pointbiserial        output   \n",
       "3873  norm_rms         logp_diff   acc_B_@1  pointbiserial        output   \n",
       "3874  norm_rms         logp_diff   acc_B_@5  pointbiserial        output   \n",
       "3875  norm_rms         logp_diff  acc_B_@10  pointbiserial        output   \n",
       "\n",
       "      layer_index       rho  rho_boot_median  ci_low  ci_high  p_perm  \\\n",
       "0              -1  0.046042              NaN     NaN      NaN     NaN   \n",
       "1              -1 -0.051954              NaN     NaN      NaN     NaN   \n",
       "2              -1  0.018521              NaN     NaN      NaN     NaN   \n",
       "3              -1  0.014579              NaN     NaN      NaN     NaN   \n",
       "4              -1 -0.014360              NaN     NaN      NaN     NaN   \n",
       "...           ...       ...              ...     ...      ...     ...   \n",
       "3871           32  0.192738              NaN     NaN      NaN     NaN   \n",
       "3872           32  0.215255              NaN     NaN      NaN     NaN   \n",
       "3873           32  0.092933              NaN     NaN      NaN     NaN   \n",
       "3874           32  0.161631              NaN     NaN      NaN     NaN   \n",
       "3875           32  0.190724              NaN     NaN      NaN     NaN   \n",
       "\n",
       "      n_prompts  n_used_mean  n_used_median     pooling  \n",
       "0            77    10.935065           10.0  per_prompt  \n",
       "1            77    10.935065           10.0  per_prompt  \n",
       "2            77    10.935065           10.0  per_prompt  \n",
       "3            77    10.935065           10.0  per_prompt  \n",
       "4            77    10.935065           10.0  per_prompt  \n",
       "...         ...          ...            ...         ...  \n",
       "3871        500    10.088000           10.0  per_prompt  \n",
       "3872        500    10.088000           10.0  per_prompt  \n",
       "3873        494    10.107287           10.0  per_prompt  \n",
       "3874        500    10.088000           10.0  per_prompt  \n",
       "3875        500    10.088000           10.0  per_prompt  \n",
       "\n",
       "[3876 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_perprompt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12453629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho_mean</th>\n",
       "      <th>rho_low</th>\n",
       "      <th>rho_high</th>\n",
       "      <th>rho_boot_mean</th>\n",
       "      <th>ci_low_emp</th>\n",
       "      <th>ci_high_emp</th>\n",
       "      <th>n_total</th>\n",
       "      <th>mode</th>\n",
       "      <th>anchor</th>\n",
       "      <th>metric</th>\n",
       "      <th>corr_type</th>\n",
       "      <th>layer_name</th>\n",
       "      <th>layer_index</th>\n",
       "      <th>rho_low_plot</th>\n",
       "      <th>rho_high_plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.12</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.14</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.096822</td>\n",
       "      <td>-0.096822</td>\n",
       "      <td>-0.096822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>norm_rms</td>\n",
       "      <td>disagree_correct</td>\n",
       "      <td>acc_A_@1</td>\n",
       "      <td>phi</td>\n",
       "      <td>layer.15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.6</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>0.031489</td>\n",
       "      <td>0.031489</td>\n",
       "      <td>0.031489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>layer.9</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>0.018948</td>\n",
       "      <td>0.018948</td>\n",
       "      <td>0.018948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>unit_rms</td>\n",
       "      <td>logp_diff</td>\n",
       "      <td>tvd</td>\n",
       "      <td>spearman</td>\n",
       "      <td>output</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3530 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rho_mean   rho_low  rho_high  rho_boot_mean  ci_low_emp  ci_high_emp  \\\n",
       "0    -0.071429 -0.071429 -0.071429            NaN         NaN          NaN   \n",
       "1    -0.076923 -0.076923 -0.076923            NaN         NaN          NaN   \n",
       "2    -0.071429 -0.071429 -0.071429            NaN         NaN          NaN   \n",
       "3    -0.071429 -0.071429 -0.071429            NaN         NaN          NaN   \n",
       "4    -0.096822 -0.096822 -0.096822            NaN         NaN          NaN   \n",
       "...        ...       ...       ...            ...         ...          ...   \n",
       "3525  0.006513  0.006513  0.006513            NaN         NaN          NaN   \n",
       "3526  0.026021  0.026021  0.026021            NaN         NaN          NaN   \n",
       "3527  0.031489  0.031489  0.031489            NaN         NaN          NaN   \n",
       "3528  0.043021  0.043021  0.043021            NaN         NaN          NaN   \n",
       "3529  0.018948  0.018948  0.018948            NaN         NaN          NaN   \n",
       "\n",
       "      n_total      mode            anchor    metric corr_type layer_name  \\\n",
       "0           1  norm_rms  disagree_correct  acc_A_@1       phi   layer.11   \n",
       "1           1  norm_rms  disagree_correct  acc_A_@1       phi   layer.12   \n",
       "2           1  norm_rms  disagree_correct  acc_A_@1       phi   layer.13   \n",
       "3           1  norm_rms  disagree_correct  acc_A_@1       phi   layer.14   \n",
       "4           1  norm_rms  disagree_correct  acc_A_@1       phi   layer.15   \n",
       "...       ...       ...               ...       ...       ...        ...   \n",
       "3525        1  unit_rms         logp_diff       tvd  spearman    layer.6   \n",
       "3526        1  unit_rms         logp_diff       tvd  spearman    layer.7   \n",
       "3527        1  unit_rms         logp_diff       tvd  spearman    layer.8   \n",
       "3528        1  unit_rms         logp_diff       tvd  spearman    layer.9   \n",
       "3529        1  unit_rms         logp_diff       tvd  spearman     output   \n",
       "\n",
       "      layer_index  rho_low_plot  rho_high_plot  \n",
       "0              11           NaN            NaN  \n",
       "1              12           NaN            NaN  \n",
       "2              13           NaN            NaN  \n",
       "3              14           NaN            NaN  \n",
       "4              15           NaN            NaN  \n",
       "...           ...           ...            ...  \n",
       "3525            6           NaN            NaN  \n",
       "3526            7           NaN            NaN  \n",
       "3527            8           NaN            NaN  \n",
       "3528            9           NaN            NaN  \n",
       "3529           32           NaN            NaN  \n",
       "\n",
       "[3530 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98e5f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode                  0\n",
       "anchor                0\n",
       "metric                0\n",
       "corr_type             0\n",
       "layer_name            0\n",
       "layer_index           0\n",
       "rho                 346\n",
       "rho_boot_median    3876\n",
       "ci_low             3876\n",
       "ci_high            3876\n",
       "p_perm             3876\n",
       "n_prompts             0\n",
       "n_used_mean         346\n",
       "n_used_median       346\n",
       "pooling               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_perprompt_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176ae5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rho_mean            0\n",
       "rho_low             0\n",
       "rho_high            0\n",
       "rho_boot_mean    3530\n",
       "ci_low_emp       3530\n",
       "ci_high_emp      3530\n",
       "n_total             0\n",
       "mode                0\n",
       "anchor              0\n",
       "metric              0\n",
       "corr_type           0\n",
       "layer_name          0\n",
       "layer_index         0\n",
       "rho_low_plot     3530\n",
       "rho_high_plot    3530\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38a5a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6214142087001617e-18\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_perprompt.csv\")\n",
    "df_sum = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_perprompt_summary_both.csv\")\n",
    "\n",
    "merged = df_sum.merge(df_raw, on=[\"mode\",\"anchor\",\"metric\",\"corr_type\",\"layer_index\"], suffixes=(\"_sum\",\"_raw\"))\n",
    "merged[\"diff\"] = merged[\"rho_mean\"] - merged[\"rho\"]\n",
    "\n",
    "print(merged[\"diff\"].abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae48d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3645738272361821e-18\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_pooled.csv\")\n",
    "df_sum = pd.read_csv(\"saved_data/summary/m_8bit/lw_m_8bit_corr_pooled_summary_both.csv\")\n",
    "\n",
    "merged = df_sum.merge(df_raw, on=[\"mode\",\"anchor\",\"metric\",\"corr_type\",\"layer_index\"], suffixes=(\"_sum\",\"_raw\"))\n",
    "merged[\"diff\"] = merged[\"rho_mean\"] - merged[\"rho\"]\n",
    "\n",
    "print(merged[\"diff\"].abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c24a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== POOLED ===\n",
      "Alle mulige lag: [np.int64(-1), np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]... (34 total)\n",
      "m_4bit: 34 lag fundet, mangler 0 → ingen mangler\n",
      "m_8bit: 34 lag fundet, mangler 0 → ingen mangler\n",
      "m_quant: 34 lag fundet, mangler 0 → ingen mangler\n",
      "\n",
      "=== PER_PROMPT ===\n",
      "Alle mulige lag: [np.int64(-1), np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]... (34 total)\n",
      "m_4bit: 34 lag fundet, mangler 0 → ingen mangler\n",
      "m_8bit: 34 lag fundet, mangler 0 → ingen mangler\n",
      "m_quant: 34 lag fundet, mangler 0 → ingen mangler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    for f in (BASE_DIR / model).glob(\"*_summary*.csv\"):\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    print(f\"\\n=== {pooling.upper()} ===\")\n",
    "    d = df[df[\"pooling\"] == pooling]\n",
    "    all_layers = sorted(df[\"layer_index\"].unique())\n",
    "    print(f\"Alle mulige lag: {all_layers[:10]}... ({len(all_layers)} total)\")\n",
    "    for model in MODELS:\n",
    "        layers = sorted(d[d[\"model\"] == model][\"layer_index\"].unique())\n",
    "        missing = sorted(set(all_layers) - set(layers))\n",
    "        print(f\"{model}: {len(layers)} lag fundet, mangler {len(missing)} → {missing[:10] if missing else 'ingen mangler'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b18e9847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique layers per model (per_prompt):\n",
      "m_4bit: [np.int64(-1), np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)] ... total 34\n",
      "m_8bit: [np.int64(-1), np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)] ... total 34\n",
      "m_quant: [np.int64(-1), np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)] ... total 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6961/2016669249.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  d = subset[(subset[\"model\"] == model) & (df[\"pooling\"] == \"per_prompt\")]\n",
      "/tmp/ipykernel_6961/2016669249.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  d = subset[(subset[\"model\"] == model) & (df[\"pooling\"] == \"per_prompt\")]\n",
      "/tmp/ipykernel_6961/2016669249.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  d = subset[(subset[\"model\"] == model) & (df[\"pooling\"] == \"per_prompt\")]\n"
     ]
    }
   ],
   "source": [
    "subset = df[\n",
    "    (df[\"anchor\"] == \"disagree_correct\") &\n",
    "    (df[\"corr_type\"] == \"pointbiserial\")\n",
    "]\n",
    "\n",
    "print(\"Unique layers per model (per_prompt):\")\n",
    "for model in df[\"model\"].unique():\n",
    "    d = subset[(subset[\"model\"] == model) & (df[\"pooling\"] == \"per_prompt\")]\n",
    "    print(f\"{model}: {sorted(d['layer_index'].unique())[:10]} ... total {d['layer_index'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1fd30",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Plot TopK Correlations =======================\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a89039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] merged 23160 rows from 3 models.\n",
      "                                            count   mean    std    min    25%  \\\n",
      "anchor           corr_type     pooling                                          \n",
      "disagree_correct phi           per_prompt  1857.0  0.018  0.156 -0.864 -0.070   \n",
      "                               pooled      2094.0  0.012  0.076 -0.529 -0.009   \n",
      "                 pointbiserial per_prompt  3366.0  0.018  0.087 -0.191 -0.036   \n",
      "                               pooled      3366.0  0.013  0.070 -0.185 -0.019   \n",
      "                 undefined     per_prompt     0.0    NaN    NaN    NaN    NaN   \n",
      "                               pooled         0.0    NaN    NaN    NaN    NaN   \n",
      "logp_diff        pointbiserial per_prompt  2094.0 -0.018  0.110 -0.328 -0.073   \n",
      "                               pooled      2094.0 -0.010  0.071 -0.338 -0.020   \n",
      "                 spearman      per_prompt  3366.0  0.028  0.107 -0.324 -0.020   \n",
      "                               pooled      3366.0  0.031  0.129 -0.390 -0.021   \n",
      "                 undefined     per_prompt     0.0    NaN    NaN    NaN    NaN   \n",
      "                               pooled         0.0    NaN    NaN    NaN    NaN   \n",
      "\n",
      "                                             50%    75%    max  \n",
      "anchor           corr_type     pooling                          \n",
      "disagree_correct phi           per_prompt  0.001  0.053  1.000  \n",
      "                               pooled     -0.003  0.018  0.489  \n",
      "                 pointbiserial per_prompt  0.013  0.061  0.455  \n",
      "                               pooled      0.010  0.048  0.401  \n",
      "                 undefined     per_prompt    NaN    NaN    NaN  \n",
      "                               pooled        NaN    NaN    NaN  \n",
      "logp_diff        pointbiserial per_prompt  0.010  0.051  0.455  \n",
      "                               pooled      0.000  0.016  0.207  \n",
      "                 spearman      per_prompt  0.017  0.063  0.442  \n",
      "                               pooled      0.018  0.069  0.522  \n",
      "                 undefined     per_prompt    NaN    NaN    NaN  \n",
      "                               pooled        NaN    NaN    NaN  \n",
      "[saved] saved_data/figures_rawcorr_all/pooled/spearman_logp_diff_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/pointbiserial_disagree_correct_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/phi_disagree_correct_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/spearman_logp_diff_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/pointbiserial_disagree_correct_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/phi_disagree_correct_per_prompt_metrics.png\n",
      "\n",
      "[mixed plotting] Generating full cross-anchor × metric plots...\n",
      "[mixed] disagree_correct (pooled) → 18 metrics plotted\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_disagree_correct_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_disagree_correct_pooled_divergence.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_disagree_correct_pooled_representation.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_disagree_correct_pooled_accuracy.png\n",
      "[mixed] logp_diff (pooled) → 18 metrics plotted\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_logp_diff_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_logp_diff_pooled_divergence.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_logp_diff_pooled_representation.png\n",
      "[saved] saved_data/figures_rawcorr_all/pooled/mixed_all_logp_diff_pooled_accuracy.png\n",
      "[mixed] disagree_correct (per_prompt) → 18 metrics plotted\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_disagree_correct_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_disagree_correct_per_prompt_divergence.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_disagree_correct_per_prompt_representation.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_disagree_correct_per_prompt_accuracy.png\n",
      "[mixed] logp_diff (per_prompt) → 18 metrics plotted\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_logp_diff_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_logp_diff_per_prompt_divergence.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_logp_diff_per_prompt_representation.png\n",
      "[saved] saved_data/figures_rawcorr_all/per_prompt/mixed_all_logp_diff_per_prompt_accuracy.png\n",
      "\n",
      "Top correlations:\n",
      "          anchor    metric     corr_type   model      rho\n",
      "       logp_diff   l2_dist      spearman m_quant 0.216796\n",
      "       logp_diff     kl_ab      spearman m_quant 0.199479\n",
      "       logp_diff    js_div      spearman m_quant 0.198268\n",
      "       logp_diff   js_dist      spearman m_quant 0.198267\n",
      "       logp_diff       tvd      spearman m_quant 0.197601\n",
      "       logp_diff     kl_ba      spearman m_quant 0.192367\n",
      "disagree_correct acc_A_@10           phi m_quant 0.104912\n",
      "disagree_correct  acc_A_@5           phi m_quant 0.092202\n",
      "disagree_correct  acc_A_@1           phi m_quant 0.078308\n",
      "disagree_correct     kl_ab pointbiserial  m_8bit 0.058194\n",
      "disagree_correct    js_div pointbiserial  m_8bit 0.058087\n",
      "disagree_correct     kl_ba pointbiserial  m_8bit 0.057839\n",
      "disagree_correct     kl_ab pointbiserial  m_4bit 0.057298\n",
      "disagree_correct    js_div pointbiserial  m_4bit 0.056793\n",
      "disagree_correct       tvd pointbiserial  m_4bit 0.056415\n",
      "disagree_correct   js_dist pointbiserial  m_4bit 0.056323\n",
      "       logp_diff acc_A_@10 pointbiserial  m_8bit 0.055988\n",
      "disagree_correct     kl_ba pointbiserial  m_4bit 0.055793\n",
      "disagree_correct   js_dist pointbiserial  m_8bit 0.054389\n",
      "disagree_correct       tvd pointbiserial  m_8bit 0.053275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# === STYLE ===\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_rawcorr_all\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"rho_smooth\"] = df[\"rho\"]  # raw values only\n",
    "\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "print(df.groupby([\"anchor\",\"corr_type\",\"pooling\"])[\"rho\"].describe().round(3))\n",
    "\n",
    "# === HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def _mark_nans(ax, dsub):\n",
    "    d_nan = dsub[dsub[\"rho\"].isna()]\n",
    "    if not d_nan.empty:\n",
    "        for i, model in enumerate(sorted(d_nan[\"model\"].unique())):\n",
    "            d_m = d_nan[d_nan[\"model\"] == model]\n",
    "            ax.scatter(\n",
    "                d_m[\"layer_index\"],\n",
    "                [-0.95 + 0.05*i] * len(d_m),\n",
    "                color=\"red\", marker=\"x\", s=50, label=f\"{model} NaN\"\n",
    "            )\n",
    "\n",
    "# === PLOT FUNCTIONS ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmode)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmet)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            _mark_nans(ax, dmode)\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} — {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial & Phi)\n",
    "    for corr_type in [\"pointbiserial\", \"phi\"]:\n",
    "        for anchor in ANCHORS_BINARY:\n",
    "            df_b = df[\n",
    "                (df[\"anchor\"] == anchor)\n",
    "                & (df[\"corr_type\"] == corr_type)\n",
    "                & (df[\"pooling\"] == pooling)\n",
    "                & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "            ]\n",
    "            if df_b.empty:\n",
    "                continue\n",
    "            if LAYOUT_MODE == \"mode\":\n",
    "                plot_by_mode(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"metric\":\n",
    "                plot_by_metric(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"group\":\n",
    "                plot_by_group(df_b, corr_type, anchor, pooling, out_dir)\n",
    "\n",
    "# === MIXED ALL-COMBINATION PLOTS ===\n",
    "print(\"\\n[mixed plotting] Generating full cross-anchor × metric plots...\")\n",
    "\n",
    "ALL_CORR_TYPES = [\"phi\", \"pointbiserial\", \"spearman\"]\n",
    "\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for anchor in ANCHORS_BINARY + ANCHORS_CONTINUOUS:\n",
    "        df_all = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"corr_type\"].isin(ALL_CORR_TYPES))\n",
    "        ]\n",
    "        if df_all.empty:\n",
    "            print(f\"[skip] no data for {anchor} ({pooling})\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[mixed] {anchor} ({pooling}) → {df_all['metric'].nunique()} metrics plotted\")\n",
    "        plot_by_metric(df_all, \"mixed_all\", anchor, pooling, out_dir)\n",
    "        plot_by_group(df_all, \"mixed_all\", anchor, pooling, out_dir)\n",
    "\n",
    "# === QUICK TOPLIST ===\n",
    "top = (\n",
    "    df.groupby([\"anchor\", \"metric\", \"corr_type\", \"model\"])[\"rho\"]\n",
    "      .mean().reset_index()\n",
    "      .sort_values(\"rho\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop correlations:\")\n",
    "print(top.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8f1c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rho\n",
       "count  0.0\n",
       "mean   NaN\n",
       "std    NaN\n",
       "min    NaN\n",
       "25%    NaN\n",
       "50%    NaN\n",
       "75%    NaN\n",
       "max    NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"metric == 'ppl_diff'\")[[\"rho\",\"corr_type\",\"anchor\",\"pooling\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd887014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] merged 23160 rows from 3 models.\n",
      "                                            count   mean    std    min    25%  \\\n",
      "anchor           corr_type     pooling                                          \n",
      "disagree_correct phi           per_prompt  1857.0  0.018  0.156 -0.864 -0.070   \n",
      "                               pooled      2094.0  0.012  0.076 -0.529 -0.009   \n",
      "                 pointbiserial per_prompt  3366.0  0.018  0.087 -0.191 -0.036   \n",
      "                               pooled      3366.0  0.013  0.070 -0.185 -0.019   \n",
      "                 undefined     per_prompt     0.0    NaN    NaN    NaN    NaN   \n",
      "                               pooled         0.0    NaN    NaN    NaN    NaN   \n",
      "logp_diff        pointbiserial per_prompt  2094.0 -0.018  0.110 -0.328 -0.073   \n",
      "                               pooled      2094.0 -0.010  0.071 -0.338 -0.020   \n",
      "                 spearman      per_prompt  3366.0  0.028  0.107 -0.324 -0.020   \n",
      "                               pooled      3366.0  0.031  0.129 -0.390 -0.021   \n",
      "                 undefined     per_prompt     0.0    NaN    NaN    NaN    NaN   \n",
      "                               pooled         0.0    NaN    NaN    NaN    NaN   \n",
      "\n",
      "                                             50%    75%    max  \n",
      "anchor           corr_type     pooling                          \n",
      "disagree_correct phi           per_prompt  0.001  0.053  1.000  \n",
      "                               pooled     -0.003  0.018  0.489  \n",
      "                 pointbiserial per_prompt  0.013  0.061  0.455  \n",
      "                               pooled      0.010  0.048  0.401  \n",
      "                 undefined     per_prompt    NaN    NaN    NaN  \n",
      "                               pooled        NaN    NaN    NaN  \n",
      "logp_diff        pointbiserial per_prompt  0.010  0.051  0.455  \n",
      "                               pooled      0.000  0.016  0.207  \n",
      "                 spearman      per_prompt  0.017  0.063  0.442  \n",
      "                               pooled      0.018  0.069  0.522  \n",
      "                 undefined     per_prompt    NaN    NaN    NaN  \n",
      "                               pooled        NaN    NaN    NaN  \n",
      "[saved] saved_data/figures_rawcorr/pooled/spearman_logp_diff_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr/pooled/pointbiserial_disagree_correct_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr/pooled/phi_disagree_correct_pooled_metrics.png\n",
      "[saved] saved_data/figures_rawcorr/per_prompt/spearman_logp_diff_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_rawcorr/per_prompt/pointbiserial_disagree_correct_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_rawcorr/per_prompt/phi_disagree_correct_per_prompt_metrics.png\n",
      "\n",
      "Top correlations:\n",
      "          anchor    metric     corr_type   model      rho\n",
      "       logp_diff   l2_dist      spearman m_quant 0.216796\n",
      "       logp_diff     kl_ab      spearman m_quant 0.199479\n",
      "       logp_diff    js_div      spearman m_quant 0.198268\n",
      "       logp_diff   js_dist      spearman m_quant 0.198267\n",
      "       logp_diff       tvd      spearman m_quant 0.197601\n",
      "       logp_diff     kl_ba      spearman m_quant 0.192367\n",
      "disagree_correct acc_A_@10           phi m_quant 0.104912\n",
      "disagree_correct  acc_A_@5           phi m_quant 0.092202\n",
      "disagree_correct  acc_A_@1           phi m_quant 0.078308\n",
      "disagree_correct     kl_ab pointbiserial  m_8bit 0.058194\n",
      "disagree_correct    js_div pointbiserial  m_8bit 0.058087\n",
      "disagree_correct     kl_ba pointbiserial  m_8bit 0.057839\n",
      "disagree_correct     kl_ab pointbiserial  m_4bit 0.057298\n",
      "disagree_correct    js_div pointbiserial  m_4bit 0.056793\n",
      "disagree_correct       tvd pointbiserial  m_4bit 0.056415\n",
      "disagree_correct   js_dist pointbiserial  m_4bit 0.056323\n",
      "       logp_diff acc_A_@10 pointbiserial  m_8bit 0.055988\n",
      "disagree_correct     kl_ba pointbiserial  m_4bit 0.055793\n",
      "disagree_correct   js_dist pointbiserial  m_8bit 0.054389\n",
      "disagree_correct       tvd pointbiserial  m_8bit 0.053275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# === STYLE ===\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"rho_smooth\"] = df[\"rho\"]  # raw values only\n",
    "\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "print(df.groupby([\"anchor\",\"corr_type\",\"pooling\"])[\"rho\"].describe().round(3))\n",
    "\n",
    "# === HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def _mark_nans(ax, dsub):\n",
    "    d_nan = dsub[dsub[\"rho\"].isna()]\n",
    "    if not d_nan.empty:\n",
    "        for i, model in enumerate(sorted(d_nan[\"model\"].unique())):\n",
    "            d_m = d_nan[d_nan[\"model\"] == model]\n",
    "            ax.scatter(\n",
    "                d_m[\"layer_index\"],\n",
    "                [-0.95 + 0.05*i] * len(d_m),  # lidt forskudt pr. model\n",
    "                color=\"red\", marker=\"x\", s=50, label=f\"{model} NaN\"\n",
    "            )\n",
    "\n",
    "# === PLOT FUNCTIONS ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmode)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmet)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            _mark_nans(ax, dmode)\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} — {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial & Phi)\n",
    "    for corr_type in [\"pointbiserial\", \"phi\"]:\n",
    "        for anchor in ANCHORS_BINARY:\n",
    "            df_b = df[\n",
    "                (df[\"anchor\"] == anchor)\n",
    "                & (df[\"corr_type\"] == corr_type)\n",
    "                & (df[\"pooling\"] == pooling)\n",
    "                & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "            ]\n",
    "            if df_b.empty:\n",
    "                continue\n",
    "            if LAYOUT_MODE == \"mode\":\n",
    "                plot_by_mode(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"metric\":\n",
    "                plot_by_metric(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"group\":\n",
    "                plot_by_group(df_b, corr_type, anchor, pooling, out_dir)\n",
    "\n",
    "# === QUICK TOPLIST ===\n",
    "top = (\n",
    "    df.groupby([\"anchor\", \"metric\", \"corr_type\", \"model\"])[\"rho\"]\n",
    "      .mean().reset_index()\n",
    "      .sort_values(\"rho\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop correlations:\")\n",
    "print(top.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f88821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# === STYLE ===\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_flexible_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# === RAW VALUES (no smoothing) ===\n",
    "df[\"rho_smooth\"] = df[\"rho\"]\n",
    "\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "print(df.groupby([\"anchor\",\"corr_type\",\"pooling\"])[\"rho\"].describe().round(3))\n",
    "\n",
    "# === HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# === ADD NAN MARKERS ===\n",
    "def _mark_nans(ax, dsub):\n",
    "    d_nan = dsub[dsub[\"rho\"].isna()]\n",
    "    if not d_nan.empty:\n",
    "        ax.scatter(\n",
    "            d_nan[\"layer_index\"],\n",
    "            [0] * len(d_nan),\n",
    "            color=\"red\", marker=\"x\", s=60, label=\"NaN / no corr\"\n",
    "        )\n",
    "\n",
    "# === PLOT FUNCTIONS ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmode)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_smooth\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        _mark_nans(ax, dmet)\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_smooth\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            _mark_nans(ax, dmode)\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.axhline(0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.axhline(-0.3, color=\"gray\", linestyle=\"--\", lw=0.6, alpha=0.4)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} — {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial & Phi)\n",
    "    for corr_type in [\"pointbiserial\", \"phi\"]:\n",
    "        for anchor in ANCHORS_BINARY:\n",
    "            df_b = df[\n",
    "                (df[\"anchor\"] == anchor)\n",
    "                & (df[\"corr_type\"] == corr_type)\n",
    "                & (df[\"pooling\"] == pooling)\n",
    "                & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "            ]\n",
    "            if df_b.empty:\n",
    "                continue\n",
    "            if LAYOUT_MODE == \"mode\":\n",
    "                plot_by_mode(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"metric\":\n",
    "                plot_by_metric(df_b, corr_type, anchor, pooling, out_dir)\n",
    "            elif LAYOUT_MODE == \"group\":\n",
    "                plot_by_group(df_b, corr_type, anchor, pooling, out_dir)\n",
    "\n",
    "# === QUICK TOPLIST ===\n",
    "top = (\n",
    "    df.groupby([\"anchor\", \"metric\", \"corr_type\", \"model\"])[\"rho\"]\n",
    "      .mean().reset_index()\n",
    "      .sort_values(\"rho\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop correlations:\")\n",
    "print(top.head(20).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a84c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] merged 27923 rows from 3 models.\n",
      "[saved] saved_data/figures_flexible_clean/pooled/spearman_logp_diff_pooled_metrics.png\n",
      "[saved] saved_data/figures_flexible_clean/pooled/pointbiserial_disagree_correct_pooled_metrics.png\n",
      "[saved] saved_data/figures_flexible_clean/per_prompt/spearman_logp_diff_per_prompt_metrics.png\n",
      "[saved] saved_data/figures_flexible_clean/per_prompt/pointbiserial_disagree_correct_per_prompt_metrics.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_flexible_clean\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*_summary*.csv\"): \n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No summary files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho_mean\",\"rho_low\",\"rho_high\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "\n",
    "\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    \"\"\"Return dynamic rows/cols for a clean subplot grid.\"\"\"\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    \"\"\"Uniform cleanup for figure layout.\"\"\"\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    modes = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "\n",
    "    for ax, mode in zip(axes, modes):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho_mean\",\n",
    "            hue=\"model\", style=\"metric\", markers=False,\n",
    "            lw=2, ax=ax\n",
    "        )\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"Mean ρ\" if mode == \"raw\" else \"\")\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes.png\")\n",
    "\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho_mean\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics.png\")\n",
    "\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho_mean\",\n",
    "                hue=\"model\", style=\"metric\", lw=2,\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} — {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}.png\")\n",
    "\n",
    "\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous anchors Spearman\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary anchors Point-biserial\n",
    "    for anchor in ANCHORS_BINARY:\n",
    "        df_b = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"pointbiserial\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "        ]\n",
    "        if df_b.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e61b4a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] merged 27923 raw correlation rows from 3 models.\n",
      "[saved] saved_data/figures_flexible_rawcorr/pooled/spearman_logp_diff_pooled_metrics_raw.png\n",
      "[saved] saved_data/figures_flexible_rawcorr/pooled/pointbiserial_disagree_correct_pooled_metrics_raw.png\n",
      "[saved] saved_data/figures_flexible_rawcorr/per_prompt/spearman_logp_diff_per_prompt_metrics_raw.png\n",
      "[saved] saved_data/figures_flexible_rawcorr/per_prompt/pointbiserial_disagree_correct_per_prompt_metrics_raw.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_flexible_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LAYOUT_MODE = \"metric\"   # \"mode\" | \"metric\" | \"group\"\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "GROUPS = {\n",
    "    \"divergence\": [\"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"ppl_diff\"],\n",
    "    \"representation\": [\"cosine_sim\",\"l2_dist\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"],\n",
    "    \"accuracy\": [\"acc_A_@1\",\"acc_B_@1\",\"acc_A_@5\",\"acc_B_@5\",\"disagree_correct\"]\n",
    "}\n",
    "\n",
    "\n",
    "# === LOAD RAW CORR FILES (not summaries) ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue  # skip summaries\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "for c in [\"rho\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "print(f\"[ok] merged {len(df)} raw correlation rows from {df['model'].nunique()} models.\")\n",
    "\n",
    "\n",
    "# === PLOTTING HELPERS ===\n",
    "def _auto_subplots(n_items, n_cols=3):\n",
    "    n_rows = int(np.ceil(n_items / n_cols))\n",
    "    return n_rows, n_cols\n",
    "\n",
    "def _finalize_grid(fig, axes, title, save_path=None):\n",
    "    for ax in axes.flat:\n",
    "        if not ax.has_data():\n",
    "            ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=15, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# === PLOT FUNCTIONS (identiske men y='rho') ===\n",
    "def plot_by_mode(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "    modes = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "    for ax, mode in zip(axes, modes):\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmode, x=\"layer_index\", y=\"rho\",\n",
    "            hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "        )\n",
    "        ax.set_title(mode.upper(), fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.legend(fontsize=8)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_modes_raw.png\")\n",
    "\n",
    "\n",
    "def plot_by_metric(df_sub, corr_type, anchor, pooling, out_dir, n_cols=3):\n",
    "    metrics = sorted(df_sub[\"metric\"].unique())\n",
    "    n_rows, n_cols = _auto_subplots(len(metrics), n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 3.5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        dmet = df_sub[df_sub[\"metric\"] == metric]\n",
    "        if dmet.empty:\n",
    "            continue\n",
    "        sns.lineplot(\n",
    "            data=dmet, x=\"layer_index\", y=\"rho\",\n",
    "            hue=\"model\", style=\"mode\", lw=2.2, ax=ax\n",
    "        )\n",
    "        ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        ax.set_ylabel(\"ρ\")\n",
    "        ax.legend(fontsize=8, frameon=True)\n",
    "    _finalize_grid(fig, axes, f\"{corr_type.capitalize()} — {anchor} ({pooling})\",\n",
    "                   out_dir / f\"{corr_type}_{anchor}_{pooling}_metrics_raw.png\")\n",
    "\n",
    "\n",
    "def plot_by_group(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    for gname, gmetrics in GROUPS.items():\n",
    "        dgroup = df_sub[df_sub[\"metric\"].isin(gmetrics)]\n",
    "        if dgroup.empty:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "        for ax, mode in zip(axes, [\"raw\", \"unit_rms\", \"norm_rms\"]):\n",
    "            dmode = dgroup[dgroup[\"mode\"] == mode]\n",
    "            if dmode.empty:\n",
    "                ax.text(0.5, 0.5, \"no data\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "                continue\n",
    "            sns.lineplot(\n",
    "                data=dmode, x=\"layer_index\", y=\"rho\",\n",
    "                hue=\"model\", style=\"metric\", lw=2, ax=ax\n",
    "            )\n",
    "            ax.axhline(0, color=\"black\", linestyle=\":\")\n",
    "            ax.set_title(f\"{gname.title()} ({mode})\", fontsize=11)\n",
    "            ax.set_xlabel(\"Layer index\")\n",
    "            ax.set_ylabel(\"ρ\" if mode == \"raw\" else \"\")\n",
    "            ax.legend(fontsize=8, frameon=True)\n",
    "        _finalize_grid(fig, axes,\n",
    "                       f\"{gname.capitalize()} — {corr_type.capitalize()} ({anchor}/{pooling})\",\n",
    "                       out_dir / f\"{corr_type}_{anchor}_{pooling}_{gname}_raw.png\")\n",
    "\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary (Point-biserial)\n",
    "    for anchor in ANCHORS_BINARY:\n",
    "        df_b = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"pointbiserial\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "        ]\n",
    "        if df_b.empty:\n",
    "            continue\n",
    "        if LAYOUT_MODE == \"mode\":\n",
    "            plot_by_mode(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"metric\":\n",
    "            plot_by_metric(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n",
    "        elif LAYOUT_MODE == \"group\":\n",
    "            plot_by_group(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b619394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] merged 27923 rows from 3 models.\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_4bit_spearman_logp_diff_pooled_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_8bit_spearman_logp_diff_pooled_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_quant_spearman_logp_diff_pooled_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_4bit_spearman_logp_diff_pooled_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_8bit_spearman_logp_diff_pooled_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_quant_spearman_logp_diff_pooled_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_4bit_spearman_logp_diff_pooled_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_8bit_spearman_logp_diff_pooled_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_quant_spearman_logp_diff_pooled_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_4bit_pointbiserial_disagree_correct_pooled_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_8bit_pointbiserial_disagree_correct_pooled_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_quant_pointbiserial_disagree_correct_pooled_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_4bit_pointbiserial_disagree_correct_pooled_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_8bit_pointbiserial_disagree_correct_pooled_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_quant_pointbiserial_disagree_correct_pooled_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_4bit_pointbiserial_disagree_correct_pooled_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_8bit_pointbiserial_disagree_correct_pooled_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/pooled/m_quant_pointbiserial_disagree_correct_pooled_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_4bit_spearman_logp_diff_per_prompt_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_8bit_spearman_logp_diff_per_prompt_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_quant_spearman_logp_diff_per_prompt_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_4bit_spearman_logp_diff_per_prompt_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_8bit_spearman_logp_diff_per_prompt_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_quant_spearman_logp_diff_per_prompt_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_4bit_spearman_logp_diff_per_prompt_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_8bit_spearman_logp_diff_per_prompt_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_quant_spearman_logp_diff_per_prompt_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_4bit_pointbiserial_disagree_correct_per_prompt_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_8bit_pointbiserial_disagree_correct_per_prompt_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_quant_pointbiserial_disagree_correct_per_prompt_raw_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_4bit_pointbiserial_disagree_correct_per_prompt_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_8bit_pointbiserial_disagree_correct_per_prompt_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_quant_pointbiserial_disagree_correct_per_prompt_unit_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_4bit_pointbiserial_disagree_correct_per_prompt_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_8bit_pointbiserial_disagree_correct_per_prompt_norm_rms_heatmap.png\n",
      "[saved] saved_data/figures_heatmaps_rawcorr/per_prompt/m_quant_pointbiserial_disagree_correct_per_prompt_norm_rms_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = Path(\"saved_data/summary\")\n",
    "MODELS = [\"m_4bit\", \"m_8bit\", \"m_quant\"]\n",
    "ANCHORS_CONTINUOUS = [\"logp_diff\"]\n",
    "ANCHORS_BINARY = [\"disagree_correct\"]\n",
    "OUT_ROOT = Path(\"saved_data/figures_heatmaps_rawcorr\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPEARMAN_METRICS = [\n",
    "    \"tvd\",\"kl_ab\",\"kl_ba\",\"js_div\",\"js_dist\",\"cosine_sim\",\n",
    "    \"l2_dist\",\"ppl_diff\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "BISERIAL_METRICS = [\n",
    "    \"acc_A_@1\",\"acc_A_@5\",\"acc_A_@10\",\n",
    "    \"acc_B_@1\",\"acc_B_@5\",\"acc_B_@10\",\n",
    "    \"disagree_correct\",\"jaccard_@1\",\"jaccard_@5\",\"jaccard_@10\"\n",
    "]\n",
    "\n",
    "# === LOAD RAW CORR FILES ===\n",
    "dfs = []\n",
    "for model in MODELS:\n",
    "    model_dir = BASE_DIR / model\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    for f in model_dir.glob(\"*corr_*.csv\"):\n",
    "        if \"summary\" in f.name:\n",
    "            continue\n",
    "        d = pd.read_csv(f)\n",
    "        d[\"model\"] = model\n",
    "        if \"pooled\" in f.stem:\n",
    "            d[\"pooling\"] = \"pooled\"\n",
    "        elif \"perprompt\" in f.stem or \"per_prompt\" in f.stem:\n",
    "            d[\"pooling\"] = \"per_prompt\"\n",
    "        else:\n",
    "            d[\"pooling\"] = \"unknown\"\n",
    "        dfs.append(d)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No raw correlation files found!\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df[\"rho\"] = pd.to_numeric(df[\"rho\"], errors=\"coerce\")\n",
    "df[\"layer_index\"] = pd.to_numeric(df[\"layer_index\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "print(f\"[ok] merged {len(df)} rows from {df['model'].nunique()} models.\")\n",
    "\n",
    "\n",
    "# === HEATMAP FUNCTION ===\n",
    "def plot_heatmap(df_sub, corr_type, anchor, pooling, out_dir):\n",
    "    \"\"\"\n",
    "    Plots rho values as a heatmap (metric x layer) for each model/mode combo.\n",
    "    \"\"\"\n",
    "    modes = [\"raw\", \"unit_rms\", \"norm_rms\"]\n",
    "    for mode in modes:\n",
    "        dmode = df_sub[df_sub[\"mode\"] == mode]\n",
    "        if dmode.empty:\n",
    "            continue\n",
    "\n",
    "        for model in dmode[\"model\"].unique():\n",
    "            dmodel = dmode[dmode[\"model\"] == model]\n",
    "            if dmodel.empty:\n",
    "                continue\n",
    "\n",
    "            # Pivot so that rows = metric, columns = layer_index\n",
    "            heat = dmodel.pivot_table(\n",
    "                index=\"metric\", columns=\"layer_index\", values=\"rho\", aggfunc=\"mean\"\n",
    "            ).sort_index()\n",
    "\n",
    "            plt.figure(figsize=(12, max(6, 0.4 * len(heat))))\n",
    "            sns.heatmap(\n",
    "                heat, cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\",\n",
    "                cbar_kws={\"label\": \"ρ (correlation)\"}, linewidths=0.4\n",
    "            )\n",
    "            plt.title(f\"{model} — {anchor} ({pooling}, {corr_type}, {mode})\", fontsize=13, weight=\"bold\")\n",
    "            plt.xlabel(\"Layer index\")\n",
    "            plt.ylabel(\"Metric\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            out_path = out_dir / f\"{model}_{corr_type}_{anchor}_{pooling}_{mode}_heatmap.png\"\n",
    "            plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"[saved] {out_path}\")\n",
    "\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for pooling in [\"pooled\", \"per_prompt\"]:\n",
    "    out_dir = OUT_ROOT / pooling\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Continuous anchors (Spearman)\n",
    "    for anchor in ANCHORS_CONTINUOUS:\n",
    "        df_s = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"spearman\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(SPEARMAN_METRICS))\n",
    "        ]\n",
    "        if not df_s.empty:\n",
    "            plot_heatmap(df_s, \"spearman\", anchor, pooling, out_dir)\n",
    "\n",
    "    # Binary anchors (Point-biserial)\n",
    "    for anchor in ANCHORS_BINARY:\n",
    "        df_b = df[\n",
    "            (df[\"anchor\"] == anchor)\n",
    "            & (df[\"corr_type\"] == \"pointbiserial\")\n",
    "            & (df[\"pooling\"] == pooling)\n",
    "            & (df[\"metric\"].isin(BISERIAL_METRICS))\n",
    "        ]\n",
    "        if not df_b.empty:\n",
    "            plot_heatmap(df_b, \"pointbiserial\", anchor, pooling, out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit-lens-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
